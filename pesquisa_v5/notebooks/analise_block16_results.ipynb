{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7f3eb3",
   "metadata": {},
   "source": [
    "# Análise Detalhada dos Resultados - Pipeline CNN AV1 (Bloco 16x16)\n",
    "\n",
    "Este notebook apresenta uma análise completa dos resultados do pipeline hierárquico de predição de particionamento AV1 para blocos de tamanho 16x16.\n",
    "\n",
    "## Estrutura da Análise\n",
    "\n",
    "1. **Carregamento e Exploração dos Dados**\n",
    "2. **Desempenho Geral do Pipeline**\n",
    "3. **Stage 1: Classificador Binário**\n",
    "4. **Stage 2: Macro Classes**\n",
    "5. **Stage 3: Especialistas**\n",
    "6. **Matriz de Confusão Final**\n",
    "7. **Métricas por Classe**\n",
    "8. **Análise de Erros**\n",
    "9. **Impacto do Threshold Stage 1**\n",
    "10. **Visualizações Consolidadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3272c",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Exploração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar estilo dos gráficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configurar tamanho padrão das figuras\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados JSON\n",
    "json_path = Path('../logs/v5_pipeline_eval_val_th050.json')\n",
    "csv_path = Path('../logs/v5_pipeline_eval_val_th050.csv')\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Carregar CSV com predições detalhadas\n",
    "df_predictions = pd.read_csv(csv_path)\n",
    "\n",
    "# Exibir informações básicas\n",
    "print(f\"✓ JSON carregado: {json_path}\")\n",
    "print(f\"✓ CSV carregado: {csv_path}\")\n",
    "print(f\"\\nTotal de amostras: {len(df_predictions)}\")\n",
    "print(f\"Threshold Stage1: {results['stage1_threshold']}\")\n",
    "print(f\"Dataset: {results['dataset']}\")\n",
    "print(f\"\\nEspecialistas disponíveis: {', '.join(results['available_specialists'])}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ACURÁCIA FINAL DO PIPELINE: {results['final_accuracy']*100:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5531e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar estrutura do DataFrame\n",
    "print(\"Colunas do CSV:\")\n",
    "print(df_predictions.columns.tolist())\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b7e63",
   "metadata": {},
   "source": [
    "## 2. Desempenho Geral do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a98ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular distribuição das classes reais\n",
    "class_distribution = df_predictions['stage0_true'].value_counts().sort_index()\n",
    "total_samples = len(df_predictions)\n",
    "\n",
    "# Visualizar distribuição\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gráfico de barras\n",
    "axes[0].bar(range(len(class_distribution)), class_distribution.values, color='steelblue')\n",
    "axes[0].set_xticks(range(len(class_distribution)))\n",
    "axes[0].set_xticklabels(class_distribution.index, rotation=45, ha='right')\n",
    "axes[0].set_title('Distribuição das Classes Reais (Ground Truth)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Número de Amostras')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores sobre as barras\n",
    "for i, v in enumerate(class_distribution.values):\n",
    "    axes[0].text(i, v + 100, f'{v}\\n({v/total_samples*100:.1f}%)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Gráfico de pizza (apenas top classes)\n",
    "top_classes = class_distribution.head(5)\n",
    "others = class_distribution[5:].sum()\n",
    "pie_data = list(top_classes.values) + [others]\n",
    "pie_labels = list(top_classes.index) + ['Outros']\n",
    "colors = plt.cm.Set3(range(len(pie_data)))\n",
    "\n",
    "axes[1].pie(pie_data, labels=pie_labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Proporção das 5 Classes Principais', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DISTRIBUIÇÃO DAS CLASSES:\")\n",
    "print(f\"{'='*70}\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"{cls:20s}: {count:6d} ({count/total_samples*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular acurácia geral e por classe\n",
    "correct_predictions = (df_predictions['stage0_true'] == df_predictions['final_pred']).sum()\n",
    "overall_accuracy = correct_predictions / total_samples\n",
    "\n",
    "# Acurácia por classe\n",
    "per_class_accuracy = {}\n",
    "for cls in class_distribution.index:\n",
    "    mask = df_predictions['stage0_true'] == cls\n",
    "    if mask.sum() > 0:\n",
    "        correct = ((df_predictions['stage0_true'] == df_predictions['final_pred']) & mask).sum()\n",
    "        per_class_accuracy[cls] = correct / mask.sum()\n",
    "    else:\n",
    "        per_class_accuracy[cls] = 0.0\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "classes = list(per_class_accuracy.keys())\n",
    "accuracies = list(per_class_accuracy.values())\n",
    "\n",
    "bars = ax.bar(range(len(classes)), accuracies, color='coral')\n",
    "ax.axhline(y=overall_accuracy, color='red', linestyle='--', linewidth=2, label=f'Acurácia Geral: {overall_accuracy*100:.2f}%')\n",
    "ax.set_xticks(range(len(classes)))\n",
    "ax.set_xticklabels(classes, rotation=45, ha='right')\n",
    "ax.set_ylabel('Acurácia')\n",
    "ax.set_title('Acurácia por Classe de Partição', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Adicionar valores sobre as barras\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax.text(i, v + 0.02, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Destacar classes com acurácia < 20% em vermelho\n",
    "for i, v in enumerate(accuracies):\n",
    "    if v < 0.2:\n",
    "        bars[i].set_color('darkred')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ACURÁCIA POR CLASSE:\")\n",
    "print(f\"{'='*70}\")\n",
    "for cls, acc in sorted(per_class_accuracy.items(), key=lambda x: x[1], reverse=True):\n",
    "    status = \"⚠️ CRÍTICO\" if acc < 0.2 else \"✓\" if acc > 0.5 else \"⚠️\"\n",
    "    print(f\"{cls:20s}: {acc*100:6.2f}% {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f2e24",
   "metadata": {},
   "source": [
    "## 3. Stage 1: Classificador Binário (Particiona vs Não-Particiona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acab5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair métricas do Stage 1\n",
    "s1 = results['stage1']\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"STAGE 1 - CLASSIFICADOR BINÁRIO\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Acurácia:  {s1['accuracy']*100:.2f}%\")\n",
    "print(f\"Precisão:  {s1['precision']*100:.2f}%\")\n",
    "print(f\"Recall:    {s1['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {s1['f1']*100:.2f}%\")\n",
    "print(f\"\\nConfusão:\")\n",
    "print(f\"  True Positives  (TP): {s1['tp']:,}\")\n",
    "print(f\"  False Positives (FP): {s1['fp']:,}\")\n",
    "print(f\"  False Negatives (FN): {s1['fn']:,}\")\n",
    "print(f\"\\nInterpretação:\")\n",
    "print(f\"  - Alto recall ({s1['recall']*100:.1f}%) = captura {s1['recall']*100:.1f}% dos blocos que devem particionar\")\n",
    "print(f\"  - Baixa precisão ({s1['precision']*100:.1f}%) = ~{(1-s1['precision'])*100:.1f}% de falsos positivos\")\n",
    "print(f\"  - {s1['fp']:,} blocos NONE enviados incorretamente para Stage 2\")\n",
    "\n",
    "# Visualizar matriz de confusão Stage 1\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Matriz de confusão\n",
    "tn = len(df_predictions) - s1['tp'] - s1['fp'] - s1['fn']  # True Negatives\n",
    "cm_s1 = np.array([[tn, s1['fp']], \n",
    "                  [s1['fn'], s1['tp']]])\n",
    "\n",
    "sns.heatmap(cm_s1, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Pred: NONE', 'Pred: Particiona'],\n",
    "            yticklabels=['Real: NONE', 'Real: Particiona'])\n",
    "axes[0].set_title('Matriz de Confusão - Stage 1', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Métricas comparativas\n",
    "metrics = ['Acurácia', 'Precisão', 'Recall', 'F1-Score']\n",
    "values = [s1['accuracy'], s1['precision'], s1['recall'], s1['f1']]\n",
    "colors_met = ['#2ecc71' if v > 0.7 else '#e74c3c' if v < 0.6 else '#f39c12' for v in values]\n",
    "\n",
    "bars = axes[1].barh(metrics, values, color=colors_met)\n",
    "axes[1].set_xlim(0, 1.0)\n",
    "axes[1].set_xlabel('Valor')\n",
    "axes[1].set_title('Métricas do Stage 1', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0.7, color='green', linestyle='--', alpha=0.5, label='Bom (>70%)')\n",
    "axes[1].axvline(x=0.6, color='orange', linestyle='--', alpha=0.5, label='Moderado (>60%)')\n",
    "axes[1].legend()\n",
    "\n",
    "for i, v in enumerate(values):\n",
    "    axes[1].text(v + 0.02, i, f'{v*100:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906fdd1d",
   "metadata": {},
   "source": [
    "## 4. Stage 2: Classificação em Macro Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf156fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair dados do Stage 2\n",
    "s2 = results['stage2']\n",
    "cm_s2 = np.array(s2['confusion_matrix'])\n",
    "class_names_s2 = s2['class_names']\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"STAGE 2 - MACRO CLASSES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Macro F1-Score: {s2['macro_f1']*100:.2f}%\")\n",
    "print(f\"\\nClasses: {', '.join(class_names_s2)}\")\n",
    "\n",
    "# Visualizar matriz de confusão Stage 2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Heatmap da matriz de confusão\n",
    "sns.heatmap(cm_s2, annot=True, fmt='d', cmap='YlOrRd', ax=axes[0],\n",
    "            xticklabels=class_names_s2, yticklabels=class_names_s2)\n",
    "axes[0].set_xlabel('Predito')\n",
    "axes[0].set_ylabel('Real')\n",
    "axes[0].set_title('Matriz de Confusão - Stage 2', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Calcular métricas por classe\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Filtrar apenas linhas/colunas com dados (excluir NONE e 1TO4)\n",
    "mask = (cm_s2.sum(axis=1) > 0) & (cm_s2.sum(axis=0) > 0)\n",
    "cm_s2_filtered = cm_s2[mask][:, mask]\n",
    "active_classes = [class_names_s2[i] for i in range(len(class_names_s2)) if mask[i]]\n",
    "\n",
    "# Calcular métricas\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(cm_s2_filtered)):\n",
    "    for j in range(len(cm_s2_filtered[i])):\n",
    "        y_true.extend([i] * cm_s2_filtered[i][j])\n",
    "        y_pred.extend([j] * cm_s2_filtered[i][j])\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "\n",
    "# Gráfico de barras com métricas\n",
    "x = np.arange(len(active_classes))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = axes[1].bar(x - width, precision, width, label='Precisão', color='skyblue')\n",
    "bars2 = axes[1].bar(x, recall, width, label='Recall', color='lightcoral')\n",
    "bars3 = axes[1].bar(x + width, f1, width, label='F1-Score', color='lightgreen')\n",
    "\n",
    "axes[1].set_xlabel('Macro Classes')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Métricas por Macro Classe - Stage 2', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(active_classes, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exibir métricas detalhadas\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MÉTRICAS POR MACRO CLASSE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Classe':<15} {'Support':>10} {'Precisão':>12} {'Recall':>12} {'F1-Score':>12}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for i, cls in enumerate(active_classes):\n",
    "    print(f\"{cls:<15} {support[i]:>10} {precision[i]*100:>11.2f}% {recall[i]*100:>11.2f}% {f1[i]*100:>11.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
