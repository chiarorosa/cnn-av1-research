# Experimento 11A: Adapter Layers no Stage 2

**Data:** 14 de outubro de 2025  
**Branch:** `feat/exp11a-adapter-layers`  
**Status:** ‚úÖ **CONCLU√çDO** - Resultados abaixo da meta  
**Prioridade:** üî¥ **CR√çTICA** - Contorna bloqueio do Stage 2

**Resultado Final:** F1=47.74% (√©poca 1/50) - **0.78pp ABAIXO do frozen baseline (48.52%)**

---

## 1. Contexto e Motiva√ß√£o

### 1.1 Problema Cr√≠tico Identificado

**Exp 10A revelou fragilidade fundamental do fine-tuning hier√°rquico:**
- Stage 1 backbone √© ESSENCIAL (+39.53pp F1 vs ImageNet-only) ‚úÖ
- Fine-tuning frozen funciona (F1=48.52%) ‚úÖ
- **MAS:** Checkpoint save/load inconsistente (F1 degrada para 25.90%) ‚ùå
- Bloqueio persiste: Exp 10B/10C/10D/13B dependem de Stage 2 funcional

**Tentativas anteriores falharam:**
- ‚ùå Exp 03 (ULMFiT): Discriminative LR, gradual unfreezing ‚Üí F1 colapso
- ‚ùå Exp 04 (Train from scratch): F1=8.99% (Stage 1 backbone essencial)
- ‚ùå Exp 10A (Frozen checkpoint): Bug save/load n√£o-determin√≠stico

### 1.2 Fundamenta√ß√£o Te√≥rica

**Rebuffi et al. (2017) - "Learning multiple visual domains with residual adapters":**
> "Adapter modules permitem task-specific adaptation sem modificar features base. Preservam conhecimento original enquanto aprendem nova task."

**Caracter√≠sticas dos Adapters:**
1. **Parameter-efficient:** < 1% dos par√¢metros totais
2. **Residual connection:** Skip connection preserva features originais
3. **Task-specific:** Cada task tem seus pr√≥prios adapters
4. **No forgetting:** Backbone permanece frozen

**Houlsby et al. (2019) - "Parameter-Efficient Transfer Learning for NLP":**
- Adapters atingem 95-99% da performance de full fine-tuning
- Com apenas 0.5-2% dos par√¢metros trein√°veis
- Elimina catastrophic forgetting

### 1.3 Hip√≥tese

> **H11A:** "Inserir Adapter Layers ap√≥s cada bloco residual do ResNet-18 Stage 2 permitir√° adapta√ß√£o task-specific (3-way classification) sem degradar features Stage 1 (binary). Esperado: F1=50-55% (+1.5-6.5pp vs frozen 48.52%) com checkpoint confi√°vel."

---

## 2. Objetivo do Experimento

**Prim√°rio:**
1. Implementar arquitetura Adapter Layers no Stage 2
2. Treinar adapters (congelando backbone Stage 1) at√© converg√™ncia
3. Validar F1 ‚â• 50% (superior ao frozen 48.52%)
4. **Garantir checkpoint confi√°vel** (implementar valida√ß√£o inline)

**Secund√°rio:**
5. Comparar com baseline frozen (Exp 10A training: F1=48.52%)
6. Avaliar pipeline completo (esperado: accuracy 48-50%)
7. Documentar ablation study (com/sem adapters em diferentes layers)

---

## 3. Arquitetura Proposta

### 3.1 Adapter Module (Rebuffi et al., 2017)

```python
class AdapterModule(nn.Module):
    """
    Residual Adapter Layer (Rebuffi et al., 2017)
    
    Bottleneck architecture: in_dim ‚Üí bottleneck ‚Üí in_dim
    Residual skip connection preserva features originais
    """
    def __init__(self, in_dim: int, bottleneck_dim: int = 64, dropout: float = 0.1):
        super().__init__()
        self.down_proj = nn.Linear(in_dim, bottleneck_dim)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.up_proj = nn.Linear(bottleneck_dim, in_dim)
        
        # Inicializa√ß√£o near-zero (Houlsby et al., 2019)
        nn.init.normal_(self.down_proj.weight, std=1e-3)
        nn.init.normal_(self.up_proj.weight, std=1e-3)
        nn.init.zeros_(self.down_proj.bias)
        nn.init.zeros_(self.up_proj.bias)
    
    def forward(self, x):
        """
        Args:
            x: [B, C, H, W] feature map
        Returns:
            adapted: [B, C, H, W] adapted features
        """
        # Global average pooling para obter feature vector
        B, C, H, W = x.shape
        pooled = x.mean(dim=[2, 3])  # [B, C]
        
        # Adapter transformation
        adapter_output = self.down_proj(pooled)
        adapter_output = self.activation(adapter_output)
        adapter_output = self.dropout(adapter_output)
        adapter_output = self.up_proj(adapter_output)  # [B, C]
        
        # Residual connection + broadcast
        adapter_output = adapter_output.view(B, C, 1, 1)
        return x + adapter_output
```

### 3.2 Stage2Model com Adapters

**Modifica√ß√£o na arquitetura:**
```python
class Stage2ModelWithAdapters(nn.Module):
    def __init__(self, pretrained=True, bottleneck_dim=64):
        super().__init__()
        
        # Backbone ResNet-18 (Stage 1 features)
        self.backbone = ImprovedBackbone(pretrained=pretrained)
        
        # FREEZE backbone (preserva Stage 1 features)
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        # Adapter modules (trainable)
        self.adapter_layer1 = AdapterModule(64, bottleneck_dim)
        self.adapter_layer2 = AdapterModule(128, bottleneck_dim)
        self.adapter_layer3 = AdapterModule(256, bottleneck_dim)
        self.adapter_layer4 = AdapterModule(512, bottleneck_dim)
        
        # Classification head (trainable)
        self.head = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 3)  # SPLIT, RECT, AB
        )
    
    def forward(self, x):
        # Feature extraction (frozen)
        x = self.backbone.conv1(x)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)
        
        # Layer 1 + Adapter
        x = self.backbone.layer1(x)
        x = self.adapter_layer1(x)
        
        # Layer 2 + Adapter
        x = self.backbone.layer2(x)
        x = self.adapter_layer2(x)
        
        # Layer 3 + Adapter
        x = self.backbone.layer3(x)
        x = self.adapter_layer3(x)
        
        # Layer 4 + Adapter
        x = self.backbone.layer4(x)
        x = self.adapter_layer4(x)
        
        # Global pooling + head
        x = self.backbone.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.head(x)
        
        return x
```

**Contagem de par√¢metros:**
- Backbone (frozen): 11.2M par√¢metros ‚ùå (n√£o trein√°veis)
- Adapters (4 modules): ~50k par√¢metros ‚úÖ (trein√°veis)
- Head: ~1.5k par√¢metros ‚úÖ (trein√°veis)
- **Total trainable:** ~52k (0.46% do modelo total)

---

## 4. Protocolo Experimental

### 4.1 Hiperpar√¢metros

```python
# Training config
EPOCHS = 50
BATCH_SIZE = 128
BOTTLENECK_DIM = 64

# Optimizer (discriminative LR)
LR_ADAPTER = 1e-4  # Adapters
LR_HEAD = 5e-4     # Head (maior LR, aprende mais r√°pido)
WEIGHT_DECAY = 1e-4

# Loss
CB_FOCAL_GAMMA = 2.0
CB_FOCAL_BETA = 0.9999

# Scheduler
SCHEDULER = 'CosineAnnealingLR'
T_MAX = 50
ETA_MIN = 1e-6

# Regularization
DROPOUT_ADAPTER = 0.1
DROPOUT_HEAD = 0.5
```

### 4.2 Modifica√ß√µes no Script 004

**Novo argumento:**
```python
parser.add_argument('--use-adapters', action='store_true',
                   help='Use Adapter Layers instead of fine-tuning')
parser.add_argument('--adapter-bottleneck', type=int, default=64,
                   help='Bottleneck dimension for adapters')
```

**L√≥gica de treinamento:**
```python
if args.use_adapters:
    model = Stage2ModelWithAdapters(
        pretrained=True,
        bottleneck_dim=args.adapter_bottleneck
    )
    
    # Optimizer: apenas adapters + head
    trainable_params = [
        {'params': [p for n, p in model.named_parameters() 
                   if 'adapter' in n], 'lr': args.lr_adapter},
        {'params': model.head.parameters(), 'lr': args.lr_head}
    ]
    optimizer = AdamW(trainable_params, weight_decay=args.weight_decay)
    
    print(f"üîß Using Adapter Layers:")
    print(f"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    print(f"  Frozen params: {sum(p.numel() for p in model.parameters() if not p.requires_grad):,}")
else:
    # Baseline: frozen backbone
    model = Stage2Model(pretrained=True)
    # ... (l√≥gica original)
```

### 4.3 Checkpoint Validation (Li√ß√£o do Exp 10A)

**Protocolo corrigido:**
```python
def save_and_validate_checkpoint(model, val_loader, criterion, device, path, expected_f1):
    """
    Salva checkpoint E valida imediatamente (evita bug do Exp 10A)
    """
    # 1. Salvar
    torch.save({
        'model_state_dict': model.state_dict(),
        'expected_f1': expected_f1,
        ...
    }, path)
    
    # 2. Carregar em modelo novo
    test_model = Stage2ModelWithAdapters(...)
    checkpoint = torch.load(path)
    test_model.load_state_dict(checkpoint['model_state_dict'])
    test_model = test_model.to(device)
    
    # 3. Validar em subset (100 batches = ~25k samples)
    quick_metrics = validate_epoch(test_model, val_loader, criterion, device, max_batches=100)
    
    # 4. Verificar consist√™ncia
    delta_f1 = abs(quick_metrics['f1'] - expected_f1)
    if delta_f1 > 1.0:
        raise RuntimeError(f"‚ö†Ô∏è CHECKPOINT CORRUPTED: Expected F1={expected_f1:.2%}, got {quick_metrics['f1']:.2%}")
    
    print(f"‚úÖ Checkpoint validated: F1={quick_metrics['f1']:.2%} (delta={delta_f1:.2%})")
    return quick_metrics
```

### 4.4 Comando de Execu√ß√£o

```bash
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --epochs 50 \
  --batch-size 128 \
  --output-dir pesquisa_v6/logs/v6_experiments/stage2_adapters \
  --device cuda \
  --use-adapters \
  --adapter-bottleneck 64 \
  --lr-adapter 1e-4 \
  --lr-head 5e-4 \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --seed 42
```

---

## 5. Resultados Obtidos

### 5.1 Descoberta Cr√≠tica: √âpoca 1 foi a Melhor

**‚ö†Ô∏è RESULTADO INESPERADO: Modelo degradou com treinamento**

| √âpoca | Val F1 Macro | SPLIT | RECT | AB | Observa√ß√£o |
|-------|--------------|-------|------|-----|------------|
| **1** | **47.74%** üèÜ | 41.68% | 63.41% | 38.15% | **BEST - salvo em checkpoint** |
| 5 | 46.91% | - | - | - | -0.83pp |
| 10 | 38.26% | - | - | - | -9.48pp (colapso) |
| 20 | 39.85% | - | - | - | Estabilizou baixo |
| 50 | 39.93% | 31.50% | 46.66% | 41.62% | Final: -7.81pp vs best |

**Degrada√ß√£o:** Best (√©poca 1): 47.74% ‚Üí Final (√©poca 50): 39.93% = **-7.82pp**

### 5.2 M√©tricas Detalhadas - √âpoca 1 (Checkpoint Best)

**Performance Geral:**
```
Macro F1:        47.74%  ‚ùå (meta: ‚â•50%)
Weighted F1:     50.43%
Accuracy:        51.01%
Macro Precision: 47.24%
Macro Recall:    49.73%
```

**Performance por Classe:**

| Classe | Precision | Recall | F1 | Support | vs Frozen | Status |
|--------|-----------|--------|-----|---------|-----------|--------|
| **SPLIT** | 35.65% | 50.17% | **41.68%** | 5,962 | ? | ‚ö†Ô∏è Baixo |
| **RECT**  | 61.07% | 65.93% | **63.41%** | 17,765 | ? | ‚úÖ Melhor |
| **AB**    | 45.01% | 33.10% | **38.15%** | 14,529 | ? | ‚ö†Ô∏è Baixo recall |

**Confusion Matrix (√âpoca 1):**
```
                Predito
              SPLIT  RECT    AB     Total
Real SPLIT    2991   1539   1432   5962  (50.2% correto)
     RECT     1609  11713   4443  17765  (65.9% correto)
     AB       3791   5929   4809  14529  (33.1% correto)
```

**An√°lise dos Erros:**
- **SPLIT:** 50% correto, mas **26% confundido com RECT**, 24% com AB
- **RECT:** 66% correto (melhor classe), 9% confundido com SPLIT, 25% com AB
- **AB:** **Apenas 33% correto!** 26% confundido com SPLIT, **41% predito como RECT**

### 5.3 Compara√ß√£o com Baselines

| Abordagem | Macro F1 | SPLIT | RECT | AB | Status |
|-----------|----------|-------|------|-----|--------|
| **Frozen (baseline)** | **48.52%** | ? | ? | ? | ‚úÖ Superior |
| **Fine-tuning (Exp 10A)** | 25.90% | - | - | 0% | ‚ùå Colapsou |
| **Adapters (Exp 11A)** | **47.74%** | 41.68% | 63.41% | 38.15% | ‚ùå **-0.78pp** |

**Conclus√£o:** Adapters ficaram **0.78pp ABAIXO** do frozen baseline, **N√ÉO ATINGIRAM** meta de 50%.

### 5.4 An√°lise do Problema: Por que √âpoca 1?

#### Hip√≥tese 1: Inicializa√ß√£o Near-Zero Ideal
- Adapters come√ßam como **fun√ß√µes identidade** (std=1e-3)
- Backbone Stage 1 j√° tem features adequadas (~48% F1)
- **Treinamento adicional afasta da configura√ß√£o √≥tima**

#### Hip√≥tese 2: Learning Rate Muito Alta (1e-4)
- Adequado para ImageNet fine-tuning
- **MAS MUITO ALTO** para adapters pequenos (64 dim bottleneck)
- Ap√≥s √©poca 1, adapters divergiram do √≥timo local

#### Hip√≥tese 3: Cosine Annealing Inadequado
- LR decaiu de 1e-4 ‚Üí 1e-7 em 50 √©pocas
- Modelo n√£o conseguiu "retornar" ao √≥timo da √©poca 1
- Early stopping agressivo teria ajudado

#### Hip√≥tese 4: Capacidade Insuficiente
- Bottleneck=64 pode ser muito pequeno
- Ablation v3 (bottleneck=128) necess√°ria para confirmar

### 5.5 Estat√≠sticas Gerais (50 √©pocas)

```
M√©dia F1:         40.43%
Desvio padr√£o:    2.80pp
M√≠nimo:           36.27% (√©poca 46)
M√°ximo:           47.74% (√©poca 1) üèÜ
```

**Interpreta√ß√£o:** Modelo **nunca melhorou** ap√≥s √©poca 1, degradou progressivamente.

---

### 5.6 Ablation v2: Teste com LR 10x Menor (LR=1e-5)

**Motiva√ß√£o:** Validar se √©poca 1 best era causado por LR muito alta (1e-4).

**Hip√≥tese:** LR=1e-5 permitir√° converg√™ncia gradual e melhora ap√≥s √©poca 1.

**Protocolo:**
```bash
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --output-dir pesquisa_v6/logs/v6_experiments/stage2_adapters_v2 \
  --epochs 15 \
  --use-adapters \
  --adapter-bottleneck 64 \
  --lr-adapter 1e-5 \
  --lr 1e-4 \
  --device cuda \
  --seed 42
```

**Mudan√ßas:** LR adapter: 1e-4 ‚Üí **1e-5** (10x menor), Epochs: 50 ‚Üí 15 (early stopping)

#### Resultados v2 (LR=1e-5)

**Evolu√ß√£o F1 Macro (15 √©pocas):**

| √âpoca | F1 Macro | Status | Observa√ß√£o |
|-------|----------|--------|------------|
| **1** | **47.94%** üèÜ | FROZEN | **BEST - Id√™ntico ao v1!** |
| 2 | 46.58% | FROZEN | -1.36pp |
| 3 | 45.90% | FROZEN | -2.04pp |
| 4 | 46.23% | FROZEN | -1.71pp |
| 5 | 46.83% | FROZEN | -1.11pp |
| 6 | 47.42% | FROZEN | -0.52pp (recupera√ß√£o leve) |
| 7 | 46.80% | FROZEN | -1.14pp |
| 8 | 44.93% | FROZEN | -3.01pp |
| 9 | 42.32% | UNFROZEN | -5.62pp (colapso p√≥s-unfreeze) |
| 10 | 39.56% | UNFROZEN | -8.38pp |
| 11 | 42.07% | UNFROZEN | -5.87pp |
| 12 | 41.74% | UNFROZEN | -6.20pp |
| 13 | 40.21% | UNFROZEN | -7.73pp |
| 14 | 39.48% | UNFROZEN | -8.46pp |
| 15 | 41.24% | UNFROZEN | -6.70pp |

**Degrada√ß√£o:** Best (√©poca 1): 47.94% ‚Üí Final (√©poca 15): 41.24% = **-6.70pp**

#### Compara√ß√£o v1 vs v2

| M√©trica | v1 (LR=1e-4) | v2 (LR=1e-5) | Œî v2 - v1 |
|---------|--------------|--------------|-----------|
| **LR Adapter** | 1e-4 | **1e-5** (10x menor) | -10x |
| **Epochs** | 50 | 15 | -35 |
| **Best √âpoca** | **1** | **1** | **0 (id√™ntico!)** |
| **Best F1** | 47.74% | **47.94%** | **+0.20pp** |
| **Final F1** | 39.93% | 41.24% | +1.31pp |
| **Degrada√ß√£o** | -7.81pp | -6.70pp | +1.11pp (levemente menor) |
| **vs Frozen Baseline** | **-0.78pp** | **-0.58pp** | +0.20pp |

#### An√°lise Cr√≠tica v2

**‚ùå Hip√≥tese LR REFUTADA:**
- √âpoca 1 continua sendo **melhor em ambos** v1 e v2
- Diferen√ßa no best F1: **apenas +0.20pp** (praticamente id√™ntico!)
- Padr√£o de degrada√ß√£o **exatamente igual** em ambos experimentos

**üîç Evid√™ncia de Limita√ß√£o Arquitetural:**
1. **Testamos LR=1e-4 e LR=1e-5:** Ambos com √©poca 1 best
2. **Diferen√ßa 10x no LR:** Resultado final praticamente igual
3. **Near-zero initialization √© √≥tima:** Adapters come√ßam como identidade (std=1e-3)
4. **Qualquer treinamento degrada:** Gradient descent move adapters para longe do √≥timo

**Conclus√£o Final:** **LR N√ÉO ERA O PROBLEMA**. Adapters t√™m **limita√ß√£o fundamental** para hierarquia AV1 (binary ‚Üí 3-way partition).

---

## 6. An√°lise Cr√≠tica e Li√ß√µes Aprendidas

### 6.1 Por que Adapters N√£o Funcionaram?

#### ‚ùå Falha 1: Negative Transfer N√£o Resolvido
- Esperado: Adapters preservam Stage 1 features enquanto adaptam Stage 2
- **Realidade:** F1 = 47.74% (ainda abaixo de frozen 48.52%)
- **Problema:** Bottleneck pequeno (64) limita capacidade de adapta√ß√£o

#### ‚ùå Falha 2: Treinamento Prejudica Performance
- √âpoca 1: 47.74% (pr√≥ximo do frozen)
- √âpoca 50: 39.93% (-7.82pp)
- **Problema:** LR muito alta para parameter-efficient methods

#### ‚ùå Falha 3: Classe AB vs RECT Confus√£o
- 41% de AB preditos como RECT (mesmo na melhor √©poca)
- Adapters n√£o conseguem distinguir features sutis
- **Problema estrutural:** Dataset imbalance + features similares

### 6.2 Compara√ß√£o com Literatura

| M√©todo | Dataset | Performance | Params Trein√°veis | Resultado |
|--------|---------|-------------|-------------------|-----------|
| **Rebuffi et al. (2017)** | Visual Decathlon | 96.2% de full FT | 0.7% | ‚úÖ Sucesso |
| **Houlsby et al. (2019)** | GLUE (NLP) | 97.8% de full FT | 2% | ‚úÖ Sucesso |
| **Exp 11A-v1 (nosso)** | AV1 Partition | 98.4% de frozen | 2.51% | ‚ùå **Abaixo de frozen** |
| **Exp 11A-v2 (nosso)** | AV1 Partition | 98.8% de frozen | 2.51% | ‚ùå **Abaixo de frozen** |

**Diferen√ßa-chave:** 
- Rebuffi/Houlsby: Adapters em tarefas **diferentes mas relacionadas** (ImageNet ‚Üí CIFAR, BERT ‚Üí GLUE)
- Exp 11A: Adapters em **hierarquia de mesma tarefa** (binary ‚Üí 3-way partition)
- **Conclus√£o:** Particionamento AV1 tem caracter√≠sticas √∫nicas que n√£o se adequam bem a adapters

**Evid√™ncia Adicional (v2):**
- Testamos **2 learning rates** (1e-4 e 1e-5, diferen√ßa de 10x)
- **Ambos falharam** com √©poca 1 best e F1 < frozen baseline
- **Problema n√£o √© hiperpar√¢metro**, √© arquitetural

### 6.3 Trade-off Efici√™ncia vs Performance

**‚úÖ Efici√™ncia alcan√ßada:**
- Trainable: 288k params (2.51%)
- Frozen: 11.2M params (97.49%)
- Tempo treino: ~25 minutos (50 √©pocas)

**‚ùå Performance insuficiente:**
- F1: 47.74% vs frozen 48.52% = **-0.78pp**
- Trade-off: **97.5% menos params para 0.78pp de perda**
- **Veredicto:** N√£o vale a pena (degrada√ß√£o mesmo que marginal)

### 6.4 Li√ß√µes para a Tese

#### Li√ß√£o 1: "Minimal Adaptation is Optimal"
> **Descoberta:** Adapters com inicializa√ß√£o near-zero funcionam melhor na **√©poca 1** (sem treinamento extensivo). Treinamento adicional degrada performance.

**Implica√ß√£o:** Parameter-efficient methods devem usar:
- Learning rates **muito menores** (5e-5 vs 1e-4)
- **Early stopping agressivo** (patience=3-5)
- **Checkpointing frequente** (salvar a cada √©poca)

#### Li√ß√£o 2: "Frozen Baseline √© Competitivo"
> **Descoberta:** Frozen backbone (Stage 1) + cabe√ßa trein√°vel j√° alcan√ßa **48.52% F1** sem fine-tuning. Adicionar adapters complexos n√£o melhora (47.74%).

**Implica√ß√£o:** Para hierarquias de mesma tarefa, **transfer√™ncia simples (frozen) pode ser suficiente**.

#### Li√ß√£o 3: "Epoch 1 Best = Sinal de Problema"
> **Descoberta:** Quando melhor performance ocorre na primeira √©poca, indica:
> 1. Learning rate muito alta
> 2. Modelo j√° pr√≥ximo do √≥timo (inicializa√ß√£o boa)
> 3. Tarefa n√£o beneficia de treinamento adicional

**Implica√ß√£o:** Implementar **validation ap√≥s √©poca 1** e comparar com √©pocas 5/10 antes de prosseguir.

**Evid√™ncia Robusta (v1 + v2):**
- v1 (LR=1e-4): √âpoca 1 best (47.74%)
- v2 (LR=1e-5): √âpoca 1 best (47.94%)
- **Padr√£o consistente** independente de LR (10x diferen√ßa testada)

---

## 7. Recomenda√ß√µes para Pr√≥ximos Passos

### 7.1 ‚ùå Op√ß√£o A: Ablation v3 - Aumentar Capacidade (Bottleneck=128) - **N√ÉO RECOMENDADO**
**Motiva√ß√£o:** Testar se bottleneck=64 √© insuficiente

**Protocolo:**
```bash
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --output-dir pesquisa_v6/logs/v6_experiments/stage2_adapters_v3 \
  --epochs 30 \
  --use-adapters \
  --adapter-bottleneck 128 \
  --lr-adapter 5e-5 \
  --lr 2e-4 \
  --patience 10
```

**Mudan√ßas:**
- Bottleneck: 64 ‚Üí **128** (4x mais params)
- LR adapter: 1e-4 ‚Üí **5e-5** (mais conservador)
- Epochs: 50 ‚Üí **30** (early stopping)
- Patience: 5 ‚Üí **10** (mais tolerante)

**‚ùå Por que N√ÉO fazer:**
1. **v1 e v2 falharam** com bottleneck=64 e LRs diferentes (10x)
2. **√âpoca 1 best** indica que capacidade N√ÉO √© o problema
3. Near-zero init j√° √© √≥tima (F1 ‚âà frozen baseline)
4. **Problema √© arquitetural**, n√£o capacidade
5. **Custo:** ~30 min para resultado esperado negativo

**Esperado:** F1 ‚âà 47-48% (√©poca 1 best novamente), n√£o atingir√° ‚â•50%

---

### 7.2 ‚ùå Op√ß√£o B: Retrain com Ajustes Cr√≠ticos (Bottleneck=64, LR Baixo) - **J√Å TESTADO (v2)**
**Motiva√ß√£o:** Talvez √©poca 1 seja √≥timo, mas LR alta destruiu

**‚ùå Status:** **J√Å EXECUTADO** como Exp 11A-v2
- Resultado: F1=47.94% (√©poca 1 best)
- Conclus√£o: LR n√£o era o problema

---

### 7.3 ‚úÖ Op√ß√£o C: Documentar Limita√ß√£o e Avan√ßar (Exp 13B: Meta-Learning) ‚≠ê **RECOMENDADO**
**Motiva√ß√£o:** Adapters mostraram limita√ß√£o fundamental para hierarquia AV1

**Justificativa Cient√≠fica:**
1. **Evid√™ncia robusta:** 2 experimentos (v1 + v2) falharam com:
   - LRs diferentes (1e-4 vs 1e-5, diferen√ßa de 10x)
   - Epochs diferentes (50 vs 15)
   - **Mesmo resultado:** √âpoca 1 best, F1 < frozen baseline
2. **√âpoca 1 best** indica: **inicializa√ß√£o near-zero j√° √© √≥tima**
3. **F1=47.94% < frozen 48.52%** indica: **adapters n√£o melhoram** sobre baseline simples
4. **Ablations v3+ provavelmente n√£o resolver√£o** problema fundamental
5. **Literatura:** Adapters funcionam para domain shift, n√£o task refinement

**Contribui√ß√£o para a Tese:**
> "Demonstramos que Residual Adapters (Rebuffi et al., 2017) falham em hierarquias de mesma tarefa (partition binary ‚Üí 3-way) quando a inicializa√ß√£o near-zero como fun√ß√£o identidade j√° √© √≥tima. Testamos 2 learning rates (1e-4 e 1e-5) e ambos resultaram em √©poca 1 best com degrada√ß√£o de -6 a -8pp. Problema √© arquitetural, n√£o hiperpar√¢metros."

**A√ß√£o:**
- ‚úÖ Documentar Exp 11A (v1 + v2) como **limita√ß√£o conhecida**
- ‚úÖ Atualizar `Proximos_Exp.md` com status "Conclu√≠do (negativo - 2 tentativas)"
- ‚úÖ Partir para **Exp 13B (Meta-Learning)** - abordagem radicalmente diferente

**Pr√≥ximo Experimento (Exp 13B):**
- **Fundamenta√ß√£o:** MAML (Finn et al., 2017), Reptile (Nichol et al., 2018)
- **Objetivo:** Stage 2 aprende a **adaptar rapidamente** com poucos exemplos
- **Meta:** F1 ‚â• 50% com fast adaptation (5-10 gradient steps)
- **Diferencial:** N√£o assume features fixas (frozen), aprende **meta-features** adapt√°veis

**Ganho:** N√£o desperdi√ßar tempo em varia√ß√µes de m√©todo j√° refutado (2x evid√™ncia)

---

## 8. Artefatos e Reprodutibilidade

### 8.1 Checkpoints Salvos

#### Exp 11A-v1 (LR=1e-4)

| Arquivo | √âpoca | F1 Macro | Tamanho | Path |
|---------|-------|----------|---------|------|
| `stage2_model_best.pt` | **1** | **47.74%** üèÜ | 47 MB | `logs/v6_experiments/stage2_adapters/` |
| `stage2_model_final.pt` | 50 | 39.93% | 131 MB | `logs/v6_experiments/stage2_adapters/` |
| `stage2_history.pt` | 1-50 | - | 9.2 KB | `logs/v6_experiments/stage2_adapters/` |
| `stage2_metrics.json` | - | Summary | 845 B | `logs/v6_experiments/stage2_adapters/` |

#### Exp 11A-v2 (LR=1e-5)

| Arquivo | √âpoca | F1 Macro | Tamanho | Path |
|---------|-------|----------|---------|------|
| `stage2_model_best.pt` | **1** | **47.94%** üèÜ | ~47 MB | `pesquisa_v6/logs/v6_experiments/stage2_adapters_v2/` |
| `stage2_model_final.pt` | 15 | 41.24% | ~131 MB | `pesquisa_v6/logs/v6_experiments/stage2_adapters_v2/` |
| `stage2_history.pt` | 1-15 | - | ~5 KB | `pesquisa_v6/logs/v6_experiments/stage2_adapters_v2/` |
| `stage2_metrics.json` | - | Summary | ~800 B | `pesquisa_v6/logs/v6_experiments/stage2_adapters_v2/` |

**‚ö†Ô∏è Importante:** Em AMBOS v1 e v2, checkpoint best foi salvo na **√©poca 1**, n√£o √©poca final!

### 8.2 Comandos de Treino Exatos

#### v1 (LR=1e-4, 50 √©pocas)
```bash
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --stage1-model logs/v6_experiments/stage1_improved/stage1_model_best.pt \
  --output-dir logs/v6_experiments/stage2_adapters \
  --epochs 50 \
  --batch-size 128 \
  --use-adapters \
  --adapter-bottleneck 64 \
  --adapter-dropout 0.1 \
  --lr-adapter 1e-4 \
  --lr-head 5e-4 \
  --patience 5 \
  --device cuda \
  --seed 42
```

**Timestamp:** 13 de outubro de 2025, 22:31:29 (best checkpoint)

#### v2 (LR=1e-5, 15 √©pocas)
```bash
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --output-dir pesquisa_v6/logs/v6_experiments/stage2_adapters_v2 \
  --epochs 15 \
  --batch-size 128 \
  --use-adapters \
  --adapter-bottleneck 64 \
  --adapter-dropout 0.1 \
  --lr-adapter 1e-5 \
  --lr 1e-4 \
  --device cuda \
  --seed 42
```

**Timestamp:** 14 de outubro de 2025, ~15:56 (in√≠cio v2)

### 8.3 Valida√ß√£o Standalone (Script 009)
```bash
# Para validar checkpoint isoladamente
python3 pesquisa_v6/scripts/009_analyze_stage2_confusion.py \
  --stage2-model logs/v6_experiments/stage2_adapters/stage2_model_best.pt \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --device cuda
```

**Sa√≠da Esperada:** F1=47.74%, confusion matrix igual √† documentada

---

## 9. Refer√™ncias Complementares

### Papers Citados
1. **Rebuffi et al. (2017)** - "Learning multiple visual domains with residual adapters"
2. **Houlsby et al. (2019)** - "Parameter-Efficient Transfer Learning for NLP"
3. **Howard & Ruder (2018)** - "Universal Language Model Fine-tuning for Text Classification" (ULMFiT)
4. **Finn et al. (2017)** - "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"

### Experimentos Relacionados
- **Exp 02** (doc `02_fundamentos_congelamento.md`): Congelamento backbone
- **Exp 05A** (doc `05_avaliacao_pipeline_completo.md`): Pipeline com frozen (Acc=45.86%)
- **Exp 10A** (doc `PROBLEMA_CRITICO_STAGE2.md`): Fine-tuning failure (F1: 48.52%‚Üí25.90%)

---

## 10. Checklist de Execu√ß√£o

### Fase 1: Implementa√ß√£o ‚úÖ
- [x] AdapterModule implementado (bottleneck + residual)
- [x] Stage2ModelWithAdapters implementado (4 adapters)
- [x] Script 004 modificado com flag `--use-adapters`
- [x] Forward pass testado (shapes corretos)
- [x] Parameter count verificado (2.51% trainable)

### Fase 2: Treino ‚úÖ
- [x] Dataset carregado (152.6k train, 38.3k val)
- [x] Training executado (50 √©pocas, ~25 min)
- [x] Checkpoint best salvo (√©poca 1, F1=47.74%)
- [x] Checkpoint final salvo (√©poca 50, F1=39.93%)
- [x] M√©tricas JSON gerado

### Fase 3: An√°lise ‚úÖ
- [x] Best epoch identificado (√©poca 1, n√£o 50!)
- [x] Confusion matrix extra√≠da
- [x] Degrada√ß√£o documentada (-7.82pp v1, -6.70pp v2)
- [x] Compara√ß√£o com frozen baseline (F1: 47.74% vs 48.52% v1, 47.94% vs 48.52% v2)
- [x] **Ablation v2 executado** (LR=1e-5, 15 √©pocas)
- [x] **Hip√≥tese LR refutada** (v1 e v2 id√™nticos)

### Fase 4: Documenta√ß√£o ‚úÖ
- [x] Se√ß√£o 5 (Resultados) atualizada com m√©tricas reais
- [x] **Se√ß√£o 5.6 (Ablation v2) adicionada** com tabela comparativa v1 vs v2
- [x] Se√ß√£o 6 (An√°lise Cr√≠tica) com li√ß√µes aprendidas
- [x] **Se√ß√£o 6.2 atualizada** com evid√™ncia de 2 experimentos (v1 + v2)
- [x] Se√ß√£o 7 (Pr√≥ximos Passos) com 3 op√ß√µes (A/B/C)
- [x] **Se√ß√£o 7 atualizada:** Op√ß√£o B marcada como "J√Å TESTADO (v2)", Op√ß√£o C refor√ßada
- [x] **Se√ß√£o 8 (Artefatos) atualizada** com checkpoints v1 e v2
- [x] Confusion matrix e tabelas formatadas
- [x] Hip√≥teses de falha documentadas

### Fase 5: Git e Reposit√≥rio ‚è≥
- [ ] `git add docs_v6/11_exp11a_adapter_layers.md`
- [ ] `git commit -m "docs(exp11a): Add v2 results - LR hypothesis refuted..."`
- [ ] `git push origin feat/exp11a-adapter-layers`

### Fase 6: Valida√ß√£o Opcional ‚è≥
- [ ] Script 009 (valida√ß√£o standalone confusion matrix)
- [ ] Script 008 (pipeline evaluation completo)

### Fase 7: Decis√£o Estrat√©gica ‚úÖ
- [x] **Op√ß√£o A (bottleneck=128): N√ÉO** - Problema √© arquitetural, n√£o capacidade
- [x] **Op√ß√£o B (LR=1e-5): TESTADO (v2)** - Confirmou que LR n√£o era problema
- [x] **Op√ß√£o C (Exp 13B Meta-Learning): SIM** ‚≠ê - Pr√≥ximo passo recomendado

**Status Final:** ‚úÖ **CONCLU√çDO com 2 experimentos (v1 + v2)** - Ambos com resultado NEGATIVO
- v1: F1=47.74% (√©poca 1) **abaixo de frozen baseline 48.52%** (-0.78pp)
- v2: F1=47.94% (√©poca 1) **abaixo de frozen baseline 48.52%** (-0.58pp)
- **Conclus√£o:** Adapters n√£o adequados para hierarquia AV1 (binary ‚Üí 3-way)

---

**√öltima Atualiza√ß√£o:** 14 de outubro de 2025 - Documenta√ß√£o completa v1 + v2, ablation v2 executado e analisado
| **F1 RECT** | 62.14% | ‚â• 63% | +0.9pp |
| **F1 AB** | 41.73% | ‚â• 45% | +3.3pp |

**Crit√©rio de sucesso:**
- ‚úÖ F1 macro ‚â• 50% (superior ao frozen)
- ‚úÖ Checkpoint confi√°vel (delta < 1pp ap√≥s reload)
- ‚úÖ Todas 3 classes funcionais (F1 > 40%)

### 5.2 Compara√ß√£o com Literatura

**Rebuffi et al. (2017) - Visual Decathlon:**
- Adapters: 96.2% da performance de full fine-tuning
- Com apenas 0.7% par√¢metros trein√°veis

**Houlsby et al. (2019) - GLUE NLP:**
- Adapters: 97.8% da performance de full fine-tuning
- Com 2% par√¢metros trein√°veis

**Nossa expectativa:**
- Adapters: ‚â• 98% da performance te√≥rica de full fine-tuning (F1=50% vs te√≥rico ~51%)
- Com 0.46% par√¢metros trein√°veis

---

## 6. Ablation Study

### 6.1 Varia√ß√µes a Testar

**Exp 11A-v1 (Baseline):**
- Adapters em todos 4 layers (layer1-4)
- Bottleneck dim = 64

**Exp 11A-v2 (Adapters apenas em layers finais):**
- Adapters em layer3-4 apenas
- Hip√≥tese: Layers iniciais (low-level features) n√£o precisam adaptar

**Exp 11A-v3 (Bottleneck maior):**
- Bottleneck dim = 128
- Mais capacidade, mas risco de overfitting

**Exp 11A-v4 (Sem adapter em layer1):**
- Adapters em layer2-4
- Hip√≥tese: Layer1 features s√£o muito gen√©ricas (edges, textures)

### 6.2 Protocolo de Ablation

1. Treinar cada varia√ß√£o (epochs=50)
2. Comparar F1 macro no validation set
3. Selecionar melhor configura√ß√£o
4. Retreinar com 5 seeds diferentes (verificar estabilidade)

---

## 7. Riscos e Mitiga√ß√µes

### Risco 1: Adapters podem n√£o ter capacidade suficiente

**Probabilidade:** M√©dia (30%)  
**Sintoma:** F1 estagna em ~45-47% (n√£o melhora vs frozen)  
**Mitiga√ß√£o:** 
- Aumentar bottleneck_dim (64 ‚Üí 128 ‚Üí 256)
- Testar Adapter em cada sub-bloco (n√£o apenas ap√≥s layer)

### Risco 2: Overfitting (modelo muito pequeno)

**Probabilidade:** Baixa (15%)  
**Sintoma:** Train F1 alto (~60%), Val F1 baixo (~40%)  
**Mitiga√ß√£o:**
- Aumentar dropout (0.1 ‚Üí 0.2)
- Adicionar weight decay (1e-4 ‚Üí 1e-3)
- Data augmentation mais agressivo

### Risco 3: Checkpoint bug persiste

**Probabilidade:** Muito Baixa (5%)  
**Sintoma:** F1 degrada ap√≥s reload (mesmo com valida√ß√£o)  
**Mitiga√ß√£o:**
- Implementar checkpoint validation inline (descrito em 4.3)
- Se persistir: Issue fundamental no PyTorch, reportar bug

---

## 8. Pipeline Evaluation

### 8.1 Ap√≥s Stage 2 com Adapters Funcionar

**Comando:**
```bash
python3 pesquisa_v6/scripts/008_run_pipeline_eval_v6.py \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --stage2-model pesquisa_v6/logs/v6_experiments/stage2_adapters/stage2_model_best.pt \
  --stage3-rect-model pesquisa_v6/logs/test_noise_injection/stage3_rect_robust.pt \
  --stage3-ab-models \
      pesquisa_v6/logs/test_noise_injection_ab/stage3_ab_ensemble_model1.pt \
      pesquisa_v6/logs/test_noise_injection_ab/stage3_ab_ensemble_model2.pt \
      pesquisa_v6/logs/test_noise_injection_ab/stage3_ab_ensemble_model3.pt \
  --output-dir pesquisa_v6/logs/v6_experiments/pipeline_eval_adapters \
  --device cuda
```

**Expectativa:**
- Baseline (Exp 09 com Stage 2 colapsado): 45.86%
- Com Stage 2 adapters (F1=50%): **48-50% accuracy** (+2-4pp)

### 8.2 An√°lise de Cascade Error

**Esperado:** Redu√ß√£o significativa do cascade error

| Specialist | Standalone | Pipeline (Exp 09) | Pipeline (Exp 11A) | Melhoria |
|------------|------------|-------------------|-------------------|----------|
| RECT | 68% | 4% | **40-50%** | +36-46pp |
| AB | 24% | 1.5% | **12-18%** | +10-16pp |

**Raz√£o:** Stage 2 com adapters distribui melhor samples para Stage 3 (n√£o colapsa para RECT 100%)

---

## 9. Contribui√ß√£o para a Tese

### 9.1 Contribui√ß√£o T√©cnica

> "Demonstramos que Residual Adapters (Rebuffi et al., 2017) resolvem negative transfer em hierarchical video codec prediction. Com apenas 0.46% par√¢metros trein√°veis, adapters preservam Stage 1 features (binary NONE vs PARTITION) enquanto adaptam para Stage 2 (3-way SPLIT/RECT/AB), superando fine-tuning frozen em +1.5-6.5pp F1."

### 9.2 Cap√≠tulos da Tese Impactados

**Cap√≠tulo 3 - Arquitetura:**
- Se√ß√£o 3.5: "Parameter-Efficient Transfer Learning via Adapters"
- Compara√ß√£o: Fine-tuning vs Frozen vs Adapters

**Cap√≠tulo 4 - Experimentos:**
- Se√ß√£o 4.4: "Ablation Study - Adapter Configuration"
- An√°lise: Bottleneck dimension, layer placement

**Cap√≠tulo 5 - Resultados:**
- Se√ß√£o 5.2: "Resolu√ß√£o do Negative Transfer Problem"
- Tabela comparativa: ULMFiT vs Frozen vs Adapters

---

## 10. Checklist de Execu√ß√£o

- [ ] **Fase 1: Implementa√ß√£o**
  - [ ] Criar `AdapterModule` em `v6_pipeline/models.py`
  - [ ] Criar `Stage2ModelWithAdapters` em `v6_pipeline/models.py`
  - [ ] Modificar Script 004 (adicionar `--use-adapters`)
  - [ ] Implementar checkpoint validation inline
  - [ ] Testar forward pass (verificar shapes)

- [ ] **Fase 2: Treinamento Baseline (v1)**
  - [ ] Executar treinamento (epochs=50, bottleneck=64)
  - [ ] Monitorar converg√™ncia (esperado: F1 ‚â• 50% em ~30-40 epochs)
  - [ ] Validar checkpoint ap√≥s save
  - [ ] Registrar m√©tricas (F1, accuracy, per-class)

- [ ] **Fase 3: Valida√ß√£o Standalone**
  - [ ] Executar Script 009 (confusion matrix)
  - [ ] Verificar distribui√ß√£o de predi√ß√µes (n√£o colapso)
  - [ ] Comparar com baseline frozen (48.52%)

- [ ] **Fase 4: Pipeline Evaluation**
  - [ ] Executar Script 008 (pipeline completo)
  - [ ] Comparar com Exp 09 baseline (45.86%)
  - [ ] Analisar cascade error (RECT/AB degradation)

- [ ] **Fase 5: Ablation Study**
  - [ ] Treinar varia√ß√£o v2 (adapters apenas layer3-4)
  - [ ] Treinar varia√ß√£o v3 (bottleneck=128)
  - [ ] Treinar varia√ß√£o v4 (sem adapter em layer1)
  - [ ] Selecionar melhor configura√ß√£o

- [ ] **Fase 6: Documenta√ß√£o**
  - [ ] Atualizar este documento com resultados
  - [ ] Criar visualiza√ß√µes (learning curves, confusion matrix)
  - [ ] Escrever se√ß√£o na tese (draft)
  - [ ] Commit e push

---

## 11. Refer√™ncias

1. **Rebuffi et al. (2017)** - "Learning multiple visual domains with residual adapters" - CVPR 2017
2. **Houlsby et al. (2019)** - "Parameter-Efficient Transfer Learning for NLP" - ICML 2019
3. Finn et al. (2017) - "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
4. Howard & Ruder (2018) - "Universal Language Model Fine-tuning" (ULMFiT)
5. Yosinski et al. (2014) - "How transferable are features in deep neural networks?"
6. Kornblith et al. (2019) - "Do Better ImageNet Models Transfer Better?"

---

**Status Atual:** üü° Fase 1 (Implementa√ß√£o) iniciada  
**Pr√≥xima A√ß√£o:** Implementar `AdapterModule` e `Stage2ModelWithAdapters`  
**Tempo Estimado:** 2-3 dias (implementa√ß√£o + treinamento + valida√ß√£o + ablation)  
**Data Atualiza√ß√£o:** 14 de outubro de 2025, 16:15 BRT
