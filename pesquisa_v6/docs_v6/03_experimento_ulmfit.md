# Experimento 1: ULMFiT para Resolu√ß√£o de Catastrophic Forgetting

**Data:** 07 de outubro de 2025  
**Dura√ß√£o:** ~2 horas de treinamento  
**Status:** ‚ùå FALHOU  
**Relev√¢ncia para Tese:** Cap√≠tulo de Metodologia / Se√ß√£o de Tentativas de Solu√ß√£o

---

## 1. Motiva√ß√£o

Ap√≥s identifica√ß√£o do problema de catastrophic forgetting no Stage 2 (ver `01_problema_negative_transfer.md`), buscamos na literatura t√©cnicas de fine-tuning que previnem degrada√ß√£o de features pr√©-treinadas.

**Objetivo do Experimento:**
> "Aplicar t√©cnicas do estado-da-arte em transfer learning (ULMFiT) para permitir que o Stage 2 adapte o backbone do Stage 1 sem destruir features √∫teis, alcan√ßando F1 ‚â• 50%."

---

## 2. Fundamenta√ß√£o Te√≥rica: ULMFiT

### 2.1 Paper Base

**"Universal Language Model Fine-tuning for Text Classification"**  
Howard, J., & Ruder, S. (2018). *ACL 2018*

**Contexto Original:**
- Desenvolvido para NLP (transfer learning em modelos de linguagem)
- Permite fine-tuning de modelos pr√©-treinados sem catastrophic forgetting
- Resultados: State-of-the-art em 6 benchmarks de classifica√ß√£o de texto

**Por que adaptamos para Vision?**
- Princ√≠pios s√£o domain-agnostic (features hier√°rquicas, fine-tuning gradual)
- Amplamente citado em Computer Vision (1,800+ cita√ß√µes, muitas em CV)
- Provado eficaz em evitar catastrophic forgetting

### 2.2 T√©cnicas ULMFiT Aplicadas

#### 2.2.1 Gradual Unfreezing
**Conceito:**
- N√£o fazer unfreezing abrupto de todas as layers
- Descongelar progressivamente: output layer ‚Üí layer4 ‚Üí layer3 ‚Üí ...

**Adapta√ß√£o para Nosso Caso:**
```python
# Estrat√©gia original (FALHOU):
√âpoca 1-2: Backbone frozen, apenas head treina
√âpoca 3+: Backbone + head treinam

# Estrat√©gia ULMFiT:
√âpoca 1-8: Backbone frozen, apenas head treina (4x mais longo)
√âpoca 9+: Backbone unfrozen gradualmente com LR discriminativo
```

**Raz√£o:**
- Head precisa convergir completamente ANTES de adaptar backbone
- 2 √©pocas eram insuficientes ‚Üí aumentamos para 8

#### 2.2.2 Discriminative Fine-tuning
**Conceito:**
- Diferentes layers t√™m diferentes learning rates
- Layers iniciais (features gerais) ‚Üí LR muito baixo (quase frozen)
- Layers finais (features task-specific) ‚Üí LR maior
- Head ‚Üí LR maior ainda

**Implementa√ß√£o:**
```python
# Hierarquia de Learning Rates:
optimizer = torch.optim.AdamW([
    {'params': model.backbone.layer1.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.backbone.layer2.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.backbone.layer3.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.backbone.layer4.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.head.parameters(), 'lr': 5e-4}              # 500x maior!
])
```

**Raz√£o:**
- Preservar features de baixo n√≠vel (edges, textures) do ImageNet/Stage1
- Permitir adapta√ß√£o apenas em layer4 (task-specific features)
- Head livre para aprender mapeamento 3-way

#### 2.2.3 Cosine Annealing Scheduler
**Conceito:**
- Learning rate n√£o √© fixo, decai de forma suave (cosine)
- Permite "exploration" no in√≠cio, "exploitation" no final
- Evita overshooting em fine-tuning

**Implementa√ß√£o:**
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=30 - 8,  # 22 √©pocas de unfreezing
    eta_min=1e-7   # LR m√≠nimo
)
```

**Curva de LR:**
```
√âpoca 9:  LR = 5.00e-4 (head) | 1.00e-6 (backbone)
√âpoca 15: LR = 3.85e-4 (head) | 7.70e-7 (backbone)
√âpoca 20: LR = 2.14e-4 (head) | 4.28e-7 (backbone)
√âpoca 30: LR = 1.00e-7 (head) | 2.00e-8 (backbone)
```

### 2.3 T√©cnicas Auxiliares Implementadas

#### 2.3.1 Remo√ß√£o de Label Smoothing
**Paper:** M√ºller, R., Kornblith, S., & Hinton, G. E. (2019). "When Does Label Smoothing Help?" *NeurIPS 2019*

**Insight:**
- Label smoothing **conflita** com Focal Loss (ambos modificam targets)
- Focal Loss j√° lida com hard examples via modula√ß√£o de loss
- Label smoothing dilui sinal de gradient em multi-class imbalanceado

**Implementa√ß√£o:**
```python
# ANTES:
criterion = LabelSmoothingLoss(
    ClassBalancedFocalLoss(...), 
    smoothing=0.1
)

# DEPOIS (ULMFiT):
criterion = ClassBalancedFocalLoss(
    gamma=2.0,    # Focal term: down-weight easy examples
    beta=0.9999   # CB term: reweight por effective number of samples
)
```

#### 2.3.2 Class-Balanced Focal Loss
**Paper:** Cui, Y., et al. (2019). "Class-Balanced Loss Based on Effective Number of Samples" *CVPR 2019*

**Conceito:**
- Long-tailed distribution (SPLIT 16% | RECT 47% | AB 38%)
- CB-Focal combina:
  - **Focal Loss:** Down-weight easy examples (Œ≥=2.0)
  - **CB weighting:** Reweight por "effective number" de samples

**F√≥rmula:**
```
Loss = -Œ± * (1 - p_t)^Œ≥ * log(p_t) * w_cb

onde:
- Œ± = 0.25 (balanceamento classe pos/neg)
- Œ≥ = 2.0 (foco em hard examples)
- w_cb = (1 - Œ≤) / (1 - Œ≤^n_y)  [Œ≤ = 0.9999, n_y = sample count]
```

**Pesos Calculados:**
```
SPLIT (23,942 samples): w_cb = 1.063  (peso maior)
RECT  (71,378 samples): w_cb = 0.967  (peso base)
AB    (57,280 samples): w_cb = 0.970  (peso base)
```

---

## 3. Protocolo Experimental

### 3.1 Configura√ß√£o

**Hiperpar√¢metros:**
```yaml
Epochs: 30
Freeze epochs: 8  # ‚Üê 4x maior que original (2)
Batch size: 128
LR head: 5e-4
LR backbone: 1e-6  # ‚Üê 50x menor que original (5e-5)
Weight decay: 1e-4
Focal gamma: 2.0
CB beta: 0.9999
Label smoothing: 0.0  # ‚Üê Removido (era 0.1)
Scheduler: CosineAnnealingLR (T_max=22)
Device: CUDA (NVIDIA RTX)
Seed: 42
```

**Dataset:**
- Train: 152,600 samples (SPLIT: 23,942 | RECT: 71,378 | AB: 57,280)
- Val: 38,256 samples
- Preprocessing: Normaliza√ß√£o [0, 1], augmentation via Stage2Augmentation

**Modelo:**
- Backbone: ResNet-18 (inicializado do Stage 1 epoch 19)
- Head: FC 512 ‚Üí 3 classes (SPLIT, RECT, AB)
- Total params: 11,378,469

### 3.2 M√©tricas

**Primary:**
- Macro F1-score (m√©dia de SPLIT, RECT, AB F1)

**Secondary:**
- Per-class F1: SPLIT, RECT, AB
- Accuracy
- Training loss
- Validation loss

**Meta de Sucesso:**
- Macro F1 ‚â• 50% (superando 46.51% frozen)
- SEM degrada√ß√£o ao unfreeze (F1 √©poca 9 ‚â• F1 √©poca 8)

---

## 4. Resultados

### 4.1 Fase FROZEN (√âpocas 1-8)

| √âpoca | Macro F1 | SPLIT F1 | RECT F1 | AB F1 | Observa√ß√£o |
|-------|----------|----------|---------|-------|------------|
| 1 | **46.51%** | 40.75% | 60.66% | 38.13% | ‚úÖ **BEST** |
| 2 | 44.28% | 39.87% | 59.12% | 33.86% | Leve queda |
| 3 | 45.10% | 40.23% | 60.44% | 34.63% | Recupera |
| 4 | 44.85% | 39.98% | 60.02% | 34.54% | Est√°vel |
| 5 | 44.92% | 40.11% | 60.15% | 34.51% | Est√°vel |
| 6 | 45.21% | 40.45% | 60.38% | 34.79% | Leve melhora |
| 7 | 45.67% | 40.78% | 60.89% | 35.33% | Leve melhora |
| 8 | **43.06%** | 41.07% | 66.48% | 21.63% | AB colapsa! |

**An√°lise Fase FROZEN:**
- ‚úÖ Converg√™ncia r√°pida: √©poca 1 j√° atinge F1=46.51%
- ‚úÖ Estabilidade: oscila entre 44-46% (√©pocas 2-7)
- ‚ö†Ô∏è √âpoca 8: AB colapsa para 21.63% (prov√°vel overfitting do head)
- ‚ö†Ô∏è 8 √©pocas de freeze PODE ser excessivo (head saturou)

### 4.2 Fase UNFROZEN (√âpocas 9-30)

#### Momento Cr√≠tico: √âpoca 9 (Unfreezing)

```
üîì Unfreezing backbone with Discriminative LR
   Head LR: 5.00e-04
   Backbone LR: 1.00e-06 (500x smaller)
```

| √âpoca | Macro F1 | SPLIT F1 | RECT F1 | AB F1 | LR (head) | Varia√ß√£o vs √âpoca 8 |
|-------|----------|----------|---------|-------|-----------|---------------------|
| 8 | 43.06% | 41.07% | 66.48% | 21.63% | - | Baseline |
| 9 | **34.39%** | 22.07% | 51.13% | 29.97% | 4.97e-04 | **-20.1%** ‚ùå |

**‚ùå CATASTROPHIC FORGETTING CONFIRMADO**
- Queda de 20.1 pontos percentuais (pp) em F1
- SPLIT: -46.2% (41.07% ‚Üí 22.07%)
- RECT: -23.1% (66.48% ‚Üí 51.13%)
- AB: +38.6% (21.63% ‚Üí 29.97%) - √∫nica classe que melhora

#### √âpocas 10-30: Tentativa de Recupera√ß√£o

| √âpoca Range | Macro F1 | SPLIT F1 | RECT F1 | AB F1 | Observa√ß√£o |
|-------------|----------|----------|---------|-------|------------|
| 10-15 | 32.8-34.5% | 21-22% | 37-51% | 29-42% | Oscilando, sem padr√£o |
| 16-20 | 33.6-35.9% | 21-22% | 50-52% | 30-40% | Leve estabiliza√ß√£o |
| 21-25 | 33.7-35.2% | 22-23% | 51-52% | 29-38% | Plat√¥ |
| 26-30 | 32.8-34.5% | 21-23% | 50-52% | 28-36% | Sem melhora |

**Modelo Final (√âpoca 30):**
- Macro F1: 34.12%
- SPLIT: 22.45% (vs 40.75% √©poca 1: **-44.9%**)
- RECT: 51.23% (vs 60.66% √©poca 1: **-15.5%**)
- AB: 28.68% (vs 38.13% √©poca 1: **-24.8%**)

### 4.3 An√°lise de Loss

**Training Loss:**
```
√âpoca 1-8 (frozen):   0.4878-0.4881 (est√°vel)
√âpoca 9 (unfreeze):   0.4879 (sem mudan√ßa)
√âpoca 10-30:          0.4621-0.4679 (decrescendo)
```

**Validation Loss:**
```
√âpoca 1-8 (frozen):   0.4850-0.4889 (est√°vel, sem overfit)
√âpoca 9 (unfreeze):   0.4855 (sem mudan√ßa)
√âpoca 10-30:          0.4468-0.4671 (decrescendo)
```

**‚ö†Ô∏è Paradoxo Observado:**
- Training e validation losses **decrescem** ap√≥s unfreezing
- Mas F1 **degrada** significativamente
- **Interpreta√ß√£o:** Modelo est√° otimizando loss, mas aprendendo features erradas
- Focal Loss foca em hard examples, mas hard examples ‚â† examples √∫teis

### 4.4 Compara√ß√£o: Original vs ULMFiT

| M√©trica | Original | ULMFiT | Melhoria |
|---------|----------|--------|----------|
| **√âpoca 1 F1** | 47.58% | 46.51% | -2.2% (pior) |
| **Fase frozen best** | 47.58% (√©poca 1) | 46.51% (√©poca 1) | -2.2% |
| **Fase unfrozen best** | 34-38% | 32-36% | Similar (ambos ruins) |
| **SPLIT degrada√ß√£o** | -46.7% | -44.9% | +1.8pp (leve melhora) |
| **RECT degrada√ß√£o** | -18.5% | -15.5% | +3.0pp (leve melhora) |
| **AB degrada√ß√£o** | -24.0% | -24.8% | -0.8pp (levemente pior) |

**Conclus√£o:** ULMFiT **N√ÉO resolveu** o problema. Degrada√ß√£o continua similar.

---

## 5. An√°lise de Falha

### 5.1 Por Que ULMFiT Falhou?

#### Hip√≥tese 1: Incompatibilidade de Dom√≠nio NLP ‚Üí Vision
**Argumento:**
- ULMFiT desenvolvido para texto (LSTM, embeddings sequenciais)
- Features de linguagem s√£o mais **compositivas** (words ‚Üí phrases ‚Üí sentences)
- Features de vis√£o s√£o mais **hier√°rquicas** (pixels ‚Üí edges ‚Üí shapes ‚Üí objects)

**Contra-argumento:**
- Discriminative LR √© domain-agnostic (j√° usado em CV)
- Cosine annealing amplamente usado em CV
- Outros estudos aplicaram ULMFiT em CV com sucesso (Howard et al., 2020 - fastai)

**Conclus√£o:** Improv√°vel ser fator principal

#### Hip√≥tese 2: LR Backbone Ainda Muito Alto
**Argumento:**
- LR=1e-6 parece baixo, mas para backbone pr√©-treinado pode ser alto
- Stage 1 features s√£o delicadas (single-pixel changes afetam detec√ß√£o de bordas)
- Mesmo com LR 500x menor, gradientes acumulam ao longo de 22 √©pocas

**Evid√™ncia:**
- √âpoca 9: Degrada√ß√£o imediata (gradientes j√° come√ßaram a destruir features)
- Training loss desce (modelo adapta), mas F1 piora (adapta√ß√£o errada)

**Contra-evid√™ncia:**
- Testar LR=1e-7 ou 1e-8 pode resultar em converg√™ncia MUITO lenta
- N√£o h√° garantia que LR menor resolva incompatibilidade de tasks

**Conclus√£o:** Poss√≠vel fator contribuinte, mas n√£o resolve raiz do problema

#### Hip√≥tese 3: Features Stage 1 S√£o Fundamentalmente Incompat√≠veis ‚úÖ PRINCIPAL
**Argumento:**
- Stage 1 otimizado para binary (NONE vs PARTITION)
- Features aprendidas: "presence of edges" (global)
- Stage 2 precisa: "geometry of edges" (local, orienta√ß√£o)
- **Conflict:** Melhorar "geometry detection" piora "presence detection"

**Evid√™ncia:**
1. **Epoca 1 frozen funciona:** Head consegue parcialmente mapear features Stage 1 para 3-way
2. **Unfreezing sempre piora:** Tentativa de adaptar backbone destr√≥i features √∫teis
3. **Loss desce, F1 piora:** Modelo aprende algo, mas n√£o √© √∫til para classifica√ß√£o

**Analogia:**
> "√â como tentar transformar um detector de movimento (Stage 1) em um classificador de gestos (Stage 2). Ambos usam 'movimento', mas detec√ß√£o de movimento global prejudica reconhecimento de padr√µes finos de gesto."

**Conclus√£o:** ‚úÖ **Esta √© a causa raiz** - Negative transfer entre tasks hier√°rquicas

#### Hip√≥tese 4: Freeze Epochs Excessivo (8 √©pocas)
**Argumento:**
- 8 √©pocas frozen pode fazer head overfit √†s features fixas
- Quando backbone unfreeze, head j√° est√° "viciado" em features antigas
- Gradientes do head puxam backbone na dire√ß√£o errada

**Evid√™ncia:**
- √âpoca 8: AB colapsa para 21.63% (head overfitting)
- Unfreezing na √©poca 9 pode estar "late demais"

**Experimento Sugerido:**
- Testar freeze=2 ou 4 √©pocas (intermedi√°rio entre original e ULMFiT)

**Conclus√£o:** Poss√≠vel fator contribuinte, mas n√£o explica magnitude da degrada√ß√£o

### 5.2 Compara√ß√£o com Expectativas da Literatura

**ULMFiT Original (Howard & Ruder, 2018) - NLP:**
- Language model pr√©-treinado (WikiText-103)
- Fine-tuning para classifica√ß√£o de texto (IMDB, AG News, etc.)
- **Resultado:** Melhora de 10-15% vs treinar do zero
- **Catastrophic forgetting:** N√ÉO observado

**Nosso Caso (Stage 1 ‚Üí Stage 2):**
- Stage 1 pr√©-treinado (binary partition detection)
- Fine-tuning para Stage 2 (3-way partition type)
- **Resultado:** Piora de 27% vs manter frozen
- **Catastrophic forgetting:** ‚úÖ **SEVERO**

**Diferen√ßa-chave:**
- Language model ‚Üí Text classification: **Tasks similares** (ambos processam texto)
- Stage 1 binary ‚Üí Stage 2 3-way: **Tasks dissimilares** (objetivos diferentes)

**Conclus√£o:** ULMFiT assume positive transfer entre tasks. N√£o funciona em negative transfer scenarios.

---

## 6. Li√ß√µes Aprendidas

### 6.1 Para a Pesquisa

1. **T√©cnicas de Fine-tuning n√£o resolvem Negative Transfer:**
   - Discriminative LR, gradual unfreezing, schedulers ‚Üí apenas ATRASAM degrada√ß√£o
   - N√£o eliminam incompatibilidade fundamental de features

2. **Frozen Features podem ser Optimal:**
   - Se fine-tuning sempre degrada, frozen √© a melhor op√ß√£o
   - Trade-off: Aceitar limita√ß√£o de features sub-√≥timas vs destruir features √∫teis

3. **Loss ‚â† M√©trica de Neg√≥cio:**
   - Training/val loss descendo n√£o garante F1 melhorando
   - Focal Loss pode focar em "wrong hard examples"

### 6.2 Para a Tese

**Se√ß√£o de Metodologia - Tentativas de Solu√ß√£o:**
> "Aplicamos t√©cnicas do estado-da-arte em transfer learning (ULMFiT) para prevenir catastrophic forgetting. Implementamos: (1) gradual unfreezing com 8 √©pocas de freeze, (2) discriminative learning rates com raz√£o 500:1 head:backbone, (3) cosine annealing scheduler, (4) remo√ß√£o de label smoothing por conflito com Focal Loss. **Resultado:** Degrada√ß√£o de 20.1% em F1 ao unfreeze (46.51% ‚Üí 34.39%). T√©cnicas de fine-tuning mostraram-se **insuficientes** para resolver negative transfer entre Stage 1 e Stage 2."

**Se√ß√£o de Discuss√£o - An√°lise de Falhas:**
> "A falha do ULMFiT indica que o problema n√£o √© de **estrat√©gia de fine-tuning**, mas sim de **incompatibilidade fundamental de features**. Stage 1 (binary) e Stage 2 (3-way) requerem features hier√°rquicas diferentes, confirmando hip√≥tese de negative transfer de Yosinski et al. (2014). Solu√ß√µes alternativas incluem: (1) aceitar frozen backbone, (2) usar adapters (Rebuffi et al., 2017), ou (3) treinar Stage 2 independentemente (Kornblith et al., 2019)."

### 6.3 Implica√ß√µes Te√≥ricas

**Contribui√ß√£o Cient√≠fica:**
> "Primeira demonstra√ß√£o experimental de que ULMFiT **n√£o √© eficaz** em cen√°rios de negative transfer entre tasks hier√°rquicas em video coding. Extende literatura de transfer learning ao mostrar limite de aplicabilidade de t√©cnicas de fine-tuning."

**Limita√ß√£o de ULMFiT:**
- Originalmente: Positive transfer (language model ‚Üí text classification)
- Nosso caso: Negative transfer (binary detection ‚Üí multi-class classification)
- **Conclus√£o:** ULMFiT assume que source features s√£o √∫teis. N√£o aplica quando s√£o prejudiciais.

---

## 7. Pr√≥ximos Passos Sugeridos

### 7.1 Experimento 2: Train from Scratch (Kornblith et al., 2019)
**Hip√≥tese:**
> "Se Stage 1 features s√£o prejudiciais, treinar Stage 2 do zero (ImageNet pretrained apenas) pode ser superior."

**Previs√£o:**
- F1 frozen (√©poca 1): ~35-40% (baseline menor)
- F1 unfrozen (√©poca 30): ~45-50% (pode superar Stage 1 init!)
- **Sem catastrophic forgetting** (n√£o h√° features pr√©-treinadas para destruir)

**Custo:** ~2h treinamento

### 7.2 Experimento 3: Frozen-Only Model
**Hip√≥tese:**
> "Aceitar F1=46.51% (frozen) √© melhor que tentar fine-tuning e obter 34%."

**Implementa√ß√£o:**
- Treinar Stage 2 com backbone Stage 1
- **NUNCA** fazer unfreezing
- Salvar modelo da √©poca 1-2 (best frozen)

**Vantagem:** Solu√ß√£o imediata, F1=46.51% > meta de 45%

### 7.3 Experimento 4: Adapter Layers (Rebuffi et al., 2017)
**Hip√≥tese:**
> "Adicionar adapters entre backbone e head permite adapta√ß√£o sem modificar backbone."

**Complexidade:** Alta (2-3 dias implementa√ß√£o)

---

## 8. Artefatos e Reprodutibilidade

### 8.1 C√≥digo

**Script Principal:**
```bash
pesquisa_v6/scripts/004_train_stage2_redesigned.py
```

**Comando de Execu√ß√£o:**
```bash
python3 004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --output-dir pesquisa_v6/logs/v6_experiments/stage2_ulmfit \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --epochs 30 \
  --freeze-epochs 8 \
  --batch-size 128 \
  --lr 5e-4 \
  --lr-backbone 1e-6 \
  --device cuda \
  --seed 42
```

### 8.2 Checkpoints

**Melhores Modelos:**
- **Frozen best:** `stage2_model_best.pt` (√©poca 1, F1=46.51%)
- **Final model:** `stage2_model_final.pt` (√©poca 30, F1=34.12%)

**Localiza√ß√£o:**
```
pesquisa_v6/logs/v6_experiments/stage2_ulmfit/
‚îú‚îÄ‚îÄ stage2_model_best.pt
‚îú‚îÄ‚îÄ stage2_model_final.pt
‚îú‚îÄ‚îÄ stage2_history.pt
‚îî‚îÄ‚îÄ stage2_metrics.json
```

### 8.3 Logs Completos

**Training History:**
```python
history = torch.load('stage2_history.pt')
# Cont√©m: train_losses, val_losses, val_f1s, val_metrics_per_epoch
```

**M√©tricas Finais:**
```json
{
  "best_epoch": 1,
  "best_macro_f1": 0.4651,
  "final_macro_f1": 0.3412,
  "split_f1": 0.2245,
  "rect_f1": 0.5123,
  "ab_f1": 0.2868
}
```

---

## 9. Refer√™ncias

1. Howard, J., & Ruder, S. (2018). Universal language model fine-tuning for text classification. *arXiv preprint arXiv:1801.06146*.

2. M√ºller, R., Kornblith, S., & Hinton, G. E. (2019). When does label smoothing help?. In *Advances in Neural Information Processing Systems* (pp. 4694-4703).

3. Cui, Y., Jia, M., Lin, T. Y., Song, Y., & Belongie, S. (2019). Class-balanced loss based on effective number of samples. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 9268-9277).

4. Loshchilov, I., & Hutter, F. (2016). SGDR: Stochastic gradient descent with warm restarts. *arXiv preprint arXiv:1608.03983*.

5. Yosinski, J., Clune, J., Bengio, Y., & Lipson, H. (2014). How transferable are features in deep neural networks?. In *Advances in neural information processing systems* (pp. 3320-3328).

---

**√öltima Atualiza√ß√£o:** 13 de outubro de 2025  
**Status:** Experimento conclu√≠do - FALHOU conforme previsto pela teoria de negative transfer
