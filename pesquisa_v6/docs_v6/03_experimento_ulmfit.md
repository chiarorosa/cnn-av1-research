# Experimento 1: ULMFiT para ResoluÃ§Ã£o de Catastrophic Forgetting

**Data:** 07 de outubro de 2025  
**DuraÃ§Ã£o:** ~2 horas de treinamento  
**Status:** âŒ FALHOU  
**RelevÃ¢ncia para Tese:** CapÃ­tulo de Metodologia / SeÃ§Ã£o de Tentativas de SoluÃ§Ã£o

---

## 1. MotivaÃ§Ã£o

ApÃ³s identificaÃ§Ã£o do problema de catastrophic forgetting no Stage 2 (ver `01_problema_negative_transfer.md`), buscamos na literatura tÃ©cnicas de fine-tuning que previnem degradaÃ§Ã£o de features prÃ©-treinadas.

**Objetivo do Experimento:**
> "Aplicar tÃ©cnicas do estado-da-arte em transfer learning (ULMFiT) para permitir que o Stage 2 adapte o backbone do Stage 1 sem destruir features Ãºteis, alcanÃ§ando F1 â‰¥ 50%."

---

## 2. FundamentaÃ§Ã£o TeÃ³rica: ULMFiT

### 2.1 Paper Base

**"Universal Language Model Fine-tuning for Text Classification"**  
Howard, J., & Ruder, S. (2018). *ACL 2018*

**Contexto Original:**
- Desenvolvido para NLP (transfer learning em modelos de linguagem)
- Permite fine-tuning de modelos prÃ©-treinados sem catastrophic forgetting
- Resultados: State-of-the-art em 6 benchmarks de classificaÃ§Ã£o de texto

**Por que adaptamos para Vision?**
- PrincÃ­pios sÃ£o domain-agnostic (features hierÃ¡rquicas, fine-tuning gradual)
- Amplamente citado em Computer Vision (1,800+ citaÃ§Ãµes, muitas em CV)
- Provado eficaz em evitar catastrophic forgetting

### 2.2 TÃ©cnicas ULMFiT Aplicadas

#### 2.2.1 Gradual Unfreezing
**Conceito:**
- NÃ£o fazer unfreezing abrupto de todas as layers
- Descongelar progressivamente: output layer â†’ layer4 â†’ layer3 â†’ ...

**AdaptaÃ§Ã£o para Nosso Caso:**
```python
# EstratÃ©gia original (FALHOU):
Ã‰poca 1-2: Backbone frozen, apenas head treina
Ã‰poca 3+: Backbone + head treinam

# EstratÃ©gia ULMFiT:
Ã‰poca 1-8: Backbone frozen, apenas head treina (4x mais longo)
Ã‰poca 9+: Backbone unfrozen gradualmente com LR discriminativo
```

**RazÃ£o:**
- Head precisa convergir completamente ANTES de adaptar backbone
- 2 Ã©pocas eram insuficientes â†’ aumentamos para 8

#### 2.2.2 Discriminative Fine-tuning
**Conceito:**
- Diferentes layers tÃªm diferentes learning rates
- Layers iniciais (features gerais) â†’ LR muito baixo (quase frozen)
- Layers finais (features task-specific) â†’ LR maior
- Head â†’ LR maior ainda

**ImplementaÃ§Ã£o:**
```python
# Hierarquia de Learning Rates:
optimizer = torch.optim.AdamW([
    {'params': model.backbone.layer1.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.backbone.layer2.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.backbone.layer3.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.backbone.layer4.parameters(), 'lr': 1e-6},  # Quase frozen
    {'params': model.head.parameters(), 'lr': 5e-4}              # 500x maior!
])
```

**RazÃ£o:**
- Preservar features de baixo nÃ­vel (edges, textures) do ImageNet/Stage1
- Permitir adaptaÃ§Ã£o apenas em layer4 (task-specific features)
- Head livre para aprender mapeamento 3-way

#### 2.2.3 Cosine Annealing Scheduler
**Conceito:**
- Learning rate nÃ£o Ã© fixo, decai de forma suave (cosine)
- Permite "exploration" no inÃ­cio, "exploitation" no final
- Evita overshooting em fine-tuning

**ImplementaÃ§Ã£o:**
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=30 - 8,  # 22 Ã©pocas de unfreezing
    eta_min=1e-7   # LR mÃ­nimo
)
```

**Curva de LR:**
```
Ã‰poca 9:  LR = 5.00e-4 (head) | 1.00e-6 (backbone)
Ã‰poca 15: LR = 3.85e-4 (head) | 7.70e-7 (backbone)
Ã‰poca 20: LR = 2.14e-4 (head) | 4.28e-7 (backbone)
Ã‰poca 30: LR = 1.00e-7 (head) | 2.00e-8 (backbone)
```

### 2.3 TÃ©cnicas Auxiliares Implementadas

#### 2.3.1 RemoÃ§Ã£o de Label Smoothing
**Paper:** MÃ¼ller, R., Kornblith, S., & Hinton, G. E. (2019). "When Does Label Smoothing Help?" *NeurIPS 2019*

**Insight:**
- Label smoothing **conflita** com Focal Loss (ambos modificam targets)
- Focal Loss jÃ¡ lida com hard examples via modulaÃ§Ã£o de loss
- Label smoothing dilui sinal de gradient em multi-class imbalanceado

**ImplementaÃ§Ã£o:**
```python
# ANTES:
criterion = LabelSmoothingLoss(
    ClassBalancedFocalLoss(...), 
    smoothing=0.1
)

# DEPOIS (ULMFiT):
criterion = ClassBalancedFocalLoss(
    gamma=2.0,    # Focal term: down-weight easy examples
    beta=0.9999   # CB term: reweight por effective number of samples
)
```

#### 2.3.2 Class-Balanced Focal Loss
**Paper:** Cui, Y., et al. (2019). "Class-Balanced Loss Based on Effective Number of Samples" *CVPR 2019*

**Conceito:**
- Long-tailed distribution (SPLIT 16% | RECT 47% | AB 38%)
- CB-Focal combina:
  - **Focal Loss:** Down-weight easy examples (Î³=2.0)
  - **CB weighting:** Reweight por "effective number" de samples

**FÃ³rmula:**
```
Loss = -Î± * (1 - p_t)^Î³ * log(p_t) * w_cb

onde:
- Î± = 0.25 (balanceamento classe pos/neg)
- Î³ = 2.0 (foco em hard examples)
- w_cb = (1 - Î²) / (1 - Î²^n_y)  [Î² = 0.9999, n_y = sample count]
```

**Pesos Calculados:**
```
SPLIT (23,942 samples): w_cb = 1.063  (peso maior)
RECT  (71,378 samples): w_cb = 0.967  (peso base)
AB    (57,280 samples): w_cb = 0.970  (peso base)
```

---

## 3. Protocolo Experimental

### 3.1 ConfiguraÃ§Ã£o

**HiperparÃ¢metros:**
```yaml
Epochs: 30
Freeze epochs: 8  # â† 4x maior que original (2)
Batch size: 128
LR head: 5e-4
LR backbone: 1e-6  # â† 50x menor que original (5e-5)
Weight decay: 1e-4
Focal gamma: 2.0
CB beta: 0.9999
Label smoothing: 0.0  # â† Removido (era 0.1)
Scheduler: CosineAnnealingLR (T_max=22)
Device: CUDA (NVIDIA RTX)
Seed: 42
```

**Dataset:**
- Train: 152,600 samples (SPLIT: 23,942 | RECT: 71,378 | AB: 57,280)
- Val: 38,256 samples
- Preprocessing: NormalizaÃ§Ã£o [0, 1], augmentation via Stage2Augmentation

**Modelo:**
- Backbone: ResNet-18 (inicializado do Stage 1 epoch 19)
- Head: FC 512 â†’ 3 classes (SPLIT, RECT, AB)
- Total params: 11,378,469

### 3.2 MÃ©tricas

**Primary:**
- Macro F1-score (mÃ©dia de SPLIT, RECT, AB F1)

**Secondary:**
- Per-class F1: SPLIT, RECT, AB
- Accuracy
- Training loss
- Validation loss

**Meta de Sucesso:**
- Macro F1 â‰¥ 50% (superando 46.51% frozen)
- SEM degradaÃ§Ã£o ao unfreeze (F1 Ã©poca 9 â‰¥ F1 Ã©poca 8)

---

## 4. Resultados

### 4.1 Fase FROZEN (Ã‰pocas 1-8)

| Ã‰poca | Macro F1 | SPLIT F1 | RECT F1 | AB F1 | ObservaÃ§Ã£o |
|-------|----------|----------|---------|-------|------------|
| 1 | **46.51%** | 40.75% | 60.66% | 38.13% | âœ… **BEST** |
| 2 | 44.28% | 39.87% | 59.12% | 33.86% | Leve queda |
| 3 | 45.10% | 40.23% | 60.44% | 34.63% | Recupera |
| 4 | 44.85% | 39.98% | 60.02% | 34.54% | EstÃ¡vel |
| 5 | 44.92% | 40.11% | 60.15% | 34.51% | EstÃ¡vel |
| 6 | 45.21% | 40.45% | 60.38% | 34.79% | Leve melhora |
| 7 | 45.67% | 40.78% | 60.89% | 35.33% | Leve melhora |
| 8 | **43.06%** | 41.07% | 66.48% | 21.63% | AB colapsa! |

**AnÃ¡lise Fase FROZEN:**
- âœ… ConvergÃªncia rÃ¡pida: Ã©poca 1 jÃ¡ atinge F1=46.51%
- âœ… Estabilidade: oscila entre 44-46% (Ã©pocas 2-7)
- âš ï¸ Ã‰poca 8: AB colapsa para 21.63% (provÃ¡vel overfitting do head)
- âš ï¸ 8 Ã©pocas de freeze PODE ser excessivo (head saturou)

### 4.2 Fase UNFROZEN (Ã‰pocas 9-30)

#### Momento CrÃ­tico: Ã‰poca 9 (Unfreezing)

```
ðŸ”“ Unfreezing backbone with Discriminative LR
   Head LR: 5.00e-04
   Backbone LR: 1.00e-06 (500x smaller)
```

| Ã‰poca | Macro F1 | SPLIT F1 | RECT F1 | AB F1 | LR (head) | VariaÃ§Ã£o vs Ã‰poca 8 |
|-------|----------|----------|---------|-------|-----------|---------------------|
| 8 | 43.06% | 41.07% | 66.48% | 21.63% | - | Baseline |
| 9 | **34.39%** | 22.07% | 51.13% | 29.97% | 4.97e-04 | **-20.1%** âŒ |

**âŒ CATASTROPHIC FORGETTING CONFIRMADO**
- Queda de 20.1 pontos percentuais (pp) em F1
- SPLIT: -46.2% (41.07% â†’ 22.07%)
- RECT: -23.1% (66.48% â†’ 51.13%)
- AB: +38.6% (21.63% â†’ 29.97%) - Ãºnica classe que melhora

#### Ã‰pocas 10-30: Tentativa de RecuperaÃ§Ã£o

| Ã‰poca Range | Macro F1 | SPLIT F1 | RECT F1 | AB F1 | ObservaÃ§Ã£o |
|-------------|----------|----------|---------|-------|------------|
| 10-15 | 32.8-34.5% | 21-22% | 37-51% | 29-42% | Oscilando, sem padrÃ£o |
| 16-20 | 33.6-35.9% | 21-22% | 50-52% | 30-40% | Leve estabilizaÃ§Ã£o |
| 21-25 | 33.7-35.2% | 22-23% | 51-52% | 29-38% | PlatÃ´ |
| 26-30 | 32.8-34.5% | 21-23% | 50-52% | 28-36% | Sem melhora |

**Modelo Final (Ã‰poca 30):**
- Macro F1: 34.12%
- SPLIT: 22.45% (vs 40.75% Ã©poca 1: **-44.9%**)
- RECT: 51.23% (vs 60.66% Ã©poca 1: **-15.5%**)
- AB: 28.68% (vs 38.13% Ã©poca 1: **-24.8%**)

### 4.3 AnÃ¡lise de Loss

**Training Loss:**
```
Ã‰poca 1-8 (frozen):   0.4878-0.4881 (estÃ¡vel)
Ã‰poca 9 (unfreeze):   0.4879 (sem mudanÃ§a)
Ã‰poca 10-30:          0.4621-0.4679 (decrescendo)
```

**Validation Loss:**
```
Ã‰poca 1-8 (frozen):   0.4850-0.4889 (estÃ¡vel, sem overfit)
Ã‰poca 9 (unfreeze):   0.4855 (sem mudanÃ§a)
Ã‰poca 10-30:          0.4468-0.4671 (decrescendo)
```

**âš ï¸ Paradoxo Observado:**
- Training e validation losses **decrescem** apÃ³s unfreezing
- Mas F1 **degrada** significativamente
- **InterpretaÃ§Ã£o:** Modelo estÃ¡ otimizando loss, mas aprendendo features erradas
- Focal Loss foca em hard examples, mas hard examples â‰  examples Ãºteis

### 4.4 ComparaÃ§Ã£o: Original vs ULMFiT

| MÃ©trica | Original | ULMFiT | Melhoria |
|---------|----------|--------|----------|
| **Ã‰poca 1 F1** | 47.58% | 46.51% | -2.2% (pior) |
| **Fase frozen best** | 47.58% (Ã©poca 1) | 46.51% (Ã©poca 1) | -2.2% |
| **Fase unfrozen best** | 34-38% | 32-36% | Similar (ambos ruins) |
| **SPLIT degradaÃ§Ã£o** | -46.7% | -44.9% | +1.8pp (leve melhora) |
| **RECT degradaÃ§Ã£o** | -18.5% | -15.5% | +3.0pp (leve melhora) |
| **AB degradaÃ§Ã£o** | -24.0% | -24.8% | -0.8pp (levemente pior) |

**ConclusÃ£o:** ULMFiT **NÃƒO resolveu** o problema. DegradaÃ§Ã£o continua similar.

---

## 5. AnÃ¡lise de Falha

### 5.1 Por Que ULMFiT Falhou?

#### HipÃ³tese 1: Incompatibilidade de DomÃ­nio NLP â†’ Vision
**Argumento:**
- ULMFiT desenvolvido para texto (LSTM, embeddings sequenciais)
- Features de linguagem sÃ£o mais **compositivas** (words â†’ phrases â†’ sentences)
- Features de visÃ£o sÃ£o mais **hierÃ¡rquicas** (pixels â†’ edges â†’ shapes â†’ objects)

**Contra-argumento:**
- Discriminative LR Ã© domain-agnostic (jÃ¡ usado em CV)
- Cosine annealing amplamente usado em CV
- Outros estudos aplicaram ULMFiT em CV com sucesso (Howard et al., 2020 - fastai)

**ConclusÃ£o:** ImprovÃ¡vel ser fator principal

#### HipÃ³tese 2: LR Backbone Ainda Muito Alto
**Argumento:**
- LR=1e-6 parece baixo, mas para backbone prÃ©-treinado pode ser alto
- Stage 1 features sÃ£o delicadas (single-pixel changes afetam detecÃ§Ã£o de bordas)
- Mesmo com LR 500x menor, gradientes acumulam ao longo de 22 Ã©pocas

**EvidÃªncia:**
- Ã‰poca 9: DegradaÃ§Ã£o imediata (gradientes jÃ¡ comeÃ§aram a destruir features)
- Training loss desce (modelo adapta), mas F1 piora (adaptaÃ§Ã£o errada)

**Contra-evidÃªncia:**
- Testar LR=1e-7 ou 1e-8 pode resultar em convergÃªncia MUITO lenta
- NÃ£o hÃ¡ garantia que LR menor resolva incompatibilidade de tasks

**ConclusÃ£o:** PossÃ­vel fator contribuinte, mas nÃ£o resolve raiz do problema

#### HipÃ³tese 3: Features Stage 1 SÃ£o Fundamentalmente IncompatÃ­veis âœ… PRINCIPAL
**Argumento:**
- Stage 1 otimizado para binary (NONE vs PARTITION)
- Features aprendidas: "presence of edges" (global)
- Stage 2 precisa: "geometry of edges" (local, orientaÃ§Ã£o)
- **Conflict:** Melhorar "geometry detection" piora "presence detection"

**EvidÃªncia:**
1. **Epoca 1 frozen funciona:** Head consegue parcialmente mapear features Stage 1 para 3-way
2. **Unfreezing sempre piora:** Tentativa de adaptar backbone destrÃ³i features Ãºteis
3. **Loss desce, F1 piora:** Modelo aprende algo, mas nÃ£o Ã© Ãºtil para classificaÃ§Ã£o

**Analogia:**
> "Ã‰ como tentar transformar um detector de movimento (Stage 1) em um classificador de gestos (Stage 2). Ambos usam 'movimento', mas detecÃ§Ã£o de movimento global prejudica reconhecimento de padrÃµes finos de gesto."

**ConclusÃ£o:** âœ… **Esta Ã© a causa raiz** - Negative transfer entre tasks hierÃ¡rquicas

#### HipÃ³tese 4: Freeze Epochs Excessivo (8 Ã©pocas)
**Argumento:**
- 8 Ã©pocas frozen pode fazer head overfit Ã s features fixas
- Quando backbone unfreeze, head jÃ¡ estÃ¡ "viciado" em features antigas
- Gradientes do head puxam backbone na direÃ§Ã£o errada

**EvidÃªncia:**
- Ã‰poca 8: AB colapsa para 21.63% (head overfitting)
- Unfreezing na Ã©poca 9 pode estar "late demais"

**Experimento Sugerido:**
- Testar freeze=2 ou 4 Ã©pocas (intermediÃ¡rio entre original e ULMFiT)

**ConclusÃ£o:** PossÃ­vel fator contribuinte, mas nÃ£o explica magnitude da degradaÃ§Ã£o

### 5.2 ComparaÃ§Ã£o com Expectativas da Literatura

**ULMFiT Original (Howard & Ruder, 2018) - NLP:**
- Language model prÃ©-treinado (WikiText-103)
- Fine-tuning para classificaÃ§Ã£o de texto (IMDB, AG News, etc.)
- **Resultado:** Melhora de 10-15% vs treinar do zero
- **Catastrophic forgetting:** NÃƒO observado

**Nosso Caso (Stage 1 â†’ Stage 2):**
- Stage 1 prÃ©-treinado (binary partition detection)
- Fine-tuning para Stage 2 (3-way partition type)
- **Resultado:** Piora de 27% vs manter frozen
- **Catastrophic forgetting:** âœ… **SEVERO**

**DiferenÃ§a-chave:**
- Language model â†’ Text classification: **Tasks similares** (ambos processam texto)
- Stage 1 binary â†’ Stage 2 3-way: **Tasks dissimilares** (objetivos diferentes)

**ConclusÃ£o:** ULMFiT assume positive transfer entre tasks. NÃ£o funciona em negative transfer scenarios.

---

## 6. LiÃ§Ãµes Aprendidas

### 6.1 Para a Pesquisa

1. **TÃ©cnicas de Fine-tuning nÃ£o resolvem Negative Transfer:**
   - Discriminative LR, gradual unfreezing, schedulers â†’ apenas ATRASAM degradaÃ§Ã£o
   - NÃ£o eliminam incompatibilidade fundamental de features

2. **Frozen Features podem ser Optimal:**
   - Se fine-tuning sempre degrada, frozen Ã© a melhor opÃ§Ã£o
   - Trade-off: Aceitar limitaÃ§Ã£o de features sub-Ã³timas vs destruir features Ãºteis

3. **Loss â‰  MÃ©trica de NegÃ³cio:**
   - Training/val loss descendo nÃ£o garante F1 melhorando
   - Focal Loss pode focar em "wrong hard examples"

### 6.2 Para a Tese

**SeÃ§Ã£o de Metodologia - Tentativas de SoluÃ§Ã£o:**
> "Aplicamos tÃ©cnicas do estado-da-arte em transfer learning (ULMFiT) para prevenir catastrophic forgetting. Implementamos: (1) gradual unfreezing com 8 Ã©pocas de freeze, (2) discriminative learning rates com razÃ£o 500:1 head:backbone, (3) cosine annealing scheduler, (4) remoÃ§Ã£o de label smoothing por conflito com Focal Loss. **Resultado:** DegradaÃ§Ã£o de 20.1% em F1 ao unfreeze (46.51% â†’ 34.39%). TÃ©cnicas de fine-tuning mostraram-se **insuficientes** para resolver negative transfer entre Stage 1 e Stage 2."

**SeÃ§Ã£o de DiscussÃ£o - AnÃ¡lise de Falhas:**
> "A falha do ULMFiT indica que o problema nÃ£o Ã© de **estratÃ©gia de fine-tuning**, mas sim de **incompatibilidade fundamental de features**. Stage 1 (binary) e Stage 2 (3-way) requerem features hierÃ¡rquicas diferentes, confirmando hipÃ³tese de negative transfer de Yosinski et al. (2014). SoluÃ§Ãµes alternativas incluem: (1) aceitar frozen backbone, (2) usar adapters (Rebuffi et al., 2017), ou (3) treinar Stage 2 independentemente (Kornblith et al., 2019)."

### 6.3 ImplicaÃ§Ãµes TeÃ³ricas

**ContribuiÃ§Ã£o CientÃ­fica:**
> "Primeira demonstraÃ§Ã£o experimental de que ULMFiT **nÃ£o Ã© eficaz** em cenÃ¡rios de negative transfer entre tasks hierÃ¡rquicas em video coding. Extende literatura de transfer learning ao mostrar limite de aplicabilidade de tÃ©cnicas de fine-tuning."

**LimitaÃ§Ã£o de ULMFiT:**
- Originalmente: Positive transfer (language model â†’ text classification)
- Nosso caso: Negative transfer (binary detection â†’ multi-class classification)
- **ConclusÃ£o:** ULMFiT assume que source features sÃ£o Ãºteis. NÃ£o aplica quando sÃ£o prejudiciais.

---

## 7. PrÃ³ximos Passos Sugeridos

### 7.1 Experimento 2: Train from Scratch (Kornblith et al., 2019)
**HipÃ³tese:**
> "Se Stage 1 features sÃ£o prejudiciais, treinar Stage 2 do zero (ImageNet pretrained apenas) pode ser superior."

**PrevisÃ£o:**
- F1 frozen (Ã©poca 1): ~35-40% (baseline menor)
- F1 unfrozen (Ã©poca 30): ~45-50% (pode superar Stage 1 init!)
- **Sem catastrophic forgetting** (nÃ£o hÃ¡ features prÃ©-treinadas para destruir)

**Custo:** ~2h treinamento

### 7.2 Experimento 3: Frozen-Only Model
**HipÃ³tese:**
> "Aceitar F1=46.51% (frozen) Ã© melhor que tentar fine-tuning e obter 34%."

**ImplementaÃ§Ã£o:**
- Treinar Stage 2 com backbone Stage 1
- **NUNCA** fazer unfreezing
- Salvar modelo da Ã©poca 1-2 (best frozen)

**Vantagem:** SoluÃ§Ã£o imediata, F1=46.51% > meta de 45%

### 7.3 Experimento 4: Adapter Layers (Rebuffi et al., 2017)
**HipÃ³tese:**
> "Adicionar adapters entre backbone e head permite adaptaÃ§Ã£o sem modificar backbone."

**Complexidade:** Alta (2-3 dias implementaÃ§Ã£o)

---

## 8. Artefatos e Reprodutibilidade

### 8.1 CÃ³digo

**Script Principal:**
```bash
pesquisa_v6/scripts/004_train_stage2_redesigned.py
```

**Comando de ExecuÃ§Ã£o:**
```bash
python3 004_train_stage2_redesigned.py \
  --dataset-dir pesquisa_v6/v6_dataset/block_16 \
  --output-dir pesquisa_v6/logs/v6_experiments/stage2_ulmfit \
  --stage1-model pesquisa_v6/logs/v6_experiments/stage1/stage1_model_best.pt \
  --epochs 30 \
  --freeze-epochs 8 \
  --batch-size 128 \
  --lr 5e-4 \
  --lr-backbone 1e-6 \
  --device cuda \
  --seed 42
```

### 8.2 Checkpoints

**Melhores Modelos:**
- **Frozen best:** `stage2_model_best.pt` (Ã©poca 1, F1=46.51%)
- **Final model:** `stage2_model_final.pt` (Ã©poca 30, F1=34.12%)

**LocalizaÃ§Ã£o:**
```
pesquisa_v6/logs/v6_experiments/stage2_ulmfit/
â”œâ”€â”€ stage2_model_best.pt
â”œâ”€â”€ stage2_model_final.pt
â”œâ”€â”€ stage2_history.pt
â””â”€â”€ stage2_metrics.json
```

### 8.3 Logs Completos

**Training History:**
```python
history = torch.load('stage2_history.pt')
# ContÃ©m: train_losses, val_losses, val_f1s, val_metrics_per_epoch
```

**MÃ©tricas Finais:**
```json
{
  "best_epoch": 1,
  "best_macro_f1": 0.4651,
  "final_macro_f1": 0.3412,
  "split_f1": 0.2245,
  "rect_f1": 0.5123,
  "ab_f1": 0.2868
}
```

---

## 9. ReferÃªncias

1. Howard, J., & Ruder, S. (2018). Universal language model fine-tuning for text classification. *arXiv preprint arXiv:1801.06146*.

2. MÃ¼ller, R., Kornblith, S., & Hinton, G. E. (2019). When does label smoothing help?. In *Advances in Neural Information Processing Systems* (pp. 4694-4703).

3. Cui, Y., Jia, M., Lin, T. Y., Song, Y., & Belongie, S. (2019). Class-balanced loss based on effective number of samples. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 9268-9277).

4. Loshchilov, I., & Hutter, F. (2016). SGDR: Stochastic gradient descent with warm restarts. *arXiv preprint arXiv:1608.03983*.

5. Yosinski, J., Clune, J., Bengio, Y., & Lipson, H. (2014). How transferable are features in deep neural networks?. In *Advances in neural information processing systems* (pp. 3320-3328).

---

**Ãšltima AtualizaÃ§Ã£o:** 13 de outubro de 2025  
**Status:** Experimento concluÃ­do - FALHOU conforme previsto pela teoria de negative transfer
