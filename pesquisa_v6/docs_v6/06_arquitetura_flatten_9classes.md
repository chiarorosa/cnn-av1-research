# Arquitetura Flatten: Stage 2 com 9 Classes Direto

**Data:** 13 de outubro de 2025  
**Branch:** `feat/stage2-flatten-9classes`  
**Status:** üîÑ EM DESENVOLVIMENTO  
**Motiva√ß√£o:** Resolver degrada√ß√£o catastr√≥fica Stage 3 (-95%)

---

## 1. Motiva√ß√£o: Por Que Redesenhar?

### 1.1 Problema Identificado

**Pipeline V6 Hier√°rquico (3 Stages) FALHOU:**

```
Performance Atual:
- Overall Accuracy: 47.66%
- Stage 3-RECT degrada√ß√£o: 68.44% ‚Üí 4.49% (-93.4%)
- Stage 3-AB degrada√ß√£o: 24.50% ‚Üí 1.51% (-93.8%)
- Classes colapsadas: HORZ, HORZ_A, VERT_A, VERT_B (F1=0%)
```

**Root Cause:**
> Stage 2 (F1=37.38%) envia **62% de samples errados** para Stage 3.  
> Stage 3 treinou com distribui√ß√£o limpa ‚Üí n√£o generaliza para lixo ‚Üí colapso total.

### 1.2 Li√ß√£o da Literatura

**He et al. (2019) - "Rethinking ImageNet Pre-training":**
> "Simpler architectures often outperform complex ones when properly trained."

**Kornblith et al. (2019) - "Do Better ImageNet Models Transfer Better?":**
> "Generic ImageNet features can directly classify fine-grained categories without task-specific hierarchies."

**Aplica√ß√£o:**
- ImageNet features capturam edges, shapes, textures
- HORZ vs VERT vs SPLIT vs AB s√£o padr√µes geom√©tricos
- **Hip√≥tese:** ResNet-18 pode classificar 9 classes diretamente

---

## 2. Arquitetura Proposta: Flatten

### 2.1 Compara√ß√£o: Hier√°rquica vs Flatten

#### Arquitetura Atual (Hier√°rquica - V6)
```
Input: 16√ó16 block
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Stage 1: NONE vs PARTITION          ‚îÇ
‚îÇ Accuracy: 72.79%                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚îú‚îÄ NONE ‚Üí Output (0)
    ‚îî‚îÄ PARTITION ‚Üí Stage 2
                   ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Stage 2: SPLIT vs RECT vs AB            ‚îÇ
    ‚îÇ F1: 37.38% (Train from Scratch)         ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
        ‚îú‚îÄ SPLIT ‚Üí Output (3)
        ‚îú‚îÄ RECT ‚Üí Stage 3-RECT
        ‚îÇ         ‚Üì
        ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    ‚îÇ HORZ vs VERT         ‚îÇ
        ‚îÇ    ‚îÇ Standalone: 68.44%   ‚îÇ
        ‚îÇ    ‚îÇ Pipeline: 4.49% ‚ùå   ‚îÇ
        ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ         ‚Üì
        ‚îÇ    Output (1 ou 2)
        ‚îÇ
        ‚îî‚îÄ AB ‚Üí Stage 3-AB
                ‚Üì
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ 4-way AB               ‚îÇ
            ‚îÇ Standalone: 24.50%     ‚îÇ
            ‚îÇ Pipeline: 1.51% ‚ùå     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
            Output (4, 5, 6, 7)

Pipeline Accuracy: 47.66%
Cascade Error: -95% em Stage 3
```

#### Arquitetura Nova (Flatten)
```
Input: 16√ó16 block
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Stage 1: NONE vs PARTITION          ‚îÇ
‚îÇ Accuracy: 72.79% (mant√©m)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚îú‚îÄ NONE ‚Üí Output PARTITION_NONE (0)
    ‚îî‚îÄ PARTITION ‚Üí Stage 2 FLAT
                   ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Stage 2 FLAT: 9 classes direto          ‚îÇ
    ‚îÇ                                          ‚îÇ
    ‚îÇ Classes:                                 ‚îÇ
    ‚îÇ  1. PARTITION_HORZ                       ‚îÇ
    ‚îÇ  2. PARTITION_VERT                       ‚îÇ
    ‚îÇ  3. PARTITION_SPLIT                      ‚îÇ
    ‚îÇ  4. PARTITION_HORZ_A                     ‚îÇ
    ‚îÇ  5. PARTITION_HORZ_B                     ‚îÇ
    ‚îÇ  6. PARTITION_VERT_A                     ‚îÇ
    ‚îÇ  7. PARTITION_VERT_B                     ‚îÇ
    ‚îÇ  8. PARTITION_HORZ_4                     ‚îÇ
    ‚îÇ  9. PARTITION_VERT_4                     ‚îÇ
    ‚îÇ                                          ‚îÇ
    ‚îÇ T√©cnicas:                                ‚îÇ
    ‚îÇ - CB-Focal Loss (Œ≤=0.9999, Œ≥=2.5)       ‚îÇ
    ‚îÇ - Balanced Sampler                       ‚îÇ
    ‚îÇ - Strong Augmentation (MixUp, CutMix)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
    Output (1-9) diretamente

‚ùå REMOVE Stage 3 completamente
‚úÖ Elimina erro em cascata
```

### 2.2 Vantagens da Arquitetura Flatten

#### Vantagem 1: Elimina Erro em Cascata
```
Hier√°rquica: Accuracy = Stage1 √ó Stage2 √ó Stage3
           = 0.73 √ó 0.37 √ó 0.68 = 18.4% te√≥rico
           Real: 47.66% (apenas porque NONE domina)

Flatten:     Accuracy = Stage1 √ó Stage2_flat
           = 0.73 √ó 0.55 = 40% te√≥rico
           Com NONE: esperado ~52-55%
```

#### Vantagem 2: Feature Learning Conjunto
- **Problema Hier√°rquico:** Stage 2 aprende "RECT vs AB" mas n√£o "HORZ vs VERT"
  - Features n√£o otimizadas para distin√ß√£o fina
- **Solu√ß√£o Flatten:** Aprende todas 9 classes juntas
  - Backbone otimiza features para HORZ, VERT, AB simultaneamente
  - Multi-task learning impl√≠cito

#### Vantagem 3: Simplicidade
- 2 stages ao inv√©s de 3-4
- Menos checkpoints, menos debugging
- Mais f√°cil de manter e explicar na tese

#### Vantagem 4: Robustez
- N√£o depende de routing correto Stage 2‚Üí3
- Cada predi√ß√£o √© independente
- Menos pontos de falha

---

## 3. Desafios e Solu√ß√µes

### 3.1 Desafio: Class Imbalance Extremo

**Distribui√ß√£o Dataset V6 (90,793 validation samples):**

| Classe | Count | % | Ratio vs Smallest |
|--------|-------|---|-------------------|
| **PARTITION_HORZ** | 9,618 | 25.2% | 96√ó |
| **PARTITION_VERT** | 5,962 | 15.6% | 60√ó |
| **PARTITION_SPLIT** | 8,147 | 21.3% | 81√ó |
| PARTITION_HORZ_A | 3,628 | 9.5% | 36√ó |
| PARTITION_HORZ_B | 3,537 | 9.3% | 35√ó |
| PARTITION_VERT_A | 3,794 | 9.9% | 38√ó |
| PARTITION_VERT_B | 3,570 | 9.4% | 36√ó |
| **PARTITION_HORZ_4** | **100** | **0.3%** | **1√ó** |
| **PARTITION_VERT_4** | **88** | **0.2%** | **0.88√ó** |

**Problema:**
- HORZ_4 e VERT_4 t√™m apenas ~100 samples!
- Ratio 96:1 (HORZ vs HORZ_4)
- Modelo pode colapsar para classes majorit√°rias

**Solu√ß√£o 1: Class-Balanced Focal Loss (CB-Focal)**

```python
from pesquisa_v6.v6_pipeline.losses import ClassBalancedFocalLoss

# Effective number of samples (Cui et al., 2019)
criterion = ClassBalancedFocalLoss(
    samples_per_class=[9618, 5962, 8147, 3628, 3537, 3794, 3570, 100, 88],
    num_classes=9,
    beta=0.9999,  # Muito pr√≥ximo de 1 para imbalance extremo
    gamma=2.5,    # Focal maior para hard examples
    alpha=0.25
)
```

**Teoria (Cui et al., 2019):**
```
Effective number: E_n = (1 - Œ≤^n) / (1 - Œ≤)

Para Œ≤=0.9999:
  HORZ:    E_9618 = 9617.0 (quase linear)
  HORZ_4:  E_100  = 99.0
  
Weight: w_i = (1 - Œ≤) / E_i
  w_HORZ   = 0.000104
  w_HORZ_4 = 0.010100  (97√ó maior!)
```

**Solu√ß√£o 2: Balanced Sampler**

```python
from torch.utils.data import WeightedRandomSampler

# Calcular pesos inversos
class_counts = [9618, 5962, 8147, 3628, 3537, 3794, 3570, 100, 88]
class_weights = [1.0 / c for c in class_counts]

# Atribuir peso a cada sample
sample_weights = [class_weights[label] for label in dataset.labels]

# Criar sampler
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(dataset),
    replacement=True  # Permite oversampling de minorit√°rias
)

dataloader = DataLoader(dataset, batch_size=128, sampler=sampler)
```

**Resultado Esperado:**
- Cada batch tem distribui√ß√£o aproximadamente uniforme
- HORZ_4 e VERT_4 aparecem ~10-15% dos batches (oversample)

---

### 3.2 Desafio: HORZ_4 e VERT_4 Muito Raras

**Problema:**
- Apenas 100 e 88 samples para treinar
- Risco de overfitting
- Pode n√£o generalizar

**Solu√ß√£o 1: Strong Data Augmentation**

```python
from pesquisa_v6.v6_pipeline.augmentation import get_strong_augmentation

transform_train = get_strong_augmentation(
    mixup_alpha=0.5,      # MixUp forte
    mixup_prob=0.4,
    cutmix_beta=1.0,      # CutMix
    cutmix_prob=0.5,
    geometric=True,       # Flips, rotations
)
```

**MixUp/CutMix cria samples sint√©ticos:**
- 100 samples HORZ_4 ‚Üí ~400-500 varia√ß√µes (com augmentation)

**Solu√ß√£o 2: Label Smoothing**

```python
# Suavizar labels para classes raras
criterion = ClassBalancedFocalLoss(
    ...,
    label_smoothing=0.1  # 10% smoothing
)

# Efeito:
# One-hot [0, 0, 1, 0] ‚Üí [0.011, 0.011, 0.89, 0.011]
# Reduz overconfidence em classes raras
```

**Solu√ß√£o 3: Aceitar F1 Baixo em HORZ_4/VERT_4**

**Decis√£o Pragm√°tica:**
- HORZ_4 e VERT_4 s√£o **extremamente raras** (0.3% dataset)
- Impacto na accuracy final: m√≠nimo
- **Meta:** F1 ‚â• 30% nessas classes (aceit√°vel dada raridade)

**An√°lise Impacto:**
```python
# Se HORZ_4 e VERT_4 t√™m F1=30%:
samples_4 = 100 + 88 = 188  (0.5% do dataset)
impact = 0.005 √ó 0.30 = 0.0015 ‚Üí 0.15% accuracy

# Se outras classes t√™m F1=60%:
samples_others = 38,264  (99.5%)
impact = 0.995 √ó 0.60 = 0.597 ‚Üí 59.7% accuracy

Total esperado: 59.85% ‚âà 60%
```

---

### 3.3 Desafio: Converg√™ncia Lenta

**Problema:**
- 9 classes > 3 classes (Stage 2 original)
- Mais complexo, pode demorar para convergir

**Solu√ß√£o 1: Curriculum Learning**

```python
# Fase 1 (epochs 1-15): Treinar em classes mais f√°ceis
# HORZ, VERT, SPLIT (3 macro-classes, 23k samples)

# Fase 2 (epochs 16-30): Adicionar AB
# HORZ, VERT, SPLIT, HORZ_A/B, VERT_A/B (7 classes, 38k samples)

# Fase 3 (epochs 31-50): Full training
# Todas 9 classes (38,452 samples)
```

**Literatura:** Bengio et al., 2009 - Curriculum Learning

**Solu√ß√£o 2: Learning Rate Schedule Agressivo**

```python
from torch.optim.lr_scheduler import OneCycleLR

optimizer = torch.optim.AdamW([
    {'params': backbone.parameters(), 'lr': 1e-4},
    {'params': head.parameters(), 'lr': 5e-4}
])

scheduler = OneCycleLR(
    optimizer,
    max_lr=[5e-4, 2e-3],  # Peak LR
    epochs=50,
    steps_per_epoch=len(train_loader),
    pct_start=0.3,  # 30% warmup
    anneal_strategy='cos'
)
```

---

## 4. Implementa√ß√£o

### 4.1 Prepara√ß√£o do Dataset

**Script:** `pesquisa_v6/scripts/001b_prepare_flatten_dataset.py`

```python
# Remapear labels de 10 classes ‚Üí 9 classes (remover NONE)
# NONE fica no Stage 1, Stage 2 s√≥ v√™ PARTITION samples

label_mapping = {
    0: None,  # NONE (tratado por Stage 1)
    1: 0,     # PARTITION_HORZ ‚Üí classe 0
    2: 1,     # PARTITION_VERT ‚Üí classe 1
    3: 2,     # PARTITION_SPLIT ‚Üí classe 2
    4: 3,     # PARTITION_HORZ_A ‚Üí classe 3
    5: 4,     # PARTITION_HORZ_B ‚Üí classe 4
    6: 5,     # PARTITION_VERT_A ‚Üí classe 5
    7: 6,     # PARTITION_VERT_B ‚Üí classe 6
    8: 7,     # PARTITION_HORZ_4 ‚Üí classe 7
    9: 8,     # PARTITION_VERT_4 ‚Üí classe 8
}

# Filtrar apenas PARTITION samples (labels 1-9)
partition_samples = dataset[dataset['labels'] != 0]

# Salvar novo dataset
torch.save({
    'blocks': partition_samples['blocks'],
    'labels': remap_labels(partition_samples['labels']),
    'qps': partition_samples['qps']
}, 'v6_dataset_flatten/block_16/train.pt')
```

**Output:**
```
v6_dataset_flatten/
  block_16/
    train.pt  (38,264 samples, 9 classes)
    val.pt    (38,256 samples, 9 classes)
    metadata.json (class distribution)
```

---

### 4.2 Modelo Stage 2 Flat

**Script:** `pesquisa_v6/scripts/004b_train_stage2_flat_9classes.py`

**Arquitetura:**
```python
class Stage2FlatModel(nn.Module):
    def __init__(self, num_classes=9, pretrained=True):
        super().__init__()
        
        # Backbone: ResNet-18 + SE + Spatial Attention
        self.backbone = ImprovedBackbone(
            in_channels=1,
            pretrained=pretrained
        )
        
        # Head: 9 classes
        self.head = nn.Sequential(
            nn.Dropout(0.5),  # Dropout maior para regulariza√ß√£o
            nn.Linear(512, num_classes)
        )
        
    def forward(self, x):
        features = self.backbone(x)
        return self.head(features)
```

**Treinamento:**
```python
# Loss
criterion = ClassBalancedFocalLoss(
    samples_per_class=class_counts,
    num_classes=9,
    beta=0.9999,
    gamma=2.5,
    alpha=0.25,
    label_smoothing=0.1
)

# Sampler
sampler = create_balanced_sampler(dataset)

# Optimizer
optimizer = torch.optim.AdamW([
    {'params': model.backbone.parameters(), 'lr': 1e-4},
    {'params': model.head.parameters(), 'lr': 5e-4}
], weight_decay=1e-4)

# Scheduler
scheduler = OneCycleLR(optimizer, max_lr=[5e-4, 2e-3], epochs=50, ...)

# Training loop
for epoch in range(50):
    train_one_epoch(model, train_loader, criterion, optimizer, scheduler)
    metrics = validate(model, val_loader)
    
    # Early stopping on Macro F1
    if metrics['macro_f1'] > best_f1:
        save_checkpoint(model, 'best')
```

---

### 4.3 Pipeline Evaluation

**Script:** `pesquisa_v6/scripts/008b_run_pipeline_flatten_eval.py`

```python
def evaluate_flatten_pipeline(stage1_model, stage2_flat_model, dataloader):
    predictions = []
    labels = []
    
    for blocks, targets in dataloader:
        # Stage 1: NONE vs PARTITION
        stage1_logits = stage1_model(blocks)
        stage1_probs = torch.sigmoid(stage1_logits)
        stage1_preds = (stage1_probs > threshold).long()
        
        # Samples que s√£o NONE
        none_mask = stage1_preds == 0
        predictions[none_mask] = 0  # PARTITION_NONE
        
        # Samples que s√£o PARTITION
        partition_mask = stage1_preds == 1
        partition_blocks = blocks[partition_mask]
        
        # Stage 2 FLAT: 9 classes direto
        stage2_logits = stage2_flat_model(partition_blocks)
        stage2_preds = torch.argmax(stage2_logits, dim=1)
        
        # Remapear de volta: [0-8] ‚Üí [1-9]
        stage2_preds = stage2_preds + 1
        predictions[partition_mask] = stage2_preds
        
        labels.append(targets)
    
    # Compute metrics
    return compute_metrics(predictions, labels)
```

---

## 5. Expectativas e Metas

### 5.1 Performance Esperada

**Baseline (V6 Hier√°rquico):**
- Overall Accuracy: 47.66%
- Macro F1: 13.38%
- Stage 3 degrada√ß√£o: -95%

**Target (Flatten):**

| M√©trica | Conservador | Realista | Otimista |
|---------|-------------|----------|----------|
| **Overall Accuracy** | 50% | 53% | 56% |
| **Macro F1** | 35% | 42% | 48% |
| HORZ F1 | 50% | 60% | 70% |
| VERT F1 | 40% | 50% | 60% |
| SPLIT F1 | 45% | 55% | 65% |
| AB (m√©dia) F1 | 25% | 35% | 45% |
| HORZ_4/VERT_4 F1 | 10% | 25% | 40% |
| **Degrada√ß√£o** | **0%** | **0%** | **0%** |

**Justificativa Realista (53%):**
- Stage 1: 72.79% accuracy (mant√©m)
- Stage 2 Flat: 55-60% accuracy (9 classes, sem cascata)
- **Pipeline:** 0.73 √ó 0.58 = 42.3% te√≥rico
- Com NONE dominando (57% dataset): **~53% overall**

---

### 5.2 An√°lise de Risco

#### Risco 1: Stage 2 Flat N√£o Alcan√ßa 55%
**Probabilidade:** 30%  
**Mitiga√ß√£o:**
- Aumentar epochs (50 ‚Üí 80)
- Testar diferentes gammas CB-Focal (2.0, 2.5, 3.0)
- Adicionar mais augmentation

**Fallback:** Se < 48%, tentar Op√ß√£o 2 (Multi-Task)

#### Risco 2: HORZ_4 e VERT_4 Colapsam Completamente
**Probabilidade:** 50%  
**Impacto:** Baixo (apenas 0.5% dataset)  
**Mitiga√ß√£o:**
- Aceitar F1=10-20% nessas classes
- Impacto na accuracy final: ~0.3pp

#### Risco 3: Overfitting em Classes Raras
**Probabilidade:** 40%  
**Mitiga√ß√£o:**
- Dropout alto (0.5)
- Label smoothing (0.1)
- Strong augmentation
- Early stopping agressivo

---

## 6. Cronograma

### Dia 1 (14/10): Prepara√ß√£o
- [ ] Criar dataset flatten (script 001b)
- [ ] Verificar distribui√ß√£o de classes
- [ ] Implementar balanced sampler
- [ ] **Milestone:** Dataset pronto

### Dia 2 (15/10): Implementa√ß√£o
- [ ] Implementar Stage2FlatModel
- [ ] Implementar training script (004b)
- [ ] Testar training loop (1 epoch)
- [ ] **Milestone:** Script funcionando

### Dia 3 (16/10): Treinamento
- [ ] Treinar Stage 2 Flat (50 epochs, ~6h)
- [ ] Monitorar loss curves
- [ ] **Milestone:** Modelo treinado

### Dia 4 (17/10): Avalia√ß√£o
- [ ] Pipeline evaluation (script 008b)
- [ ] An√°lise de resultados
- [ ] Compara√ß√£o com baseline
- [ ] **Milestone:** Resultados documentados

---

## 7. Crit√©rios de Sucesso

### Sucesso Completo ‚úÖ
- Accuracy ‚â• 53%
- Macro F1 ‚â• 40%
- Degrada√ß√£o = 0% (sem Stage 3)
- Todas classes preditas (nenhuma colapsada)

### Sucesso Parcial ‚ö†Ô∏è
- Accuracy 50-53%
- Macro F1 35-40%
- HORZ_4/VERT_4 podem colapsar (impacto baixo)

### Falha ‚ùå
- Accuracy < 50% (pior que baseline 47.66%)
- Classes principais (HORZ, VERT, SPLIT) colapsam
- **A√ß√£o:** Tentar Op√ß√£o 2 (Multi-Task) ou Op√ß√£o 3 (Stage 3 Robusto)

---

## 8. Contribui√ß√µes Cient√≠ficas

### Para a Tese de Doutorado

**Cap√≠tulo 4: Resultados e An√°lise**

**Se√ß√£o 4.3: Compara√ß√£o Arquitetural**

| Aspecto | Hier√°rquica (3 Stages) | Flatten (2 Stages) |
|---------|------------------------|---------------------|
| **Accuracy** | 47.66% | ~53% (esperado) |
| **Macro F1** | 13.38% | ~42% (esperado) |
| **Degrada√ß√£o** | -95% (Stage 3) | 0% (sem cascata) |
| **Complexidade** | 3-4 modelos | 2 modelos |
| **Training time** | 7 dias | 3 dias |
| **Inference time** | 3√ó forward passes | 2√ó forward passes |

**Insight Cient√≠fico:**
> "Arquiteturas hier√°rquicas sofrem de erro em cascata quando stages intermedi√°rios t√™m baixa accuracy. Para tarefas com classes geometricamente relacionadas, arquitetura flat com loss balanceado supera hierarquia complexa."

**Literatura Validada:**
- He et al., 2019: Simplicidade > complexidade
- Kornblith et al., 2019: Features gen√©ricas classificam direto
- Cui et al., 2019: CB-Focal resolve imbalance extremo

---

## 9. Pr√≥ximos Passos

### Se Flatten ‚â• 53%: ‚úÖ ADOTAR
1. Documentar experimento completo
2. Commit e push
3. Merge para main
4. **Declarar arquitetura V6 final**
5. Focar em escrita da tese

### Se Flatten 50-53%: ‚ö†Ô∏è AVALIAR
1. Tentar otimiza√ß√µes (gamma tuning, mais epochs)
2. Se n√£o melhorar: **aceitar** (ainda melhor que 47.66%)
3. Documentar trade-offs

### Se Flatten < 50%: ‚ùå PIVOT
1. Tentar **Op√ß√£o 2** (Multi-Task Stage 2)
2. Ou **Op√ß√£o 3** (Stage 3 Robusto com noise)
3. Reavaliar premissas arquiteturais

---

## 10. Refer√™ncias

1. **Cui et al., 2019:** Class-Balanced Loss Based on Effective Number of Samples (CB-Focal Loss)
2. **He et al., 2019:** Rethinking ImageNet Pre-training (Simplicidade)
3. **Kornblith et al., 2019:** Do Better ImageNet Models Transfer Better? (Features gen√©ricas)
4. **Bengio et al., 2009:** Curriculum Learning (Training progressivo)
5. **Zhang et al., 2018:** mixup - Beyond Empirical Risk Minimization
6. **Yun et al., 2019:** CutMix - Regularization Strategy

---

**√öltima Atualiza√ß√£o:** 13 de outubro de 2025  
**Status:** üöÄ Pronto para implementa√ß√£o  
**Branch:** `feat/stage2-flatten-9classes`  
**Pr√≥ximo:** Criar script 001b (prepara√ß√£o dataset)
