# Pipeline v6 - Plano de Desenvolvimento

## ğŸ“‹ SumÃ¡rio Executivo

O pipeline v6 Ã© uma reformulaÃ§Ã£o completa da arquitetura hierÃ¡rquica, focando em resolver os **3 problemas crÃ­ticos** identificados na v5:

1. **Stage 1**: Baixa precisÃ£o (53.71%) â†’ 27k falsos positivos contaminando o pipeline
2. **Stage 2**: ConfusÃ£o entre macro-classes (33.41% Macro F1) â†’ Erros propagados
3. **Stage 3-AB**: Colapso total do especialista (25.26% F1) â†’ Classes perdidas

---

## ğŸ¯ Objetivos e Metas

### MÃ©tricas Alvo (block_16)

| Componente | v5 Atual | v6 Meta Fase 1 | v6 M3. **Implementar v6_pipeline module** (base code)
4. **Preparar datasets v6** (script 001)
5. **Executar Fase 1** (Experimentos 1.1 e 1.2)
6. **Avaliar resultados** e ajustar plano
7. **Iterar** atÃ© atingir critÃ©rios de sucesso

---

**Status**: ğŸŸ¡ Em Desenvolvimento (Training Scripts 6/7 - Script 008 validado)  
**PrÃ³ximo**: Script 009 (compare_v5_v6) - ÃšLTIMO SCRIPT  
**Ãšltima AtualizaÃ§Ã£o**: 2025-10-06  
**ResponsÃ¡vel**: @chiarorosatus**: ğŸŸ¢ Core Modules Completos | Training Scripts 3/7  
**Ãšltima AtualizaÃ§Ã£o**: 2025-10-06  
**ResponsÃ¡vel**: @chiarorosa  
**PrÃ³ximo**: Implementar 006_train_stage3_ab_ensemble.pye 2 |
|------------|----------|----------------|----------------|
| **Stage 1 F1** | 65.19% | 68-70% | 72-75% |
| **Stage 1 PrecisÃ£o** | 53.71% | 62-65% | 68-72% |
| **Stage 2 Macro F1** | 33.41% | 45-50% | 55-60% |
| **Stage 3-RECT F1** | 72.50% | 75-78% | 80-83% |
| **Stage 3-AB F1** | 25.26% | 45-50% | 60-65% |
| **AcurÃ¡cia Final** | 39.56% | 48-52% | 58-63% |

---

## ğŸ—ï¸ Arquitetura Proposta

### EstratÃ©gia Principal: Hierarquia Revisada (3 EstÃ¡gios Otimizados)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Input Block    â”‚
                    â”‚    (16x16)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Backbone      â”‚
                    â”‚  (ResNet-18+)   â”‚
                    â”‚   + Attention   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   STAGE 1 (Binary)  â”‚
                    â”‚  NONE vs PARTITION  â”‚
                    â”‚   + Focal Loss      â”‚
                    â”‚   + Threshold Opt   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚   IF NONE   â”‚â”€â”€â†’ OUTPUT: PARTITION_NONE
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   STAGE 2 (3-way)  â”‚
                    â”‚  SPLIT | RECT | AB â”‚
                    â”‚  + Class Weights   â”‚
                    â”‚  + Data Aug        â”‚
                    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                       â”‚        â”‚    â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  SPLIT    â”‚  â”‚  STAGE 3-RECT   â”‚
              â”‚  (direct) â”‚  â”‚  HORZ vs VERT   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚   STAGE 3-AB         â”‚
                             â”‚  HORZ_A | HORZ_B |   â”‚
                             â”‚  VERT_A | VERT_B     â”‚
                             â”‚  + Ensemble (3 nets) â”‚
                             â”‚  + Heavy Aug         â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MudanÃ§as Chave vs v5

1. **Stage 2 Simplificado**: 
   - âŒ Remove classe "NONE" (jÃ¡ filtrada em Stage 1)
   - âŒ Remove classe "1TO4" (nÃ£o existe no dataset)
   - âœ… Apenas 3 classes: SPLIT, RECT, AB
   - âœ… Reduz confusÃ£o e melhora separabilidade

2. **Stage 1 Melhorado**:
   - âœ… Focal Loss com Î³=2.5 (mais agressivo)
   - âœ… Hard Negative Mining
   - âœ… Threshold otimizado via validaÃ§Ã£o (testar 0.45-0.6)
   - âœ… CalibraÃ§Ã£o de probabilidades (Temperature Scaling)

3. **Stage 3-AB Redesenhado**:
   - âœ… Ensemble de 3 modelos (votaÃ§Ã£o majoritÃ¡ria)
   - âœ… Data Augmentation pesado (rotaÃ§Ã£o, flip, cutout)
   - âœ… Oversampling agressivo (5x para classes minoritÃ¡rias)
   - âœ… Mixup entre classes AB

---

## ğŸ”§ Melhorias TÃ©cnicas Implementadas

### 1. Arquitetura do Modelo

#### Backbone Upgrade
```python
# v5: ResNet-18 simples
# v6: ResNet-18 + SE-Block + Dropout adaptativo

- ResNet-18 base
- Squeeze-and-Excitation blocks (channel attention)
- Spatial Attention Module (SAM)
- Dropout progressivo: [0.1, 0.2, 0.3, 0.4]
- Group Normalization (alternativa a BatchNorm)
```

#### Heads Especializados
```python
# Stage 1: Binary Head
- FC [512 â†’ 256 â†’ 1]
- Dropout 0.3
- Sigmoid + Temperature Scaling

# Stage 2: 3-way Head  
- FC [512 â†’ 256 â†’ 128 â†’ 3]
- Dropout 0.4
- Softmax + Label Smoothing (0.1)

# Stage 3-RECT: Binary Head
- FC [512 â†’ 128 â†’ 64 â†’ 2]
- Dropout 0.2

# Stage 3-AB: 4-way Ensemble
- 3x FC [512 â†’ 256 â†’ 128 â†’ 4]
- Dropout 0.5
- VotaÃ§Ã£o majoritÃ¡ria
```

### 2. Loss Functions

#### Stage 1: Focal Loss + Hard Negative Mining
```python
FocalLoss(alpha=0.25, gamma=2.5) 
+ HardNegativeMining(ratio=3:1)
```

#### Stage 2: Class-Balanced Focal Loss
```python
weights = [w_SPLIT, w_RECT, w_AB]
# Baseado em effective number of samples
CB_FocalLoss(beta=0.9999, gamma=2.0)
```

#### Stage 3-AB: Mixup + Focal Loss
```python
Mixup(alpha=0.4) + FocalLoss(gamma=2.0)
```

### 3. Data Augmentation Strategy

#### Stage 1 (Treino Backbone)
- RandomHorizontalFlip (p=0.5)
- RandomVerticalFlip (p=0.5)
- RandomRotation (90Â°, 180Â°, 270Â°)
- GaussianNoise (Ïƒ=0.01)

#### Stage 2 (Macro-classes)
- Todas do Stage 1 +
- Cutout (16x16, p=0.3)
- GridShuffle (4x4, p=0.2)

#### Stage 3-AB (CrÃ­tico!)
- **Heavy Augmentation**:
  - HorizontalFlip com label swap (HORZ_A â†” HORZ_B)
  - VerticalFlip com label swap (VERT_A â†” VERT_B)
  - Rotation 90Â° com label rotate (HORZ â†” VERT)
  - Mixup (Î±=0.4) entre classes AB
  - CoarseDropout (mÃºltiplos patches)

### 4. Sampling Strategy

#### Stage 1
```python
# Balanceamento 1:1 (NONE vs PARTITION)
WeightedRandomSampler(weights=[0.5, 0.5])
```

#### Stage 2
```python
# Effective Number Sampling
weights = (1 - beta^n_i) / (1 - beta)
# beta=0.9999, favorece classes raras
```

#### Stage 3-AB
```python
# Oversampling 5x para minoritÃ¡rias
# HORZ_B, VERT_A â†’ repetir 5x
# HORZ_A, VERT_B â†’ repetir 3x
```

### 5. Training Strategy

#### Phase 1: Backbone Pre-training
```
Epochs: 20
LR: 1e-3 â†’ 1e-5 (cosine decay)
Optimizer: AdamW (wd=1e-4)
Task: Stage 1 (Binary)
```

#### Phase 2: Stage 2 Fine-tuning
```
Epochs: 25
LR: 5e-4 â†’ 5e-6 (cosine decay)
Freeze: Backbone (primeiras 2 Ã©pocas)
Unfreeze: Backbone (Ãºltimas 23 Ã©pocas, LR=1e-5)
```

#### Phase 3: Stage 3 Specialists
```
Epochs: 30 (RECT), 50 (AB)
LR: 3e-4 â†’ 3e-6
Freeze: Backbone sempre
Train: Apenas specialist heads

AB Ensemble:
- Treinar 3 modelos com seeds diferentes
- Augmentation aleatÃ³rio diferente
- Voting na inferÃªncia
```

### 6. Threshold Optimization

#### Stage 1 Threshold Search
```python
# Grid search no validation set
thresholds = np.linspace(0.4, 0.7, 31)
for th in thresholds:
    precision, recall, f1 = evaluate(th)
# Escolher threshold que maximiza F1 ou Precision
```

#### CalibraÃ§Ã£o de Probabilidades
```python
# Temperature Scaling (Guo et al., 2017)
# Aprender T no validation set
probs_calibrated = softmax(logits / T)
```

---

## ğŸ“Š Dataset Preparation

### ModificaÃ§Ãµes nos Scripts

#### `001_prepare_v6_dataset.py`
```python
# MudanÃ§as vs v5:
1. Stage 2 labels: apenas [SPLIT, RECT, AB] (remove NONE, 1TO4)
2. Oversampling AB classes: 5x minoritÃ¡rias
3. ValidaÃ§Ã£o estratificada: manter distribuiÃ§Ã£o AB
4. Salvar metadados estendidos (confusion matrix baseline)
```

#### `002_prepare_v6_stage3_ab_ensemble.py`
```python
# Novo script especÃ­fico para AB
1. Criar 3 versÃµes do dataset AB com augmentation diferente
2. Adicionar exemplos sintÃ©ticos (Mixup offline)
3. AnÃ¡lise de hard negatives (erros do modelo v5)
```

### Estrutura de Dados

```
pesquisa_v6/
â”œâ”€â”€ v6_dataset/
â”‚   â””â”€â”€ block_16/
â”‚       â”œâ”€â”€ train.pt          # Stage 1+2 combined
â”‚       â”œâ”€â”€ val.pt
â”‚       â””â”€â”€ metadata.json
â”œâ”€â”€ v6_dataset_stage3/
â”‚   â”œâ”€â”€ RECT/
â”‚   â”‚   â””â”€â”€ block_16/
â”‚   â”‚       â”œâ”€â”€ train.pt
â”‚   â”‚       â””â”€â”€ val.pt
â”‚   â””â”€â”€ AB/
â”‚       â””â”€â”€ block_16/
â”‚           â”œâ”€â”€ train_v1.pt   # Ensemble member 1
â”‚           â”œâ”€â”€ train_v2.pt   # Ensemble member 2
â”‚           â”œâ”€â”€ train_v3.pt   # Ensemble member 3
â”‚           â””â”€â”€ val.pt
```

---

## ğŸ§ª Experiments Roadmap

### Fase 1: Baseline Improvements (Semana 1-2)

**Experimento 1.1: Stage 1 Optimization**
- [x] Implementar Focal Loss com Î³ variÃ¡vel
- [ ] Hard Negative Mining
- [ ] Threshold grid search
- [ ] Temperature Scaling
- **Meta**: PrecisÃ£o 62%+, F1 68%+

**Experimento 1.2: Stage 2 Redesign**
- [x] Remover NONE da Stage 2 (implementado em data_hub.py)
- [x] Scripts de preparaÃ§Ã£o 001 e 002 (validados)
- [x] Class-Balanced Focal Loss (implementado em losses.py)
- [x] Heavy augmentation (implementado em augmentation.py)
- **Meta**: Macro F1 45%+

### Fase 2: Specialist Improvements (Semana 3-4)

**Experimento 2.1: Stage 3-RECT**
- [ ] Attention mechanism
- [ ] Test-time augmentation (TTA)
- **Meta**: F1 78%+

**Experimento 2.2: Stage 3-AB Ensemble**
- [ ] Implementar 3 modelos independentes
- [ ] Mixup + Heavy Aug
- [ ] Oversampling 5x
- **Meta**: F1 50%+ (dobrar performance v5)

### Fase 3: Advanced Techniques (Semana 5-6)

**Experimento 3.1: Knowledge Distillation**
- [ ] Teacher: ensemble grande
- [ ] Student: modelo compacto
- **Meta**: Manter performance, reduzir latÃªncia

**Experimento 3.2: Multi-task Learning**
- [ ] PrediÃ§Ã£o conjunta Stage 2 + Stage 3
- [ ] Shared representations
- **Meta**: AcurÃ¡cia final 55%+

### Fase 4: Alternative Architectures (Semana 7-8)

**Experimento 4.1: Transformer-based**
- [ ] Vision Transformer (ViT-Tiny)
- [ ] Patch embedding 4x4
- **Meta**: Comparar vs CNN

**Experimento 4.2: Hybrid CNN-Transformer**
- [ ] CNN backbone + Transformer head
- [ ] Self-attention para AB classes
- **Meta**: Best of both worlds

---

## ğŸ“ File Structure

```
pesquisa_v6/
â”œâ”€â”€ PLANO_V6.md                          # Este documento
â”‚
â”œâ”€â”€ v6_pipeline/                         # Core modules
â”‚   â”œâ”€â”€ __init__.py                      # âœ… 
â”‚   â”œâ”€â”€ data_hub.py                      # âœ… Dataset utils (standalone)
â”‚   â”œâ”€â”€ models.py                        # âœ… Backbones + Heads
â”‚   â”œâ”€â”€ losses.py                        # âœ… Focal, CB, Mixup losses
â”‚   â”œâ”€â”€ augmentation.py                  # âœ… Aug pipelines
â”‚   â”œâ”€â”€ ensemble.py                      # âœ… AB ensemble logic
â”‚   â””â”€â”€ metrics.py                       # âœ… Evaluation utils
â”‚
â”œâ”€â”€ scripts/                             # Training scripts
â”‚   â”œâ”€â”€ 001_prepare_v6_dataset.py        # âœ… Validado
â”‚   â”œâ”€â”€ 002_prepare_v6_stage3_datasets.py # âœ… Validado
â”‚   â”œâ”€â”€ 003_train_stage1_improved.py     # âœ… Validado
â”‚   â”œâ”€â”€ 004_train_stage2_redesigned.py   # âœ… Validado
â”‚   â”œâ”€â”€ 005_train_stage3_rect.py         # âœ… Validado
â”‚   â”œâ”€â”€ 006_train_stage3_ab_ensemble.py  # âœ… Validado
â”‚   â”œâ”€â”€ 007_optimize_thresholds.py       # âœ… Validado
â”‚   â”œâ”€â”€ 008_run_pipeline_eval_v6.py      # âœ… Validado
â”‚   â””â”€â”€ 009_compare_v5_v6.py
â”‚
â”œâ”€â”€ experiments/                         # Experiment configs
â”‚   â”œâ”€â”€ exp1_stage1_optimization.yaml
â”‚   â”œâ”€â”€ exp2_stage2_redesign.yaml
â”‚   â”œâ”€â”€ exp3_ab_ensemble.yaml
â”‚   â””â”€â”€ exp4_full_pipeline.yaml
â”‚
â”œâ”€â”€ notebooks/                           # Analysis notebooks
â”‚   â”œâ”€â”€ v6_ablation_study.ipynb
â”‚   â”œâ”€â”€ v6_error_analysis.ipynb
â”‚   â””â”€â”€ v6_vs_v5_comparison.ipynb
â”‚
â””â”€â”€ logs/                                # Results
    â””â”€â”€ v6_experiments/
        â”œâ”€â”€ stage1/
        â”œâ”€â”€ stage2/
        â”œâ”€â”€ stage3_rect/
        â”œâ”€â”€ stage3_ab/
        â””â”€â”€ pipeline/
```

---

## ğŸ”¬ Ablation Studies

### Study 1: Stage 1 Components
| Config | Focal Î³ | Hard Mining | Threshold | F1 | Precision |
|--------|---------|-------------|-----------|-----|-----------|
| Baseline | 2.0 | No | 0.5 | 65.19% | 53.71% |
| +Focal Î³=2.5 | 2.5 | No | 0.5 | ? | ? |
| +Hard Mining | 2.5 | Yes (3:1) | 0.5 | ? | ? |
| +Threshold Opt | 2.5 | Yes | 0.55 | ? | ? |
| Full | 2.5 | Yes | 0.55 | **Target: 70%** | **Target: 65%** |

### Study 2: Stage 2 Redesign
| Config | Classes | Loss | Aug | Macro F1 |
|--------|---------|------|-----|----------|
| v5 Baseline | 5 (NONE,SPLIT,RECT,AB,1TO4) | CE | Basic | 33.41% |
| -NONE, -1TO4 | 3 (SPLIT,RECT,AB) | CE | Basic | ? |
| +CB Focal | 3 | CB-Focal | Basic | ? |
| +Heavy Aug | 3 | CB-Focal | Heavy | ? |
| Full v6 | 3 | CB-Focal | Heavy | **Target: 48%** |

### Study 3: Stage 3-AB Ensemble
| Config | Models | Aug | Sampling | F1 |
|--------|--------|-----|----------|-----|
| v5 Baseline | 1 | Basic | Balanced | 25.26% |
| +Heavy Aug | 1 | Heavy | Balanced | ? |
| +Oversampling | 1 | Heavy | 5x minority | ? |
| +Mixup | 1 | Heavy+Mixup | 5x | ? |
| Ensemble-3 | 3 | Heavy+Mixup | 5x | **Target: 50%** |

---

## ğŸ“ˆ Expected Performance Gains

### Stage-by-Stage Improvements

```
Stage 1:
  v5: F1=65.19%, Prec=53.71%, Rec=82.93%
  v6: F1=70%â†‘,   Prec=65%â†‘,    Rec=80%â†“
  EstratÃ©gia: Trocar recall por precisÃ£o (reduzir FP)

Stage 2:
  v5: Macro F1=33.41%
  v6: Macro F1=48%â†‘ (+44% relativo)
  EstratÃ©gia: Simplificar classes, melhorar separabilidade

Stage 3-RECT:
  v5: F1=72.50%
  v6: F1=78%â†‘ (+8% relativo)
  EstratÃ©gia: AtenÃ§Ã£o + TTA

Stage 3-AB:
  v5: F1=25.26% (CRÃTICO)
  v6: F1=50%â†‘ (+98% relativo)
  EstratÃ©gia: Ensemble + Aug pesado
```

### Final Pipeline Accuracy

```
v5: 39.56%
v6 (conservative): 48% (+21% relativo)
v6 (optimistic): 55% (+39% relativo)
```

---

## ğŸš€ Implementation Priority

### High Priority (Must Have)
1. âœ… Stage 2 redesign (remover NONE/1TO4)
2. âœ… Stage 3-AB: 2 implementaÃ§Ãµes testadas
   - `006_train_stage3_ab_ensemble_reference.py`: F1=10.35% (arquivado como referÃªncia)
   - `006_train_stage3_ab_fgvc.py`: F1=24.50% (aceito, 4/4 classes funcionando)
3. âœ… Focal Loss em todos estÃ¡gios
4. âœ… Heavy augmentation para AB
5. â­• Threshold optimization Stage 1 (prÃ³ximo: script 007)

### Medium Priority (Should Have)
6. â­• Attention mechanism no backbone
7. â­• Temperature scaling (calibraÃ§Ã£o)
8. â­• Hard negative mining
9. â­• Mixup para AB
10. â­• TTA (test-time augmentation)

### Low Priority (Nice to Have)
11. â¸ï¸ Knowledge distillation
12. â¸ï¸ Transformer heads
13. â¸ï¸ Multi-task learning
14. â¸ï¸ Neural architecture search
15. â¸ï¸ Quantization (deploy)

---

## ğŸ“ Success Criteria

### CritÃ©rios MÃ­nimos (Fase 1)
- [ ] Stage 1 F1 â‰¥ 68%
- [ ] Stage 1 Precision â‰¥ 62%
- [ ] Stage 2 Macro F1 â‰¥ 45%
- [ ] Stage 3-AB F1 â‰¥ 40%
- [ ] Final Accuracy â‰¥ 48%

### CritÃ©rios Ideais (Fase 2)
- [ ] Stage 1 F1 â‰¥ 72%
- [ ] Stage 1 Precision â‰¥ 68%
- [ ] Stage 2 Macro F1 â‰¥ 52%
- [ ] Stage 3-AB F1 â‰¥ 55%
- [ ] Final Accuracy â‰¥ 55%

### CritÃ©rios Stretch (Fase 3)
- [ ] Stage 1 F1 â‰¥ 75%
- [ ] Stage 2 Macro F1 â‰¥ 58%
- [ ] Stage 3-AB F1 â‰¥ 65%
- [ ] Final Accuracy â‰¥ 60%

---

## ğŸ” Monitoring & Analysis

### MÃ©tricas de Acompanhamento
1. **Por EstÃ¡gio**: Accuracy, F1, Precision, Recall, Confusion Matrix
2. **Por Classe**: F1 individual, Support, Erros especÃ­ficos
3. **Pipeline**: AcurÃ¡cia final, DistribuiÃ§Ã£o de erros, LatÃªncia
4. **Treino**: Loss curves, Learning rate, Gradients

### Dashboards
```python
# Usar TensorBoard para:
- Loss curves (train/val)
- Confusion matrices (interativo)
- Embedding visualizations (t-SNE)
- Attention maps (heatmaps)

# Usar Weights & Biases para:
- Hyperparameter sweeps
- Model comparisons
- Experiment tracking
```

---

## ğŸ“š References & Inspiration

### Papers
1. Focal Loss (Lin et al., 2017) - Lidar com desbalanceamento
2. Class-Balanced Loss (Cui et al., 2019) - Effective number
3. Mixup (Zhang et al., 2018) - Data augmentation
4. Temperature Scaling (Guo et al., 2017) - CalibraÃ§Ã£o
5. Squeeze-and-Excitation (Hu et al., 2018) - Attention

### Codebases
- timm (PyTorch Image Models) - Backbones
- albumentations - Augmentations
- pytorch-metric-learning - Loss functions

---

## ğŸ¤ Next Steps

1. **Revisar este plano** com o time
2. **Priorizar experimentos** (High â†’ Medium â†’ Low)
3. **Implementar v6_pipeline module** (base code)
4. **Preparar datasets v6** (script 001)
5. **Executar Fase 1** (Experimentos 1.1 e 1.2)
6. **Avaliar resultados** e ajustar plano
7. **Iterar** atÃ© atingir critÃ©rios de sucesso

---

**Status**: ï¿½ Em Desenvolvimento (Data prep completo)  
**Ãšltima AtualizaÃ§Ã£o**: 2025-10-06  
**ResponsÃ¡vel**: @chiarorosa  
**PrÃ³ximo**: Implementar losses.py e models.py
