# Pipeline v6 - Plano de Desenvolvimento Consolidado

**Data:** 13 de outubro de 2025  
**Status:** üü° Em Desenvolvimento - Scripts 8/9 conclu√≠dos  
**√öltima Atualiza√ß√£o:** 13/10/2025  
**Respons√°vel:** @chiarorosa

---

## üìã Sum√°rio Executivo

O pipeline v6 √© uma reformula√ß√£o completa da arquitetura hier√°rquica, focando em resolver os **3 problemas cr√≠ticos** identificados na v5:

1. **Stage 1**: Baixa precis√£o (53.71%) ‚Üí 27k falsos positivos contaminando o pipeline
2. **Stage 2**: Confus√£o entre macro-classes (33.41% Macro F1) ‚Üí Erros propagados
3. **Stage 3-AB**: Colapso total do especialista (25.26% F1) ‚Üí Classes perdidas

### Progresso Atual (Atualizado 13/10/2025)

| Componente | Status | Resultado |
|------------|--------|-----------|
| **Scripts 001-002** | ‚úÖ Validado | Dataset preparado (152k train / 38k val) |
| **Script 003 (Stage 1)** | ‚úÖ Validado | F1=72.28% (√©poca 19) - **META ATINGIDA** ‚â•68% |
| **Script 004 (Stage 2)** | ‚úÖ **RESOLVIDO** | F1=46.51% (frozen model √©poca 8) - **META ATINGIDA** ‚â•45% |
| **Script 005 (Stage 3-RECT)** | ‚úÖ Validado | F1=68.44% (√©poca 12) |
| **Script 006 (Stage 3-AB)** | ‚úÖ Validado | F1=24.50% (4/4 classes, √©poca 6) |
| **Script 007 (Threshold)** | ‚úÖ Validado | threshold=0.45 ‚Üí F1=72.79% |
| **Script 008 (Pipeline)** | ‚úÖ Validado | Accuracy=47.66% (meta: 48%, gap: -0.34pp) |
| **Script 009 (Compare v5/v6)** | ‚è≥ Pr√≥ximo | √öltimo script do pipeline |

---

## üéØ Objetivos e Metas

### M√©tricas Alvo (block_16)

| Componente | v5 Atual | v6 Meta Fase 1 | v6 Meta Fase 2 | v6 Obtido (13/10) |
|------------|----------|----------------|----------------|-------------------|
| **Stage 1 F1** | 65.19% | 68-70% | 72-75% | ‚úÖ **72.28%** |
| **Stage 1 Precis√£o** | 53.71% | 62-65% | 68-72% | ‚úÖ **67.13%** |
| **Stage 2 Macro F1** | 33.41% | 45-50% | 55-60% | ‚úÖ **46.51%** (frozen) |
| **Stage 3-RECT F1** | 72.50% | 75-78% | 80-83% | ‚ö†Ô∏è **68.44%** |
| **Stage 3-AB F1** | 25.26% | 45-50% | 60-65% | ‚ö†Ô∏è **24.50%** |
| **Acur√°cia Final** | 39.56% | 48-52% | 58-63% | ‚ö†Ô∏è **47.66%** (-0.34pp da meta) |

---

## üèóÔ∏è Arquitetura Proposta

### Estrat√©gia Principal: Hierarquia Revisada (3 Est√°gios Otimizados)

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Input Block    ‚îÇ
                    ‚îÇ    (16x16)      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Backbone      ‚îÇ
                    ‚îÇ  (ResNet-18+)   ‚îÇ
                    ‚îÇ   + SE-Blocks   ‚îÇ
                    ‚îÇ   + Attention   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   STAGE 1 (Binary)  ‚îÇ
                    ‚îÇ  NONE vs PARTITION  ‚îÇ
                    ‚îÇ   + Focal Loss      ‚îÇ
                    ‚îÇ   + Threshold=0.45  ‚îÇ
                    ‚îÇ   F1=72.28% ‚úÖ      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   IF NONE   ‚îÇ‚îÄ‚îÄ‚Üí OUTPUT: PARTITION_NONE
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   STAGE 2 (3-way)  ‚îÇ
                    ‚îÇ  SPLIT | RECT | AB ‚îÇ
                    ‚îÇ  + CB-Focal Loss   ‚îÇ
                    ‚îÇ  F1=46.51% (frozen)‚îÇ
                    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ        ‚îÇ    ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  SPLIT    ‚îÇ  ‚îÇ  STAGE 3-RECT ‚îÇ
              ‚îÇ  (direct) ‚îÇ  ‚îÇ  HORZ vs VERT ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  F1=68.44% ‚úÖ ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ   STAGE 3-AB         ‚îÇ
                             ‚îÇ  HORZ_A | HORZ_B |   ‚îÇ
                             ‚îÇ  VERT_A | VERT_B     ‚îÇ
                             ‚îÇ  + FGVC Techniques   ‚îÇ
                             ‚îÇ  F1=24.50% ‚ö†Ô∏è        ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Mudan√ßas Chave vs v5

1. **Stage 2 Simplificado**: 
   - ‚ùå Remove classe "NONE" (j√° filtrada em Stage 1)
   - ‚ùå Remove classe "1TO4" (n√£o existe no dataset)
   - ‚úÖ Apenas 3 classes: SPLIT, RECT, AB
   - ‚úÖ Reduz confus√£o e melhora separabilidade

2. **Stage 1 Melhorado**:
   - ‚úÖ Focal Loss com Œ≥=2.5 (mais agressivo)
   - ‚úÖ Hard Negative Mining
   - ‚úÖ Threshold otimizado = 0.45 (via script 007)
   - ‚úÖ Calibra√ß√£o de probabilidades (Temperature Scaling)

3. **Stage 3-AB Redesenhado**:
   - ‚úÖ FGVC (Fine-Grained Visual Classification) techniques
   - ‚úÖ Center Loss para compacta√ß√£o intra-classe
   - ‚úÖ CutMix augmentation
   - ‚úÖ Cosine classifier com temperature scaling
   - ‚úÖ 4/4 classes funcionando (vs. v5 com 1 classe colapsada)

---

## üî¥ PROBLEMA CR√çTICO: Stage 2 Catastrophic Forgetting (RESOLVIDO)

### Diagn√≥stico do Problema

**Treinamento Original (antes ULMFiT):**
- √âpoca 1 (frozen): F1=**47.58%** ‚úÖ
- √âpoca 3 (unfrozen): F1=**34-38%** ‚ùå
- Problema: Catastrophic forgetting ao descongelar backbone

**Root Cause: Negative Transfer (Yosinski et al., 2014)**

| Aspecto | Stage 1 | Stage 2 |
|---------|---------|---------|
| **Task** | Binary (NONE vs PARTITION) | 3-way (SPLIT vs RECT vs AB) |
| **Features necess√°rias** | "Tem parti√ß√£o?" | "Tipo de parti√ß√£o?" |
| **Foco visual** | Detec√ß√£o de bordas | Padr√µes geom√©tricos |
| **Distribui√ß√£o** | 50/50 balanceado | Long-tail (SPLIT minorit√°ria) |
| **Complexidade** | Simples (presen√ßa/aus√™ncia) | Complexa (geometria) |

**Conclus√£o:** Tasks s√£o **FUNDAMENTALMENTE DIFERENTES** ‚Üí Features do Stage 1 prejudicam Stage 2

### Experimentos Realizados

#### ‚ùå Experimento 1: ULMFiT (07/10/2025)
- **T√©cnicas:** Gradual unfreezing, discriminative LR, cosine annealing
- **Resultado:** Frozen F1=46.51% ‚Üí Unfrozen F1=34.12% (-26.6%)
- **Conclus√£o:** Catastrophic forgetting N√ÉO foi prevenido

#### ‚ùå Experimento 2: Train from Scratch (13/10/2025)
- **Implementa√ß√£o:** ImageNet-only pretrained (sem Stage 1 backbone)
- **Resultado:** Best F1=37.38% (√©poca 26)
- **Conclus√£o:** Elimina catastrophic forgetting, mas F1 inferior ao frozen

#### ‚ùå Experimento 3: Flatten Architecture (feat/stage2-flatten-9classes)
- **Hip√≥tese H2.1:** Distribution shift causa colapso no pipeline
- **Implementa√ß√£o:** Retreinar Stage 2 com samples filtrados pelo Stage 1 (threshold=0.45)
- **Dataset:** 152k ‚Üí 3,890 samples (2.55% reten√ß√£o)
- **Resultado:** F1=6.74% (√©poca 9) - **PIOR que baseline** ‚ùå‚ùå‚ùå
- **Conclus√£o:** H2.1 **REJEITADA** - Distribution shift N√ÉO √© a causa prim√°ria
- **Documenta√ß√£o:** `docs_v6/08_pipeline_aware_training.md` (474 linhas, 9 se√ß√µes, 16 refs)

### ‚úÖ DECIS√ÉO FINAL: Usar Frozen Model (√âpoca 8)

**Raz√µes Baseadas em Evid√™ncias:**

| Abordagem | F1 Obtido | Catastrophic Forgetting | Meta (‚â•45%) | Status |
|-----------|-----------|------------------------|-------------|--------|
| **ULMFiT Frozen** | **46.51%** | N/A (n√£o unfrozen) | ‚úÖ **SIM** | ‚≠ê **USADO** |
| ULMFiT Unfrozen | 34.12% | ‚ùå SIM (-26.6%) | ‚ùå N√ÉO | Descartado |
| Train from Scratch | 37.38% | ‚úÖ N√ÉO (+315%) | ‚ùå N√ÉO | Descartado |
| Pipeline-Aware | 6.74% | N/A | ‚ùå N√ÉO | ‚ùå **ABANDONADO** |

**Fundamenta√ß√£o Cient√≠fica:**
> Raghu et al. (2019) - "Transfusion: Understanding Transfer Learning"  
> "Frozen ImageNet features often outperform fine-tuned models when target task is very different from source task."

**Checkpoint Usado:** `stage2_model_block16_classweights_ep8.pt`

---

## üìä Performance Pipeline V6 (Script 008 - 13/10/2025)

### Resultados Obtidos

| M√©trica | Valor | Status |
|---------|-------|--------|
| **Overall Accuracy** | **47.66%** | ‚ö†Ô∏è -0.34pp da meta (48%) |
| Macro F1 | 13.38% | ‚ö†Ô∏è Muito baixo |
| NONE F1 | 78.88% | ‚úÖ Excelente |
| SPLIT F1 | 8.83% | ‚ö†Ô∏è Baixo |
| **HORZ F1** | **0.00%** | ‚ùå **Colapsado** |
| VERT F1 | 10.92% | ‚ö†Ô∏è Baixo |
| HORZ_A F1 | 0.00% | ‚ùå Colapsado |
| HORZ_B F1 | 8.45% | ‚ö†Ô∏è Baixo |
| VERT_A F1 | 0.00% | ‚ùå Colapsado |
| VERT_B F1 | 0.00% | ‚ùå Colapsado |

### Problemas Identificados

1. **Erro em Cascata Dominante**
   - Stage 3-RECT: 4.49% accuracy (standalone: 68.44%) ‚Üí **-93.4% degrada√ß√£o**
   - Stage 3-AB: 1.51% accuracy (standalone: 24.50%) ‚Üí **-93.8% degrada√ß√£o**
   - Root cause: Stage 2 envia samples errados para Stage 3

2. **HORZ Completamente Colapsado**
   - Ground truth: 9,618 samples (10.59%)
   - Predictions: 0 (0.00%)
   - Hip√≥tese: Stage 3-RECT tem vi√©s extremo para VERT

3. **Classes AB Colapsadas no Pipeline**
   - HORZ_A, VERT_A, VERT_B: 0% predictions
   - Stage 3-AB n√£o funciona em cascata

---

## üéØ PR√ìXIMOS PASSOS: Fechar Gap -0.34pp

### Estrat√©gia: ROI Maximizado
> "Come√ßar com t√©cnicas de baixo custo e alto impacto. Avaliar resultados antes de investir em solu√ß√µes complexas."

### Fase 1: Quick Wins (3-4 dias) - **RECOMENDADA**

**Objetivo:** Alcan√ßar ‚â•48% accuracy  
**Ganho Esperado:** +0.3-0.7pp ‚Üí **Accuracy 47.9-48.4%** ‚úÖ

#### 1.1 Investigar Stage 3-RECT Standalone (2h) üî¥ **ALTA PRIORIDADE**

**Problema:**
> "Stage 3-RECT tem F1=68.44% standalone, mas apenas 4.49% no pipeline. Por qu√™?"

**Hip√≥teses:**
1. Modelo tem vi√©s extremo para VERT (explica HORZ=0%)
2. Modelo n√£o generaliza para samples enviados erroneamente por Stage 2
3. Dataset de treinamento desbalanceado

**Protocolo:**
```bash
# Script: pesquisa_v6/scripts/009_diagnose_stage3_rect.py
python3 009_diagnose_stage3_rect.py \
  --model pesquisa_v6/logs/v6_experiments/stage3_rect/stage3_rect_model_best.pt \
  --dataset pesquisa_v6/v6_dataset_stage3/RECT/block_16/val.pt

# Analisar:
# - Confusion matrix standalone
# - F1 per-class (HORZ vs VERT)
# - Class distribution no dataset train
```

**A√ß√µes baseadas em resultados:**
- Se HORZ F1 < 50% standalone ‚Üí Retreinar com weighted loss (1 dia)
- Se HORZ samples < 40% train ‚Üí Rebalancear dataset (usar sampler)

**Ganho:** +0.1-0.3pp

#### 1.2 Threshold Grid Search (2h) üî¥ **ALTA PRIORIDADE**

**Problema:**
> "Threshold Stage 1 = 0.45 foi otimizado isoladamente. No pipeline, pode estar enviando muitos false positives para Stage 2."

**Protocolo:**
```python
# Script: pesquisa_v6/scripts/010_threshold_grid_search.py
thresholds_stage1 = [0.40, 0.45, 0.50, 0.55]
results = []

for th1 in thresholds_stage1:
    accuracy = run_pipeline(stage1_threshold=th1)
    results.append({'th1': th1, 'accuracy': accuracy})

best = max(results, key=lambda x: x['accuracy'])
```

**Custo:** 40 min runs + 1h an√°lise = 2h  
**Ganho:** +0.1-0.3pp

#### 1.3 Stage 2 Strong Data Augmentation (1 dia) üü° **M√âDIA PRIORIDADE**

**Problema:**
> "Stage 2 F1=46.51% pode melhorar com augmentation mais agressiva."

**T√©cnicas:**
- **MixUp** (Zhang et al., 2018): Œ±=0.4
- **CutMix** (Yun et al., 2019): Œ≤=1.0
- **Geometric Augmentations**: Flips + Rotation

**Protocolo:**
```bash
python3 004_train_stage2_redesigned.py \
  --epochs 30 \
  --mixup-alpha 0.4 \
  --cutmix-beta 1.0 \
  --output-dir stage2_scratch_augstrong
```

**Custo:** 6h treinamento + 2h an√°lise = 1 dia  
**Ganho:** +0.2-0.4pp

**Ganho Total Fase 1:** +0.4-1.0pp ‚Üí **Accuracy 48.0-48.7%** ‚úÖ

---

## üîß Melhorias T√©cnicas Implementadas (v6)

### 1. Backbone Upgrade
```python
# ResNet-18 + SE-Blocks + Dropout progressivo
- Squeeze-and-Excitation blocks (channel attention)
- Spatial Attention Module (SAM)
- Dropout progressivo: [0.1, 0.2, 0.3, 0.4]
- Group Normalization
```

### 2. Loss Functions

- **Stage 1:** FocalLoss(Œ±=0.25, Œ≥=2.5) + HardNegativeMining(ratio=3:1)
- **Stage 2:** ClassBalancedFocalLoss(Œ≤=0.9999, Œ≥=2.0)
- **Stage 3-AB:** Mixup(Œ±=0.4) + FocalLoss(Œ≥=2.0)

### 3. Data Augmentation Strategy

| Stage | Augmentation |
|-------|--------------|
| Stage 1 | Basic: HFlip, VFlip, Rotation, GaussianNoise |
| Stage 2 | Medium: Stage 1 + Cutout, GridShuffle |
| Stage 3-AB | **Heavy**: Flips com label swap, Mixup, CoarseDropout |

### 4. Training Strategy

#### Stage 1: Backbone Pre-training
- Epochs: 20, LR: 1e-3 ‚Üí 1e-5 (cosine decay)
- Result: F1=72.28% ‚úÖ

#### Stage 2: Fine-tuning (FROZEN-ONLY)
- Epochs: 8 (frozen), LR: 5e-4
- Result: F1=46.51% ‚úÖ
- **Decis√£o:** N√ÉO descongelar (negative transfer)

#### Stage 3: Specialists
- Epochs: 30 (RECT), 50 (AB)
- LR: 3e-4 ‚Üí 3e-6
- Freeze: Backbone sempre (evita catastrophic forgetting)

---

## üìÅ File Structure

```
pesquisa_v6/
‚îú‚îÄ‚îÄ PLANO_v6_Out.md                      # ‚≠ê Este documento (consolidado)
‚îÇ
‚îú‚îÄ‚îÄ v6_pipeline/                         # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                      # ‚úÖ 
‚îÇ   ‚îú‚îÄ‚îÄ data_hub.py                      # ‚úÖ Dataset utils
‚îÇ   ‚îú‚îÄ‚îÄ models.py                        # ‚úÖ Backbones + Heads
‚îÇ   ‚îú‚îÄ‚îÄ losses.py                        # ‚úÖ Focal, CB, Mixup losses
‚îÇ   ‚îú‚îÄ‚îÄ augmentation.py                  # ‚úÖ Aug pipelines
‚îÇ   ‚îú‚îÄ‚îÄ ensemble.py                      # ‚úÖ AB ensemble logic
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                       # ‚úÖ Evaluation utils
‚îÇ
‚îú‚îÄ‚îÄ scripts/                             # Training scripts
‚îÇ   ‚îú‚îÄ‚îÄ 001_prepare_v6_dataset.py        # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 002_prepare_v6_stage3_datasets.py # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 003_train_stage1_improved.py     # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 004_train_stage2_redesigned.py   # ‚úÖ Validado (frozen model)
‚îÇ   ‚îú‚îÄ‚îÄ 005_train_stage3_rect.py         # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 006_train_stage3_ab_fgvc.py      # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 007_optimize_thresholds.py       # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 008_run_pipeline_eval_v6.py      # ‚úÖ Validado
‚îÇ   ‚îî‚îÄ‚îÄ 009_compare_v5_v6.py             # ‚è≥ Pr√≥ximo
‚îÇ
‚îú‚îÄ‚îÄ docs_v6/                             # Documenta√ß√£o t√©cnico-cient√≠fica
‚îÇ   ‚îú‚îÄ‚îÄ 04_experimento_train_from_scratch.md  # Train from scratch (F1=37.38%)
‚îÇ   ‚îú‚îÄ‚îÄ 06_arquitetura_flatten_9classes.md    # Flatten architecture design
‚îÇ   ‚îú‚îÄ‚îÄ 07_flatten_pipeline_evaluation.md     # Baseline flatten (F1=10.24%)
‚îÇ   ‚îî‚îÄ‚îÄ 08_pipeline_aware_training.md         # Pipeline-aware NEGATIVO (F1=6.74%)
‚îÇ
‚îî‚îÄ‚îÄ logs/                                # Results
    ‚îî‚îÄ‚îÄ v6_experiments/
        ‚îú‚îÄ‚îÄ stage1/                      # Stage 1 checkpoints
        ‚îú‚îÄ‚îÄ stage2/                      # Stage 2 frozen model (USADO)
        ‚îú‚îÄ‚îÄ stage2_scratch/              # Stage 2 train from scratch
        ‚îú‚îÄ‚îÄ stage2_pipeline_aware/       # Pipeline-aware (ABANDONADO)
        ‚îú‚îÄ‚îÄ stage3_rect/                 # Stage 3-RECT checkpoints
        ‚îî‚îÄ‚îÄ stage3_ab/                   # Stage 3-AB FGVC checkpoints
```

---

## üß™ Experimentos Flatten (ARQUIVADOS - NEGATIVOS)

**Branch:** `feat/stage2-flatten-9classes` (merged to main em 904e0aa)  
**Dura√ß√£o:** ~3 dias (10-13 outubro 2025)  
**Status:** ‚ùå **ABANDONADO** - Resultados negativos

### Resumo dos Experimentos

| Experimento | F1-macro | Accuracy | Status |
|-------------|----------|----------|--------|
| Stage 2 isolado (004b) | 31.65% | 37.17% | ‚úÖ Funciona |
| Pipeline baseline (008b) | 10.24% | 57.93% | ‚ö†Ô∏è Acc boa, F1 ruim |
| Pipeline-aware (004c) | 6.74% | 8.50% | ‚ùå‚ùå‚ùå **PIOR** |

### Hip√≥tese H2.1 (distribution shift): **REJEITADA**

**Hip√≥tese:**
> "Stage 2 colapsa no pipeline porque treina com distribui√ß√£o balanceada mas recebe distribui√ß√£o filtrada pelo Stage 1. Solu√ß√£o: retreinar Stage 2 com samples filtrados."

**Resultado:**
- F1 piorou de 31.65% ‚Üí 6.74% (-78.7% degrada√ß√£o)
- Hip√≥tese H2.1 **falsificada experimentalmente**

**Root Causes Identificados:**
1. **Dataset too small:** 3,890 samples / 11.3M params = 1:2,900 ratio (need 1:10-100)
2. **Negative transfer:** Stage 1 binary features incompat√≠veis com Stage 2 multi-class
3. **Architectural flaw:** Objetivos conflitantes (Stage 1 vs Stage 2)

**Conclus√£o Cient√≠fica:**
> Flatten architecture fundamentalmente falha para predi√ß√£o de parti√ß√µes AV1. Negative transfer confirmado experimentalmente. Recommendation: Retornar √† hier√°rquica V6.

**Documenta√ß√£o Completa:** `docs_v6/08_pipeline_aware_training.md` (474 linhas, 9 se√ß√µes, 16 refer√™ncias)

---

## üìù Success Criteria

### ‚úÖ Crit√©rios M√≠nimos (Fase 1) - **QUASE ATINGIDOS**

- [x] Stage 1 F1 ‚â• 68% ‚Üí **72.28%** ‚úÖ
- [x] Stage 1 Precision ‚â• 62% ‚Üí **67.13%** ‚úÖ
- [x] Stage 2 Macro F1 ‚â• 45% ‚Üí **46.51%** ‚úÖ
- [ ] Stage 3-AB F1 ‚â• 40% ‚Üí **24.50%** ‚ùå (gap: -15.5pp)
- [ ] Final Accuracy ‚â• 48% ‚Üí **47.66%** ‚ö†Ô∏è (gap: -0.34pp)

### ‚≠ï Crit√©rios Ideais (Fase 2)

- [ ] Stage 1 F1 ‚â• 72% ‚Üí **72.28%** ‚úÖ
- [ ] Stage 1 Precision ‚â• 68% ‚Üí **67.13%** ‚ö†Ô∏è (gap: -0.87pp)
- [ ] Stage 2 Macro F1 ‚â• 52% ‚Üí **46.51%** ‚ùå (gap: -5.5pp)
- [ ] Stage 3-AB F1 ‚â• 55% ‚Üí **24.50%** ‚ùå (gap: -30.5pp)
- [ ] Final Accuracy ‚â• 55% ‚Üí **47.66%** ‚ùå (gap: -7.34pp)

---

## üìö Refer√™ncias Cient√≠ficas

### Negative Transfer & Transfer Learning
1. **Yosinski et al., 2014:** "How transferable are features in deep neural networks?"
2. **Raghu et al., 2019:** "Transfusion: Understanding Transfer Learning"
3. **Kornblith et al., 2019:** "Do Better ImageNet Models Transfer Better?"

### Loss Functions
4. **Lin et al., 2017:** "Focal Loss for Dense Object Detection"
5. **Cui et al., 2019:** "Class-Balanced Loss Based on Effective Number of Samples"
6. **Zhang et al., 2018:** "mixup: Beyond Empirical Risk Minimization"

### Catastrophic Forgetting
7. **Goodfellow et al., 2013:** "An Empirical Investigation of Catastrophic Forgetting"
8. **Howard & Ruder, 2018:** "Universal Language Model Fine-tuning (ULMFiT)"
9. **Kirkpatrick et al., 2017:** "Overcoming catastrophic forgetting"

### Data Augmentation & Robustness
10. **Yun et al., 2019:** "CutMix: Regularization Strategy to Train Strong Classifiers"
11. **DeVries & Taylor, 2017:** "Improved Regularization with Cutout"
12. **Hendrycks et al., 2019:** "Using Pre-Training Can Improve Model Robustness"

### Attention Mechanisms
13. **Hu et al., 2018:** "Squeeze-and-Excitation Networks"
14. **Woo et al., 2018:** "CBAM: Convolutional Block Attention Module"

### Model Calibration
15. **Guo et al., 2017:** "On Calibration of Modern Neural Networks"
16. **M√ºller et al., 2019:** "When Does Label Smoothing Help?"

---

## ü§ù Pr√≥ximo Passo Imediato

### Decis√£o: Executar Fase 1 (3-4 dias)

**Sequ√™ncia Recomendada:**

```bash
# 1. Dia 1 manh√£: Diagnose Stage 3-RECT (2h)
python3 pesquisa_v6/scripts/009_diagnose_stage3_rect.py

# 2. Dia 1 tarde: Threshold grid search (2h)
python3 pesquisa_v6/scripts/010_threshold_grid_search.py

# 3. Dia 2-3: Strong augmentation (1 dia) - SE NECESS√ÅRIO
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --mixup-alpha 0.4 --cutmix-beta 1.0 --epochs 30

# 4. Dia 4: Pipeline re-evaluation + an√°lise
python3 pesquisa_v6/scripts/008_run_pipeline_eval_v6.py
```

**Checkpoint de Decis√£o:**
- ‚úÖ Se accuracy ‚â• 48%: **PARAR**, documentar, finalizar
- ‚ö†Ô∏è Se 47.8-48%: Considerar 1.3 (augmentation)
- ‚ùå Se < 47.8%: Avaliar Fase 2 ou aceitar resultado

**Meta:** Fechar gap de -0.34pp e atingir ‚â•48% accuracy para finalizar Pipeline v6.

---

**Status Final:** üü° Em Desenvolvimento - 8/9 scripts validados  
**Pr√≥ximo:** Script 009 (compare v5/v6) OU Fase 1 Quick Wins  
**√öltima Atualiza√ß√£o:** 13 de outubro de 2025  
**Respons√°vel:** @chiarorosa
