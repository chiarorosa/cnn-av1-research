# Pipeline v6 - Plano de Desenvolvimento Consolidado

**Data:** 13 de outubro de 2025  
**Status:** üü° Em Desenvolvimento - Scripts 8/9 conclu√≠dos  
**√öltima Atualiza√ß√£o:** 13/10/2025  
**Respons√°vel:** @chiarorosa

---

## üìã Sum√°rio Executivo

O pipeline v6 √© uma reformula√ß√£o completa da arquitetura hier√°rquica, focando em resolver os **3 problemas cr√≠ticos** identificados na v5:

1. **Stage 1**: Baixa precis√£o (53.71%) ‚Üí 27k falsos positivos contaminando o pipeline
2. **Stage 2**: Confus√£o entre macro-classes (33.41% Macro F1) ‚Üí Erros propagados
3. **Stage 3-AB**: Colapso total do especialista (25.26% F1) ‚Üí Classes perdidas

### Progresso Atual (Atualizado 13/10/2025)

| Componente | Status | Resultado |
|------------|--------|-----------|
| **Scripts 001-002** | ‚úÖ Validado | Dataset preparado (152k train / 38k val) |
| **Script 003 (Stage 1)** | ‚úÖ Validado | F1=72.28% (√©poca 19) - **META ATINGIDA** ‚â•68% |
| **Script 004 (Stage 2)** | ‚úÖ **RESOLVIDO** | F1=46.51% (frozen model √©poca 8) - **META ATINGIDA** ‚â•45% |
| **Script 005 (Stage 3-RECT)** | ‚úÖ Validado | F1=68.44% (√©poca 12) |
| **Script 006 (Stage 3-AB)** | ‚úÖ Validado | F1=24.50% (4/4 classes, √©poca 6) |
| **Script 007 (Threshold)** | ‚úÖ Validado | threshold=0.45 ‚Üí F1=72.79% |
| **Script 008 (Pipeline)** | ‚úÖ Validado | Accuracy=47.66% (meta: 48%, gap: -0.34pp) |
---

## üéØ Objetivos e Metas

### M√©tricas Alvo (block_16)

| Componente | Meta Fase 1 | Meta Fase 2 | v6 Obtido (13/10) | Status |
|------------|-------------|-------------|-------------------|--------|
| **Stage 1 F1** | 68-70% | 72-75% | ‚úÖ **72.28%** | Meta Fase 2 atingida |
| **Stage 1 Precis√£o** | 62-65% | 68-72% | ‚úÖ **67.13%** | Meta Fase 1 atingida |
| **Stage 2 Macro F1** | 45-50% | 55-60% | ‚úÖ **46.51%** (frozen) | Meta Fase 1 atingida |
| **Stage 3-RECT F1** | 75-78% | 80-83% | ‚ö†Ô∏è **68.44%** | **-6.6pp abaixo da meta** |
| **Stage 3-AB F1** | 45-50% | 60-65% | ‚ö†Ô∏è **24.50%** | **-20.5pp abaixo da meta** |
| **Acur√°cia Final** | 48-52% | 58-63% | ‚ö†Ô∏è **47.66%** | **-0.34pp abaixo da meta** |

---

## üèóÔ∏è Arquitetura Proposta

### Estrat√©gia Principal: Hierarquia Revisada (3 Est√°gios Otimizados)

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Input Block    ‚îÇ
                    ‚îÇ    (16x16)      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Backbone      ‚îÇ
                    ‚îÇ  (ResNet-18+)   ‚îÇ
                    ‚îÇ   + SE-Blocks   ‚îÇ
                    ‚îÇ   + Attention   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   STAGE 1 (Binary)  ‚îÇ
                    ‚îÇ  NONE vs PARTITION  ‚îÇ
                    ‚îÇ   + Focal Loss      ‚îÇ
                    ‚îÇ   + Threshold=0.45  ‚îÇ
                    ‚îÇ   F1=72.28% ‚úÖ      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   IF NONE   ‚îÇ‚îÄ‚îÄ‚Üí OUTPUT: PARTITION_NONE
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   STAGE 2 (3-way)  ‚îÇ
                    ‚îÇ  SPLIT | RECT | AB ‚îÇ
                    ‚îÇ  + CB-Focal Loss   ‚îÇ
                    ‚îÇ  F1=46.51% (frozen)‚îÇ
                    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ        ‚îÇ    ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  SPLIT    ‚îÇ  ‚îÇ  STAGE 3-RECT ‚îÇ
              ‚îÇ  (direct) ‚îÇ  ‚îÇ  HORZ vs VERT ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  F1=68.44% ‚úÖ ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ   STAGE 3-AB         ‚îÇ
                             ‚îÇ  HORZ_A | HORZ_B |   ‚îÇ
                             ‚îÇ  VERT_A | VERT_B     ‚îÇ
                             ‚îÇ  + FGVC Techniques   ‚îÇ
                             ‚îÇ  F1=24.50% ‚ö†Ô∏è        ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Mudan√ßas Chave vs v5

1. **Stage 2 Simplificado**: 
   - ‚ùå Remove classe "NONE" (j√° filtrada em Stage 1)
   - ‚ùå Remove classe "1TO4" (n√£o existe no dataset)
   - ‚úÖ Apenas 3 classes: SPLIT, RECT, AB
   - ‚úÖ Reduz confus√£o e melhora separabilidade

2. **Stage 1 Melhorado**:
   - ‚úÖ Focal Loss com Œ≥=2.5 (mais agressivo)
   - ‚úÖ Hard Negative Mining
   - ‚úÖ Threshold otimizado = 0.45 (via script 007)
   - ‚úÖ Calibra√ß√£o de probabilidades (Temperature Scaling)

3. **Stage 3-AB Redesenhado**:
   - ‚úÖ FGVC (Fine-Grained Visual Classification) techniques
   - ‚úÖ Center Loss para compacta√ß√£o intra-classe
   - ‚úÖ CutMix augmentation
   - ‚úÖ Cosine classifier com temperature scaling
   - ‚úÖ 4/4 classes funcionando (vs. v5 com 1 classe colapsada)

---

## üî¥ PROBLEMA CR√çTICO: Stage 2 Catastrophic Forgetting (RESOLVIDO)

### Diagn√≥stico do Problema

**Treinamento Original (antes ULMFiT):**
- √âpoca 1 (frozen): F1=**47.58%** ‚úÖ
- √âpoca 3 (unfrozen): F1=**34-38%** ‚ùå
- Problema: Catastrophic forgetting ao descongelar backbone

**Root Cause: Negative Transfer (Yosinski et al., 2014)**

| Aspecto | Stage 1 | Stage 2 |
|---------|---------|---------|
| **Task** | Binary (NONE vs PARTITION) | 3-way (SPLIT vs RECT vs AB) |
| **Features necess√°rias** | "Tem parti√ß√£o?" | "Tipo de parti√ß√£o?" |
| **Foco visual** | Detec√ß√£o de bordas | Padr√µes geom√©tricos |
| **Distribui√ß√£o** | 50/50 balanceado | Long-tail (SPLIT minorit√°ria) |
| **Complexidade** | Simples (presen√ßa/aus√™ncia) | Complexa (geometria) |

**Conclus√£o:** Tasks s√£o **FUNDAMENTALMENTE DIFERENTES** ‚Üí Features do Stage 1 prejudicam Stage 2

### Experimentos Realizados

#### ‚ùå Experimento 1: ULMFiT (07/10/2025)
- **T√©cnicas:** Gradual unfreezing, discriminative LR, cosine annealing
- **Resultado:** Frozen F1=46.51% ‚Üí Unfrozen F1=34.12% (-26.6%)
- **Conclus√£o:** Catastrophic forgetting N√ÉO foi prevenido

#### ‚ùå Experimento 2: Train from Scratch (13/10/2025)
- **Implementa√ß√£o:** ImageNet-only pretrained (sem Stage 1 backbone)
- **Resultado:** Best F1=37.38% (√©poca 26)
- **Conclus√£o:** Elimina catastrophic forgetting, mas F1 inferior ao frozen

#### ‚ùå Experimento 3: Flatten Architecture (feat/stage2-flatten-9classes)
- **Hip√≥tese H2.1:** Distribution shift causa colapso no pipeline
- **Implementa√ß√£o:** Retreinar Stage 2 com samples filtrados pelo Stage 1 (threshold=0.45)
- **Dataset:** 152k ‚Üí 3,890 samples (2.55% reten√ß√£o)
- **Resultado:** F1=6.74% (√©poca 9) - **PIOR que baseline** ‚ùå‚ùå‚ùå
- **Conclus√£o:** H2.1 **REJEITADA** - Distribution shift N√ÉO √© a causa prim√°ria
- **Documenta√ß√£o:** `docs_v6/08_pipeline_aware_training.md` (474 linhas, 9 se√ß√µes, 16 refs)

### ‚úÖ DECIS√ÉO FINAL: Usar Frozen Model (√âpoca 8)

**Raz√µes Baseadas em Evid√™ncias:**

| Abordagem | F1 Obtido | Catastrophic Forgetting | Meta (‚â•45%) | Status |
|-----------|-----------|------------------------|-------------|--------|
| **ULMFiT Frozen** | **46.51%** | N/A (n√£o unfrozen) | ‚úÖ **SIM** | ‚≠ê **USADO** |
| ULMFiT Unfrozen | 34.12% | ‚ùå SIM (-26.6%) | ‚ùå N√ÉO | Descartado |
| Train from Scratch | 37.38% | ‚úÖ N√ÉO (+315%) | ‚ùå N√ÉO | Descartado |
| Pipeline-Aware | 6.74% | N/A | ‚ùå N√ÉO | ‚ùå **ABANDONADO** |

**Fundamenta√ß√£o Cient√≠fica:**
> Raghu et al. (2019) - "Transfusion: Understanding Transfer Learning"  
> "Frozen ImageNet features often outperform fine-tuned models when target task is very different from source task."

**Checkpoint Usado:** `stage2_model_block16_classweights_ep8.pt`

---

## üìä Performance Pipeline V6 (Script 008 - 13/10/2025)

### Resultados Obtidos

| M√©trica | Valor | Status |
|---------|-------|--------|
| **Overall Accuracy** | **47.66%** | ‚ö†Ô∏è -0.34pp da meta (48%) |
| Macro F1 | 13.38% | ‚ö†Ô∏è Muito baixo |
| NONE F1 | 78.88% | ‚úÖ Excelente |
| SPLIT F1 | 8.83% | ‚ö†Ô∏è Baixo |
| **HORZ F1** | **0.00%** | ‚ùå **Colapsado** |
| VERT F1 | 10.92% | ‚ö†Ô∏è Baixo |
| HORZ_A F1 | 0.00% | ‚ùå Colapsado |
| HORZ_B F1 | 8.45% | ‚ö†Ô∏è Baixo |
| VERT_A F1 | 0.00% | ‚ùå Colapsado |
| VERT_B F1 | 0.00% | ‚ùå Colapsado |

### Problemas Identificados

1. **Erro em Cascata Dominante**
   - Stage 3-RECT: 4.49% accuracy (standalone: 68.44%) ‚Üí **-93.4% degrada√ß√£o**
   - Stage 3-AB: 1.51% accuracy (standalone: 24.50%) ‚Üí **-93.8% degrada√ß√£o**
   - Root cause: Stage 2 envia samples errados para Stage 3

2. **HORZ Completamente Colapsado**
   - Ground truth: 9,618 samples (10.59%)
   - Predictions: 0 (0.00%)
   - Hip√≥tese: Stage 3-RECT tem vi√©s extremo para VERT

3. **Classes AB Colapsadas no Pipeline**
   - HORZ_A, VERT_A, VERT_B: 0% predictions
   - Stage 3-AB n√£o funciona em cascata

---

## üéØ AN√ÅLISE DOS PROBLEMAS CR√çTICOS (Baseado em docs_v6/)

### Problema Central: Erro em Cascata Severo

**Diagn√≥stico (doc 05_avaliacao_pipeline_completo.md):**

| Componente | F1 Standalone | Accuracy Pipeline | Degrada√ß√£o |
|------------|---------------|-------------------|------------|
| Stage 3-RECT | 68.44% | **4.49%** | **-93.4%** ‚ùå‚ùå‚ùå |
| Stage 3-AB | 24.50% | **1.51%** | **-93.8%** ‚ùå‚ùå‚ùå |

**Classes Colapsadas no Pipeline:**
- HORZ: 0% F1 (9,618 samples preditos como 0)
- HORZ_A: 0% F1
- VERT_A: 0% F1
- VERT_B: 0% F1

**Root Cause Identificado:** Stage 2 confunde sistematicamente RECT vs AB

### Hip√≥teses Testadas e REJEITADAS

#### ‚ùå H2.1: Distribution Shift (doc 08_pipeline_aware_training.md)

**Hip√≥tese:**
> "Stage 2 colapsa porque treinou com distribui√ß√£o balanceada mas recebe distribui√ß√£o filtrada por Stage 1."

**Teste:**
- Retreinar Stage 2 com 3,890 samples filtrados por Stage 1 (threshold 0.45)
- **Resultado:** F1=6.74% (pior que baseline 31.65%) - **degrada√ß√£o de -78.7%**

**Conclus√£o:** REJEITADA - Distribution shift N√ÉO √© a causa prim√°ria

**Root causes reais identificados:**
1. Dataset insuficiente (3,890 / 11M params = 1:2,900 ratio)
2. Negative transfer (Stage 1 binary features ‚â† Stage 2 multi-class)
3. Architectural flaw (objetivos conflitantes)

---

## üéØ PR√ìXIMOS PASSOS: Resolver Erro em Cascata

### Estrat√©gia Baseada em Evid√™ncias
> "Focar no problema real: Stage 2 confunde RECT vs AB, causando colapso dos Stage 3."

### Fase 1: Diagn√≥stico Profundo (1-2 dias) - **CR√çTICO**

#### 1.1 Analisar Confus√£o RECT vs AB no Stage 2 üî¥ **CR√çTICO**

**Problema Documentado (doc 05):**
```
Ground Truth: HORZ (RECT)
    ‚Üì
Stage 2 Frozen classifica como: AB (ERRADO!)
    ‚Üì
Envia para Stage 3-AB (que nunca viu HORZ!)
    ‚Üì
Stage 3-AB colapsa ‚Üí prediz HORZ_B (default)
    ‚Üì
Resultado: HORZ_B (ERRADO!)
```

**Protocolo:**
```bash
# Script: pesquisa_v6/scripts/009_analyze_stage2_confusion.py

# 1. Carregar Stage 2 frozen model
# 2. Inferir validation set completo
# 3. Analisar confusion matrix RECT vs AB
# 4. Identificar padr√µes visuais que causam confus√£o
# 5. Calcular % de RECT enviado erroneamente para Stage 3-AB
```

**M√©tricas-chave:**
- Precision RECT (Stage 2): Quantos "RECT" s√£o realmente RECT?
- Recall AB (Stage 2): Quantos AB s√£o corretamente identificados?
- Taxa de "RECT ‚Üí AB error": % de RECT enviados para Stage 3-AB

**Ganho esperado:** Entendimento claro do erro cascata

#### 1.2 Avaliar Vi√©s do Stage 3-RECT (2h) ÔøΩ **CR√çTICO**

**Problema Observado:** HORZ colapsou (0% F1), VERT superestimado (+16.19%)

**Hip√≥tese:** Stage 3-RECT tem vi√©s extremo para VERT

**Protocolo:**
```bash
# Script: pesquisa_v6/scripts/010_diagnose_stage3_rect.py

python3 010_diagnose_stage3_rect.py \
  --model logs/v6_experiments/stage3_rect/stage3_rect_model_best.pt \
  --dataset v6_dataset_stage3/RECT/block_16/val.pt

# An√°lises:
# 1. Confusion matrix standalone (HORZ vs VERT)
# 2. F1 per-class standalone  
# 3. Distribui√ß√£o de probabilidades
# 4. Class distribution no dataset de treino
```

**A√ß√µes baseadas em resultado:**
- Se HORZ F1 < 60% standalone ‚Üí Retreinar com class weights
- Se dataset desbalanceado (VERT > 60%) ‚Üí Rebalancear com sampler

**Ganho esperado:** +0.1-0.3pp (se vi√©s confirmado e corrigido)

### Fase 2: Solu√ß√µes Robustas (3-5 dias) - **RECOMENDADA**

**Objetivo:** Resolver confus√£o RECT vs AB + Robustez dos Stage 3

#### 2.1 Noise Injection em Stage 3 (3 dias) ÔøΩ **ALTA PRIORIDADE**

**Problema Fundamental:**
> "Stage 3-RECT e Stage 3-AB foram treinados apenas com samples CORRETOS. No pipeline, recebem samples ERRADOS do Stage 2 e colapsam."

**Solu√ß√£o:** Adversarial Training / Noise Injection
- Treinar Stage 3 com 20-30% "dirty samples"
- Simula distribui√ß√£o real que Stage 3 receber√° no pipeline
- Modelo aprende robustez a erros do Stage 2

**Stage 3-RECT Robusto:**
```python
# Durante treinamento
for epoch in range(epochs):
    for batch in dataloader_RECT:
        x_rect, y_rect = batch
        
        # 20-30% das vezes, injetar sample AB
        if np.random.rand() < 0.25:
            idx = np.random.randint(len(dataset_AB))
            x_noise, y_noise = dataset_AB[idx]
            
            # Substituir um sample RECT por AB
            x_rect[0] = x_noise
            y_rect[0] = np.random.choice([0, 1])  # Random label
            
        loss = criterion(model(x_rect), y_rect)
```

**Protocolo:**
```bash
# 1. Retreinar Stage 3-RECT com noise
python3 005_train_stage3_rect.py \
  --noise-injection 0.25 \
  --noise-source AB \
  --noise-source SPLIT \
  --epochs 30 \
  --output-dir logs/v6_experiments/stage3_rect_robust

# 2. Retreinar Stage 3-AB com noise  
python3 006_train_stage3_ab_fgvc.py \
  --noise-injection 0.25 \
  --noise-source RECT \
  --noise-source SPLIT \
  --epochs 30 \
  --output-dir logs/v6_experiments/stage3_ab_robust

# 3. Re-avaliar pipeline
python3 008_run_pipeline_eval_v6.py \
  --stage3-rect-model stage3_rect_robust/model_best.pt \
  --stage3-ab-model stage3_ab_robust/model_best.pt
```

**Fundamenta√ß√£o Te√≥rica:**
- Hendrycks et al., 2019: "Using Pre-Training Can Improve Model Robustness"
- Natarajan et al., 2013: "Learning with Noisy Labels"
- Recht et al., 2019: "Do ImageNet Classifiers Generalize to ImageNet?"

**Ganho Esperado:**
- Stage 3-RECT pipeline accuracy: 4.49% ‚Üí 15-25% (+234-457%)
- Stage 3-AB pipeline accuracy: 1.51% ‚Üí 5-10% (+231-562%)
- Overall pipeline: **+1.0-2.5pp** ‚Üí Accuracy 48.7-50.2% ‚úÖ‚úÖ

**Custo:**
- 1.5 dia retreino Stage 3-RECT (30 epochs)
- 1.5 dia retreino Stage 3-AB (30 epochs)
- 0.5 dia pipeline evaluation + an√°lise
- **Total:** 3.5 dias

#### 2.2 Melhorar Separa√ß√£o RECT vs AB no Stage 2 (2 dias) üü° **M√âDIA PRIORIDADE**

**Problema:** Stage 2 confunde RECT vs AB (causa do erro cascata)

**Solu√ß√£o 1: Contrastive Learning**
- Adicionar contrastive loss (Chen et al., 2020 - SimCLR)
- For√ßa backbone a separar melhor RECT vs AB no espa√ßo de features

```python
# Adicionar ao treinamento Stage 2
contrastive_loss = SimCLR_loss(features_rect, features_ab, temperature=0.5)
total_loss = cb_focal_loss + 0.3 * contrastive_loss
```

**Solu√ß√£o 2: Focal Loss Tuning**
- Testar Œ≥=[2.0, 2.5, 3.0] (mais foco em hard examples)
- Testar Œ± customizado por classe (mais peso em AB/RECT)

**Protocolo:**
```bash
# Grid search
gammas = [2.0, 2.5, 3.0]
for gamma in gammas:
    python3 004_train_stage2_redesigned.py \
      --gamma $gamma \
      --epochs 25 \
      --output-dir stage2_gamma${gamma}

# Avaliar qual melhor separa RECT vs AB
```

**Ganho Esperado:** Stage 2 confusion RECT‚ÜîAB reduz 30-40% ‚Üí +0.5-1.0pp pipeline

**Custo:** 2 dias

### Fase 3: T√©cnicas Avan√ßadas (1-2 semanas) - **EXPLORAT√ìRIO**

**Condi√ß√£o:** Fase 2 n√£o atingiu 50% + disponibilidade de tempo

#### 3.1 Multi-Task Learning Stage 2 (5 dias)

**Problema:** Stage 2 n√£o aprende geometria interna de RECT

**Solu√ß√£o:** Dual-head architecture
```python
class MultiTaskStage2(nn.Module):
    def __init__(self):
        self.backbone = ImprovedBackbone()
        
        # Head principal: 3-way (SPLIT, RECT, AB)
        self.head_3way = nn.Linear(512, 3)
        
        # Head auxiliar: 2-way (HORZ, VERT) - apenas para RECT
        self.head_rect_geometry = nn.Linear(512, 2)
        
    def forward(self, x):
        features = self.backbone(x)
        pred_3way = self.head_3way(features)
        pred_rect = self.head_rect_geometry(features)
        return pred_3way, pred_rect

# Loss
loss = cb_focal_3way + 0.5 * cross_entropy_rect_geometry
```

**Vantagens:**
- Backbone aprende features para HORZ vs VERT
- Melhora separa√ß√£o RECT vs AB
- Regulariza√ß√£o impl√≠cita (Caruana, 1997)

**Ganho esperado:** +0.5-1.0pp

**Custo:** 5 dias

#### 3.2 Stage 2.5 Intermediate (4 dias)

**Problema:** Stage 3-RECT recebe samples ruins e colapsa

**Solu√ß√£o:** Adicionar stage intermedi√°rio robusto

```
Stage 2 ‚Üí RECT ‚Üí Stage 2.5 (HORZ vs VERT, treinado com noise) ‚Üí Output
```

**Diferen√ßa vs Stage 3-RECT:**
- Stage 3-RECT: Treinou s√≥ com RECT limpos
- Stage 2.5: Treina com RECT + 30% noise (AB + SPLIT)

**Ganho esperado:** +0.3-0.8pp

**Custo:** 4 dias

---

## üìä Resumo de Prioridades

| ID | T√©cnica | Fase | Custo | Ganho | Prioridade |
|----|---------|------|-------|-------|------------|
| 1.1 | Analisar Confus√£o RECT vs AB | 1 | 4h | Diagn√≥stico | üî¥üî¥üî¥ CR√çTICO |
| 1.2 | Diagnose Stage 3-RECT Vi√©s | 1 | 2h | +0.1-0.3pp | üî¥üî¥üî¥ CR√çTICO |
| **2.1** | **Noise Injection Stage 3** | **2** | **3.5d** | **+1.0-2.5pp** | üî¥üî¥üî¥ **RECOMENDADO** |
| 2.2 | Melhorar RECT vs AB (Stage 2) | 2 | 2d | +0.5-1.0pp | üî¥üî¥ Alta |
| 3.1 | Multi-Task Stage 2 | 3 | 5d | +0.5-1.0pp | üü° Explorat√≥rio |
| 3.2 | Stage 2.5 Intermediate | 3 | 4d | +0.3-0.8pp | üü° Explorat√≥rio |

---

## üéØ Recomenda√ß√£o Estrat√©gica Final

### **PLANO RECOMENDADO: Fase 1 + Fase 2 (5-7 dias)**

```
Dia 1: 
  - 1.1 Analisar Confus√£o RECT vs AB (4h)
  - 1.2 Diagnose Stage 3-RECT (2h)
  - Decis√£o: Confirmar diagn√≥stico do erro cascata

Dias 2-4:
  - 2.1 Noise Injection Stage 3-RECT (1.5 dias)
  - 2.1 Noise Injection Stage 3-AB (1.5 dias)
  
Dia 5:
  - Re-avaliar pipeline com modelos robustos
  - An√°lise de resultados

Dias 6-7 (SE NECESS√ÅRIO):
  - 2.2 Melhorar RECT vs AB com Focal Loss tuning
```

**Probabilidade de sucesso:** 70-85%  
**Ganho esperado:** +1.0-3.0pp ‚Üí **Accuracy 48.7-50.7%** ‚úÖ‚úÖ

**Fundamenta√ß√£o:**
- **Fase 1** confirma diagn√≥stico (erro cascata Stage 2‚Üí3)
- **Fase 2 (2.1)** ataca causa raiz (Stage 3 n√£o robusto a erros)
- T√©cnica validada na literatura (Hendrycks et al., 2019; Natarajan et al., 2013)
- Implementa√ß√£o relativamente simples
- Risco baixo (pior caso: sem melhoria, mas n√£o piora)

---

## ÔøΩ A√ß√£o Imediata Recomendada (HOJE - 13/10)

### **COME√áAR COM FASE 1: Diagn√≥stico Profundo**

```bash
# 1. Criar Script 009: An√°lise de Confus√£o RECT vs AB (2-3 horas)
cd pesquisa_v6/scripts
# Implementar 009_analyze_stage2_confusion.py
# - Carregar Stage 2 frozen model
# - Inferir validation set
# - Gerar confusion matrix detalhada RECT vs AB
# - Calcular taxa de erro Stage 2 ‚Üí Stage 3

# 2. Criar Script 010: Diagnose Stage 3-RECT (1-2 horas)
# Implementar 010_diagnose_stage3_rect.py
# - Avaliar Stage 3-RECT standalone
# - Verificar vi√©s VERT vs HORZ
# - Analisar class distribution treino

# 3. Executar diagn√≥sticos (1 hora)
python3 009_analyze_stage2_confusion.py
python3 010_diagnose_stage3_rect.py

# 4. An√°lise de resultados e decis√£o (1 hora)
# - Confirmar hip√≥tese de erro cascata
# - Decidir: prosseguir para Fase 2 (Noise Injection)
```

**Meta do dia:** Confirmar diagn√≥stico e planejar Fase 2

**Pr√≥ximos passos (14-15/10):**
- Implementar noise injection nos scripts 005 e 006
- Treinar Stage 3-RECT e Stage 3-AB robustos
- Re-avaliar pipeline

---

## ÔøΩüîß Melhorias T√©cnicas Implementadas (v6)

### 1. Backbone Upgrade
```python
# ResNet-18 + SE-Blocks + Dropout progressivo
- Squeeze-and-Excitation blocks (channel attention)
- Spatial Attention Module (SAM)
- Dropout progressivo: [0.1, 0.2, 0.3, 0.4]
- Group Normalization
```

### 2. Loss Functions

- **Stage 1:** FocalLoss(Œ±=0.25, Œ≥=2.5) + HardNegativeMining(ratio=3:1)
- **Stage 2:** ClassBalancedFocalLoss(Œ≤=0.9999, Œ≥=2.0)
- **Stage 3-AB:** Mixup(Œ±=0.4) + FocalLoss(Œ≥=2.0)

### 3. Data Augmentation Strategy

| Stage | Augmentation |
|-------|--------------|
| Stage 1 | Basic: HFlip, VFlip, Rotation, GaussianNoise |
| Stage 2 | Medium: Stage 1 + Cutout, GridShuffle |
| Stage 3-AB | **Heavy**: Flips com label swap, Mixup, CoarseDropout |

### 4. Training Strategy

#### Stage 1: Backbone Pre-training
- Epochs: 20, LR: 1e-3 ‚Üí 1e-5 (cosine decay)
- Result: F1=72.28% ‚úÖ

#### Stage 2: Fine-tuning (FROZEN-ONLY)
- Epochs: 8 (frozen), LR: 5e-4
- Result: F1=46.51% ‚úÖ
- **Decis√£o:** N√ÉO descongelar (negative transfer)

#### Stage 3: Specialists
- Epochs: 30 (RECT), 50 (AB)
- LR: 3e-4 ‚Üí 3e-6
- Freeze: Backbone sempre (evita catastrophic forgetting)

---

## üìÅ File Structure

```
pesquisa_v6/
‚îú‚îÄ‚îÄ PLANO_v6_Out.md                      # ‚≠ê Este documento (consolidado)
‚îÇ
‚îú‚îÄ‚îÄ v6_pipeline/                         # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                      # ‚úÖ 
‚îÇ   ‚îú‚îÄ‚îÄ data_hub.py                      # ‚úÖ Dataset utils
‚îÇ   ‚îú‚îÄ‚îÄ models.py                        # ‚úÖ Backbones + Heads
‚îÇ   ‚îú‚îÄ‚îÄ losses.py                        # ‚úÖ Focal, CB, Mixup losses
‚îÇ   ‚îú‚îÄ‚îÄ augmentation.py                  # ‚úÖ Aug pipelines
‚îÇ   ‚îú‚îÄ‚îÄ ensemble.py                      # ‚úÖ AB ensemble logic
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                       # ‚úÖ Evaluation utils
‚îÇ
‚îú‚îÄ‚îÄ scripts/                             # Training scripts
‚îÇ   ‚îú‚îÄ‚îÄ 001_prepare_v6_dataset.py        # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 002_prepare_v6_stage3_datasets.py # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 003_train_stage1_improved.py     # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 004_train_stage2_redesigned.py   # ‚úÖ Validado (frozen model)
‚îÇ   ‚îú‚îÄ‚îÄ 005_train_stage3_rect.py         # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 006_train_stage3_ab_fgvc.py      # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 007_optimize_thresholds.py       # ‚úÖ Validado
‚îÇ   ‚îú‚îÄ‚îÄ 008_run_pipeline_eval_v6.py      # ‚úÖ Validado
‚îÇ   ‚îî‚îÄ‚îÄ 009_compare_v5_v6.py             # ‚è≥ Pr√≥ximo
‚îÇ
‚îú‚îÄ‚îÄ docs_v6/                             # Documenta√ß√£o t√©cnico-cient√≠fica
‚îÇ   ‚îú‚îÄ‚îÄ 04_experimento_train_from_scratch.md  # Train from scratch (F1=37.38%)
‚îÇ   ‚îú‚îÄ‚îÄ 06_arquitetura_flatten_9classes.md    # Flatten architecture design
‚îÇ   ‚îú‚îÄ‚îÄ 07_flatten_pipeline_evaluation.md     # Baseline flatten (F1=10.24%)
‚îÇ   ‚îî‚îÄ‚îÄ 08_pipeline_aware_training.md         # Pipeline-aware NEGATIVO (F1=6.74%)
‚îÇ
‚îî‚îÄ‚îÄ logs/                                # Results
    ‚îî‚îÄ‚îÄ v6_experiments/
        ‚îú‚îÄ‚îÄ stage1/                      # Stage 1 checkpoints
        ‚îú‚îÄ‚îÄ stage2/                      # Stage 2 frozen model (USADO)
        ‚îú‚îÄ‚îÄ stage2_scratch/              # Stage 2 train from scratch
        ‚îú‚îÄ‚îÄ stage2_pipeline_aware/       # Pipeline-aware (ABANDONADO)
        ‚îú‚îÄ‚îÄ stage3_rect/                 # Stage 3-RECT checkpoints
        ‚îî‚îÄ‚îÄ stage3_ab/                   # Stage 3-AB FGVC checkpoints
```

---

## üß™ Experimentos Flatten (ARQUIVADOS - NEGATIVOS)

**Branch:** `feat/stage2-flatten-9classes` (merged to main em 904e0aa)  
**Dura√ß√£o:** ~3 dias (10-13 outubro 2025)  
**Status:** ‚ùå **ABANDONADO** - Resultados negativos

### Resumo dos Experimentos

| Experimento | F1-macro | Accuracy | Status |
|-------------|----------|----------|--------|
| Stage 2 isolado (004b) | 31.65% | 37.17% | ‚úÖ Funciona |
| Pipeline baseline (008b) | 10.24% | 57.93% | ‚ö†Ô∏è Acc boa, F1 ruim |
| Pipeline-aware (004c) | 6.74% | 8.50% | ‚ùå‚ùå‚ùå **PIOR** |

### Hip√≥tese H2.1 (distribution shift): **REJEITADA**

**Hip√≥tese:**
> "Stage 2 colapsa no pipeline porque treina com distribui√ß√£o balanceada mas recebe distribui√ß√£o filtrada pelo Stage 1. Solu√ß√£o: retreinar Stage 2 com samples filtrados."

**Resultado:**
- F1 piorou de 31.65% ‚Üí 6.74% (-78.7% degrada√ß√£o)
- Hip√≥tese H2.1 **falsificada experimentalmente**

**Root Causes Identificados:**
1. **Dataset too small:** 3,890 samples / 11.3M params = 1:2,900 ratio (need 1:10-100)
2. **Negative transfer:** Stage 1 binary features incompat√≠veis com Stage 2 multi-class
3. **Architectural flaw:** Objetivos conflitantes (Stage 1 vs Stage 2)

**Conclus√£o Cient√≠fica:**
> Flatten architecture fundamentalmente falha para predi√ß√£o de parti√ß√µes AV1. Negative transfer confirmado experimentalmente. Recommendation: Retornar √† hier√°rquica V6.

**Documenta√ß√£o Completa:** `docs_v6/08_pipeline_aware_training.md` (474 linhas, 9 se√ß√µes, 16 refer√™ncias)

---

## üìù Success Criteria

### ‚úÖ Crit√©rios M√≠nimos (Fase 1) - **QUASE ATINGIDOS**

- [x] Stage 1 F1 ‚â• 68% ‚Üí **72.28%** ‚úÖ
- [x] Stage 1 Precision ‚â• 62% ‚Üí **67.13%** ‚úÖ
- [x] Stage 2 Macro F1 ‚â• 45% ‚Üí **46.51%** ‚úÖ
- [ ] Stage 3-AB F1 ‚â• 40% ‚Üí **24.50%** ‚ùå (gap: -15.5pp)
- [ ] Final Accuracy ‚â• 48% ‚Üí **47.66%** ‚ö†Ô∏è (gap: -0.34pp)

### ‚≠ï Crit√©rios Ideais (Fase 2)

- [ ] Stage 1 F1 ‚â• 72% ‚Üí **72.28%** ‚úÖ
- [ ] Stage 1 Precision ‚â• 68% ‚Üí **67.13%** ‚ö†Ô∏è (gap: -0.87pp)
- [ ] Stage 2 Macro F1 ‚â• 52% ‚Üí **46.51%** ‚ùå (gap: -5.5pp)
- [ ] Stage 3-AB F1 ‚â• 55% ‚Üí **24.50%** ‚ùå (gap: -30.5pp)
- [ ] Final Accuracy ‚â• 55% ‚Üí **47.66%** ‚ùå (gap: -7.34pp)

---

## üìö Refer√™ncias Cient√≠ficas

### Negative Transfer & Transfer Learning
1. **Yosinski et al., 2014:** "How transferable are features in deep neural networks?"
2. **Raghu et al., 2019:** "Transfusion: Understanding Transfer Learning"
3. **Kornblith et al., 2019:** "Do Better ImageNet Models Transfer Better?"

### Loss Functions
4. **Lin et al., 2017:** "Focal Loss for Dense Object Detection"
5. **Cui et al., 2019:** "Class-Balanced Loss Based on Effective Number of Samples"
6. **Zhang et al., 2018:** "mixup: Beyond Empirical Risk Minimization"

### Catastrophic Forgetting
7. **Goodfellow et al., 2013:** "An Empirical Investigation of Catastrophic Forgetting"
8. **Howard & Ruder, 2018:** "Universal Language Model Fine-tuning (ULMFiT)"
9. **Kirkpatrick et al., 2017:** "Overcoming catastrophic forgetting"

### Data Augmentation & Robustness
10. **Yun et al., 2019:** "CutMix: Regularization Strategy to Train Strong Classifiers"
11. **DeVries & Taylor, 2017:** "Improved Regularization with Cutout"
12. **Hendrycks et al., 2019:** "Using Pre-Training Can Improve Model Robustness"

### Attention Mechanisms
13. **Hu et al., 2018:** "Squeeze-and-Excitation Networks"
14. **Woo et al., 2018:** "CBAM: Convolutional Block Attention Module"

### Model Calibration
15. **Guo et al., 2017:** "On Calibration of Modern Neural Networks"
16. **M√ºller et al., 2019:** "When Does Label Smoothing Help?"

---

## ü§ù Pr√≥ximo Passo Imediato

### Decis√£o: Executar Fase 1 (3-4 dias)

**Sequ√™ncia Recomendada:**

```bash
# 1. Dia 1 manh√£: Diagnose Stage 3-RECT (2h)
python3 pesquisa_v6/scripts/009_diagnose_stage3_rect.py

# 2. Dia 1 tarde: Threshold grid search (2h)
python3 pesquisa_v6/scripts/010_threshold_grid_search.py

# 3. Dia 2-3: Strong augmentation (1 dia) - SE NECESS√ÅRIO
python3 pesquisa_v6/scripts/004_train_stage2_redesigned.py \
  --mixup-alpha 0.4 --cutmix-beta 1.0 --epochs 30

# 4. Dia 4: Pipeline re-evaluation + an√°lise
python3 pesquisa_v6/scripts/008_run_pipeline_eval_v6.py
```

**Checkpoint de Decis√£o:**
- ‚úÖ Se accuracy ‚â• 48%: **PARAR**, documentar, finalizar
- ‚ö†Ô∏è Se 47.8-48%: Considerar 1.3 (augmentation)
- ‚ùå Se < 47.8%: Avaliar Fase 2 ou aceitar resultado

**Meta:** Fechar gap de -0.34pp e atingir ‚â•48% accuracy para finalizar Pipeline v6.

---

**Status Final:** üü° Em Desenvolvimento - 8/9 scripts validados  
**Pr√≥ximo:** Script 009 (compare v5/v6) OU Fase 1 Quick Wins  
**√öltima Atualiza√ß√£o:** 13 de outubro de 2025  
**Respons√°vel:** @chiarorosa
