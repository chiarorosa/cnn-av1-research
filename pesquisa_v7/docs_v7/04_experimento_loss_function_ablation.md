# Experimento 04: Loss Function Ablation

**Data de In√≠cio:** 16/10/2025  
**Pesquisador:** Chiaro Rosa  
**Status:** üîÑ EM PLANEJAMENTO

---

## 1. Motiva√ß√£o

### Contexto do Problema

Ap√≥s 3 experimentos (baseline, adapter capacity, BatchNorm fix), o **F1 do Stage 2 estagnou em ~58.5%**:

```
Baseline (Œ≥=4):        58.21%
Exp 02 (Œ≥=2):          58.18%  (-0.04 pp, sem ganho)
Exp 03 (BN fix):       58.53%  (+0.32 pp, ganho pequeno)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Progresso total:       +0.32 pp em 3 experimentos
```

**Hip√≥tese atual:** O **gargalo est√° na loss function**, n√£o na arquitetura.

### Por Que a Loss Function?

#### Problema 1: Hard Negatives Insuficientemente Penalizados

**Loss atual:** ClassBalancedFocalLoss com Œ≥=2.0

```python
FL(pt) = -Œ±t * (1 - pt)^Œ≥ * log(pt)
```

Onde:
- `pt` = probabilidade da classe correta
- `Œ≥ = 2.0` = focusing parameter (penaliza erros confiantes)
- `Œ±t` = class weight (balanceamento)

**Problema identificado:**
- Œ≥=2.0 √© padr√£o de Lin et al. (2017) para object detection
- Mas **particionamento AV1 pode ser mais dif√≠cil** (classes muito similares)
- **Erros confiantes** (high confidence, wrong class) podem n√£o estar sendo punidos suficientemente

#### Problema 2: Cross-Entropy Pode N√£o Ser Ideal

**Leng et al. (2022)** demonstraram que:
- Cross-entropy saturates gradients para classes hard
- Poly Loss (substitui√ß√£o polinomial) mant√©m gradients mais ativos
- **Ganhos reportados:** +1.2 pp (ImageNet), +2.3 pp (COCO)

#### Problema 3: Assimetria Entre False Positives e False Negatives

**Ridnik et al. (2021)** mostraram que:
- Multi-label classification se beneficia de penalidades assim√©tricas
- False Positives e False Negatives t√™m impactos diferentes
- **Asymmetric Loss** trouxe +0.6 pp (MS-COCO), +1.4 pp (NUS-WIDE)

---

## 2. Revis√£o de Literatura

### 2.1 Focal Loss (Lin et al., 2017)

**Paper:** "Focal Loss for Dense Object Detection"  
**Venue:** ICCV 2017  
**Citations:** 15,000+

**Contribui√ß√£o:**
- Introduziu focusing parameter Œ≥ para penalizar hard negatives
- Œ≥=0 ‚Üí Cross-Entropy padr√£o
- Œ≥=2 ‚Üí Focal Loss (padr√£o)
- Œ≥=5 ‚Üí Penaliza√ß√£o extrema

**Resultados:**
- Object detection: +3.9 AP (COCO)
- Œ≥=2 foi √≥timo para detection
- Œ≥>3 trouxe instabilidade em alguns datasets

**Aplica√ß√£o ao nosso caso:**
- Atual: Œ≥=2.0
- **Testaremos Œ≥=3.0:** Maior penaliza√ß√£o para hard negatives

---

### 2.2 Poly Loss (Leng et al., 2022)

**Paper:** "PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions"  
**Venue:** NeurIPS 2022  
**Citations:** 200+

**Contribui√ß√£o:**
- Reformula cross-entropy como s√©rie polinomial de Taylor
- Adiciona termos polinomiais de ordem superior
- Mant√©m gradientes ativos para hard samples

**Formula√ß√£o:**

```
PolyLoss = CE + Œµ1 * Poly1(pt) + Œµ2 * Poly2(pt) + ...
```

Onde:
- `Poly1(pt) = (1 - pt)`
- `Poly2(pt) = (1 - pt)^2`
- `Œµ1, Œµ2` = coeficientes (tipicamente Œµ1=1.0)

**Resultados originais:**
- ImageNet: +1.2 pp (top-1 accuracy)
- COCO detection: +2.3 AP
- Semantic segmentation: +1.8 mIoU

**Por que pode funcionar para AV1:**
- Parti√ß√µes AB s√£o **hard classes** (baixo F1 atual)
- Poly Loss mant√©m gradientes ativos ‚Üí melhora aprendizado hard classes

---

### 2.3 Asymmetric Loss (Ridnik et al., 2021)

**Paper:** "Asymmetric Loss For Multi-Label Classification"  
**Venue:** ICCV 2021  
**Citations:** 500+

**Contribui√ß√£o:**
- Penalidades diferentes para False Positives (FP) e False Negatives (FN)
- `Œ≥_pos` para positivos, `Œ≥_neg` para negativos
- √ötil quando FP e FN t√™m custos diferentes

**Formula√ß√£o:**

```
ASL = (1 - pt)^Œ≥_pos * log(pt)     [se y=1]
      pt^Œ≥_neg * log(1 - pt)       [se y=0]
```

**Resultados originais:**
- MS-COCO: +0.6 mAP
- NUS-WIDE: +1.4 mAP
- OpenImages: +1.1 mAP

**Aplica√ß√£o ao nosso caso:**
- AV1: FN (miss SPLIT) pode ser **mais custoso** que FP (predict SPLIT errado)
- Testaremos `Œ≥_pos=2, Œ≥_neg=4` (penalizar mais FN)

---

### 2.4 Label Smoothing (M√ºller et al., 2019)

**Paper:** "When Does Label Smoothing Help?"  
**Venue:** NeurIPS 2019  
**Citations:** 3,000+

**Contribui√ß√£o:**
- Suaviza one-hot labels: [0, 1, 0] ‚Üí [Œµ, 1-Œµ, Œµ]
- Reduz overconfidence
- Melhora calibration (confidence ‚âà accuracy)

**Formula√ß√£o:**

```
y_smooth = (1 - Œµ) * y_hard + Œµ / K
```

Onde:
- `Œµ = 0.1` (padr√£o)
- `K = num_classes`

**Resultados:**
- ImageNet: +0.2 pp (pequeno ganho, mas melhora calibration)
- CIFAR-100: +0.5 pp

**Por que pode funcionar:**
- Stage 2 tem classes similares (SPLIT, RECT, AB)
- Label smoothing pode evitar overconfidence em fronteiras amb√≠guas

---

## 3. Hip√≥teses

### H1: Focal Loss Œ≥=3.0 (Penaliza√ß√£o Maior)

**Hip√≥tese:** Aumentar Œ≥ de 2.0 para 3.0 melhora F1 das classes hard (AB).

**Fundamenta√ß√£o:**
- Lin et al. (2017): Œ≥ controla penaliza√ß√£o de hard negatives
- Œ≥=2 √© padr√£o para object detection (task mais simples)
- AV1 partition pode ser mais dif√≠cil ‚Üí requer Œ≥ maior

**Predi√ß√£o:** 
- **Val F1:** 58.53% ‚Üí **60.0-61.0%** (+1.5-2.5 pp)
- **AB F1:** Esperado maior ganho (classe mais hard)

**Risco:** Œ≥ muito alto pode causar instabilidade (Lin et al. reportaram issues com Œ≥=5)

---

### H2: Poly Loss (Gradientes Ativos)

**Hip√≥tese:** Poly Loss mant√©m gradientes ativos para hard samples ‚Üí melhora AB F1.

**Fundamenta√ß√£o:**
- Leng et al. (2022): +1.2-2.3 pp em ImageNet/COCO
- Cross-entropy satura gradientes para pt pr√≥ximo de 0 ou 1
- AB classes t√™m baixo F1 ‚Üí s√£o hard samples

**Predi√ß√£o:**
- **Val F1:** 58.53% ‚Üí **60.5-61.5%** (+2.0-3.0 pp)
- **AB F1:** Ganho esperado > SPLIT/RECT

**Configura√ß√£o:** Œµ1=1.0 (padr√£o Leng et al.)

---

### H3: Asymmetric Loss (Penalizar Mais FN)

**Hip√≥tese:** Penalizar mais FN (miss SPLIT) melhora recall ‚Üí aumenta F1.

**Fundamenta√ß√£o:**
- Ridnik et al. (2021): +0.6-1.4 mAP em multi-label
- AV1: FN (n√£o detectar SPLIT) pode ser mais custoso que FP
- Asymmetric Loss permite ajustar trade-off precision/recall

**Predi√ß√£o:**
- **Val F1:** 58.53% ‚Üí **59.5-60.5%** (+1.0-2.0 pp)
- **Recall:** Ganho esperado > precision

**Configura√ß√£o:** Œ≥_pos=2, Œ≥_neg=4 (penalizar mais FN)

---

### H4: Focal Loss + Label Smoothing (H√≠brido)

**Hip√≥tese:** Combinar Focal Loss (hard negatives) + Label Smoothing (calibration) ‚Üí melhor F1 + calibration.

**Fundamenta√ß√£o:**
- M√ºller et al. (2019): Label smoothing melhora calibration
- Focal Loss: hard negatives
- Combina√ß√£o pode trazer benef√≠cios complementares

**Predi√ß√£o:**
- **Val F1:** 58.53% ‚Üí **59.0-60.0%** (+0.5-1.5 pp)
- **Calibration:** Expected Calibration Error (ECE) deve reduzir

**Configura√ß√£o:** Œ≥=2.0, Œµ=0.1

---

## 4. Protocolo Experimental

### 4.1 Configura√ß√£o Base (Fixada para Todas Losses)

**Importante:** Manter **tudo fixo** exceto loss function para ablation limpa.

```python
# Dataset
dataset_dir = "pesquisa_v7/v7_dataset/block_16"
train_split = 80%
val_split = 20%

# Architecture
backbone = ResNet-18 (frozen, pr√©-treinado Stage 1)
adapter_reduction = 4  (Œ≥=4, 166k params)
adapter_locations = [layer3, layer4]

# Training
batch_size = 128
epochs = 50
early_stopping_patience = 15
optimizer = AdamW
lr_adapter = 0.001
lr_head = 0.0001
weight_decay = 0.01
scheduler = ReduceLROnPlateau (factor=0.5, patience=5)

# Regularization
dropout = [0.1, 0.2, 0.3, 0.4]  (progressive)
BatchNorm handling = backbone.eval()  (fix aplicado)

# Reproducibility
seed = 42
device = cuda
deterministic = True
```

---

### 4.2 Loss Functions a Testar

#### Experimento 4A: Focal Loss Œ≥=3.0

**Objetivo:** Testar se maior penaliza√ß√£o de hard negatives melhora F1.

**Mudan√ßa:**
```python
# Baseline
criterion = ClassBalancedFocalLoss(gamma=2.0, alpha=class_weights)

# Exp 4A
criterion = ClassBalancedFocalLoss(gamma=3.0, alpha=class_weights)
```

**Output dir:** `pesquisa_v7/logs/v7_experiments/exp04a_focal_gamma3`

---

#### Experimento 4B: Poly Loss

**Objetivo:** Testar se gradientes ativos melhoram hard classes.

**Implementa√ß√£o:**
```python
class PolyLoss(nn.Module):
    """
    Leng et al., NeurIPS 2022
    PolyLoss = CE + Œµ1 * (1 - pt)
    """
    def __init__(self, epsilon=1.0, class_weights=None):
        super().__init__()
        self.epsilon = epsilon
        self.class_weights = class_weights
        
    def forward(self, logits, targets):
        ce = F.cross_entropy(logits, targets, 
                            weight=self.class_weights, 
                            reduction='none')
        pt = torch.exp(-ce)  # pt = exp(-CE)
        poly1 = 1 - pt
        loss = ce + self.epsilon * poly1
        return loss.mean()
```

**Output dir:** `pesquisa_v7/logs/v7_experiments/exp04b_poly_loss`

---

#### Experimento 4C: Asymmetric Loss

**Objetivo:** Testar penaliza√ß√£o assim√©trica FP vs FN.

**Implementa√ß√£o:**
```python
class AsymmetricLoss(nn.Module):
    """
    Ridnik et al., ICCV 2021
    Adaptado para multi-class (one-vs-rest)
    """
    def __init__(self, gamma_pos=2, gamma_neg=4, class_weights=None):
        super().__init__()
        self.gamma_pos = gamma_pos
        self.gamma_neg = gamma_neg
        self.class_weights = class_weights
        
    def forward(self, logits, targets):
        # Convert to one-hot
        num_classes = logits.shape[1]
        targets_one_hot = F.one_hot(targets, num_classes).float()
        
        # Softmax
        probs = F.softmax(logits, dim=1)
        
        # Asymmetric penalties
        pos_loss = targets_one_hot * torch.pow(1 - probs, self.gamma_pos) * torch.log(probs + 1e-8)
        neg_loss = (1 - targets_one_hot) * torch.pow(probs, self.gamma_neg) * torch.log(1 - probs + 1e-8)
        
        loss = -(pos_loss + neg_loss)
        
        # Apply class weights
        if self.class_weights is not None:
            weights = self.class_weights[targets]
            loss = loss.mean(dim=1) * weights
            
        return loss.mean()
```

**Output dir:** `pesquisa_v7/logs/v7_experiments/exp04c_asymmetric_loss`

---

#### Experimento 4D: Focal + Label Smoothing

**Objetivo:** Testar combina√ß√£o (hard negatives + calibration).

**Implementa√ß√£o:**
```python
class FocalLossWithLabelSmoothing(nn.Module):
    """
    Focal Loss + Label Smoothing
    """
    def __init__(self, gamma=2.0, epsilon=0.1, class_weights=None):
        super().__init__()
        self.gamma = gamma
        self.epsilon = epsilon
        self.class_weights = class_weights
        
    def forward(self, logits, targets):
        num_classes = logits.shape[1]
        
        # Label smoothing
        targets_one_hot = F.one_hot(targets, num_classes).float()
        targets_smooth = (1 - self.epsilon) * targets_one_hot + self.epsilon / num_classes
        
        # Focal loss
        log_probs = F.log_softmax(logits, dim=1)
        probs = torch.exp(log_probs)
        focal_weight = torch.pow(1 - probs, self.gamma)
        loss = -focal_weight * targets_smooth * log_probs
        
        # Class weights
        if self.class_weights is not None:
            weights = self.class_weights[targets]
            loss = loss.sum(dim=1) * weights
        else:
            loss = loss.sum(dim=1)
            
        return loss.mean()
```

**Output dir:** `pesquisa_v7/logs/v7_experiments/exp04d_focal_label_smoothing`

---

### 4.3 Ordem de Execu√ß√£o

**Executar em paralelo** (4 GPUs ou sequencial):

1. **Exp 4A** (Focal Œ≥=3): ~2h training
2. **Exp 4B** (Poly Loss): ~2h training
3. **Exp 4C** (Asymmetric): ~2h training
4. **Exp 4D** (Focal + LS): ~2h training

**Total:** ~8h se sequencial, ~2h se paralelo

---

## 5. M√©tricas de Avalia√ß√£o

### 5.1 M√©tricas Prim√°rias

**M√©trica principal:** Validation F1-score (macro)

**M√©tricas secund√°rias:**
- Per-class F1 (SPLIT, RECT, AB)
- Precision (macro)
- Recall (macro)
- Accuracy

**Threshold de sucesso:**
- Ganho > **+1.0 pp** √© significativo
- Ganho > **+2.0 pp** √© breakthrough

---

### 5.2 M√©tricas de Calibration

**Para Exp 4D (Label Smoothing):**

- **Expected Calibration Error (ECE):**
  ```
  ECE = Œ£ (|confidence - accuracy| * bin_size)
  ```
  
- **Maximum Calibration Error (MCE):**
  ```
  MCE = max(|confidence - accuracy|)
  ```

**Threshold:** ECE < 0.10 √© bem calibrado

---

### 5.3 An√°lise de Hard Classes

**Foco especial em AB (classe mais dif√≠cil):**

- AB F1 atual: ~10-15% (v5), unknown (v7)
- **Target:** AB F1 > 20%

**An√°lise:**
- Confusion matrix: onde AB √© confundido?
- Confidence distribution: AB tem low confidence?

---

## 6. An√°lise Planejada

### 6.1 Compara√ß√£o Quantitativa

**Tabela 1: Overall Performance**

| Loss Function | Val F1 | Delta | Precision | Recall | Accuracy |
|---------------|--------|-------|-----------|--------|----------|
| Baseline (Œ≥=2) | 58.53% | - | - | - | - |
| Exp 4A (Œ≥=3) | ? | ? | ? | ? | ? |
| Exp 4B (Poly) | ? | ? | ? | ? | ? |
| Exp 4C (Asym) | ? | ? | ? | ? | ? |
| Exp 4D (Focal+LS) | ? | ? | ? | ? | ? |

**Tabela 2: Per-Class F1**

| Loss Function | SPLIT F1 | RECT F1 | AB F1 | Mean F1 |
|---------------|----------|---------|-------|---------|
| Baseline | ? | ? | ? | 58.53% |
| Exp 4A | ? | ? | ? | ? |
| Exp 4B | ? | ? | ? | ? |
| Exp 4C | ? | ? | ? | ? |
| Exp 4D | ? | ? | ? | ? |

---

### 6.2 An√°lise Estat√≠stica

**Quest√µes:**
1. Qual loss trouxe maior ganho?
2. Qual loss beneficiou mais AB (hard class)?
3. H√° trade-off precision/recall?
4. Label smoothing melhorou calibration?

**Testes:**
- Paired t-test (comparar losses)
- Effect size (Cohen's d)

---

### 6.3 An√°lise Qualitativa

**Confusion matrices:**
- Onde cada loss erra?
- AB √© confundido com RECT ou SPLIT?

**Confidence distributions:**
- Losses produzem confidences diferentes?
- Poly Loss tem confidences mais altas para AB?

---

## 7. Crit√©rios de Sucesso

### 7.1 Sucesso Completo ‚úÖ

- **Val F1 > 60.5%** (+2.0 pp sobre baseline 58.53%)
- **AB F1 > 20%** (ganho significativo em hard class)
- Training est√°vel (sem diverg√™ncia)
- Reprodut√≠vel (seed 42)

---

### 7.2 Sucesso Parcial ‚ö†Ô∏è

- **Val F1 > 59.5%** (+1.0 pp)
- AB F1 aumentou (mesmo se < 20%)
- Trade-offs aceit√°veis (e.g., -1% precision, +3% recall)

---

### 7.3 Falha ‚ùå

- Val F1 < 59.0% (< +0.5 pp)
- AB F1 n√£o melhorou ou piorou
- Training inst√°vel (NaN losses, diverg√™ncia)

---

## 8. Riscos e Mitiga√ß√£o

### Risco 1: Instabilidade com Œ≥=3.0

**Problema:** Lin et al. (2017) reportaram instabilidade com Œ≥ > 3

**Mitiga√ß√£o:**
- Monitorar loss/metrics a cada epoch
- Se divergir, testar Œ≥=2.5 (intermedi√°rio)
- Reduzir LR se necess√°rio

---

### Risco 2: Poly Loss Requer Tuning

**Problema:** Œµ1 ideal pode n√£o ser 1.0

**Mitiga√ß√£o:**
- Come√ßar com Œµ1=1.0 (padr√£o Leng et al.)
- Se n√£o funcionar, testar Œµ1=0.5 ou Œµ1=2.0

---

### Risco 3: Asymmetric Loss para Multi-Class

**Problema:** Original √© para multi-label, adaptamos para multi-class

**Mitiga√ß√£o:**
- Implementar one-vs-rest approach
- Validar implementa√ß√£o com toy example
- Comparar com literatura (buscar adapta√ß√µes existentes)

---

### Risco 4: Nenhuma Loss Melhora

**Problema:** Loss function n√£o √© o gargalo

**Mitiga√ß√£o:**
- Documentar resultado negativo (PhD-level)
- Prosseguir para Data Augmentation (pr√≥xima prioridade)
- Considerar que problema est√° em Stage 1 features

---

## 9. Cronograma

### Fase 1: Implementa√ß√£o (0.5h)

- [ ] Criar `v7_pipeline/losses_ablation.py`
- [ ] Implementar PolyLoss, AsymmetricLoss, FocalLabelSmoothing
- [ ] Validar implementa√ß√µes com toy examples
- [ ] Unit tests (verificar gradientes)

### Fase 2: Treinamento (8h sequencial ou 2h paralelo)

- [ ] Exp 4A: Focal Œ≥=3.0 (~2h)
- [ ] Exp 4B: Poly Loss (~2h)
- [ ] Exp 4C: Asymmetric Loss (~2h)
- [ ] Exp 4D: Focal + Label Smoothing (~2h)

### Fase 3: An√°lise (1h)

- [ ] Extrair m√©tricas de todos experiments
- [ ] Gerar tabelas comparativas
- [ ] Confusion matrices
- [ ] Confidence distributions
- [ ] Testes estat√≠sticos

### Fase 4: Documenta√ß√£o (1h)

- [ ] `04b_resultados_loss_ablation.md`
- [ ] Integrar com tese (Cap√≠tulo 5)
- [ ] Decidir pr√≥ximo passo

**Total estimado:** ~10h

---

## 10. Artefatos

### C√≥digo
```
pesquisa_v7/v7_pipeline/losses_ablation.py  (novas losses)
pesquisa_v7/scripts/021_train_loss_ablation.py  (script de treino)
```

### Checkpoints
```
pesquisa_v7/logs/v7_experiments/exp04a_focal_gamma3/
pesquisa_v7/logs/v7_experiments/exp04b_poly_loss/
pesquisa_v7/logs/v7_experiments/exp04c_asymmetric_loss/
pesquisa_v7/logs/v7_experiments/exp04d_focal_label_smoothing/
```

### Documenta√ß√£o
```
pesquisa_v7/docs_v7/04_experimento_loss_function_ablation.md  (este doc)
pesquisa_v7/docs_v7/04b_resultados_loss_ablation.md  (resultados)
```

---

## 11. Refer√™ncias

1. **Lin et al. (2017)** - "Focal Loss for Dense Object Detection", ICCV 2017
2. **Leng et al. (2022)** - "PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions", NeurIPS 2022
3. **Ridnik et al. (2021)** - "Asymmetric Loss For Multi-Label Classification", ICCV 2021
4. **M√ºller et al. (2019)** - "When Does Label Smoothing Help?", NeurIPS 2019
5. **Cui et al. (2019)** - "Class-Balanced Loss Based on Effective Number of Samples", CVPR 2019

---

## 12. Checklist de Execu√ß√£o

### Antes de Come√ßar
- [ ] Confirmar baseline F1=58.53% (Exp 03 BN fix)
- [ ] Dataset pronto (`v7_dataset/block_16/`)
- [ ] Stage 1 checkpoint dispon√≠vel
- [ ] GPU dispon√≠vel (8GB VRAM m√≠nimo)

### Durante Experimentos
- [ ] Monitorar loss/metrics a cada epoch
- [ ] Verificar stability (NaN, diverg√™ncia)
- [ ] Salvar checkpoints (best + final)
- [ ] Log completo de treinamento

### Ap√≥s Treinamento
- [ ] Extrair m√©tricas de todos experimentos
- [ ] Compara√ß√£o estat√≠stica (paired t-test)
- [ ] An√°lise qualitativa (confusion matrices)
- [ ] Documentar resultados (04b_resultados_loss_ablation.md)
- [ ] Decidir melhor loss function
- [ ] Atualizar c√≥digo base se necess√°rio

---

**√öltima atualiza√ß√£o:** 16/10/2025 - 00:30  
**Status:** üîÑ PRONTO PARA IMPLEMENTA√á√ÉO  
**Pr√≥ximo passo:** Criar `losses_ablation.py` e script de treino
