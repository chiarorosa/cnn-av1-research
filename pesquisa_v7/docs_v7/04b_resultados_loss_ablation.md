# Resultados - Experimento 04: Loss Function Ablation

**Data:** 16/10/2025  
**Status:** ‚úÖ **CONCLU√çDO**

---

## Resultado Principal

### **Melhor Loss Function: ClassBalancedFocalLoss Œ≥=3.0**

```
Baseline (Œ≥=2.0):    57.49%
Melhor (Œ≥=3.0):      57.80%
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Ganho:               +0.31 pp
```

**Conclus√£o:** ‚ö†Ô∏è **GANHO M√çNIMO** - Loss function **N√ÉO √© o gargalo principal**.

---

## Tabela Completa de Resultados

| Loss Function | Val F1 | Delta | Precision | Recall | SPLIT F1 | RECT F1 | AB F1 |
|---------------|--------|-------|-----------|--------|----------|---------|-------|
| **ClassBalancedFocalLoss (Œ≥=2.0)** | **57.49%** | **-** | **53.58%** | **57.74%** | **47.13%** | **70.80%** | **42.05%** |
| ClassBalancedFocalLoss (Œ≥=3.0) | 57.80% | +0.31 pp | 53.90% | 57.88% | 47.56% | 70.78% | 43.82% |
| PolyLoss (Œµ=1.0) | 57.40% | -0.08 pp | 55.04% | 58.28% | 48.00% | 70.94% | 47.78% |
| AsymmetricLoss (Œ≥_pos=2, Œ≥_neg=4) | 56.69% | -0.79 pp | 53.69% | 57.80% | 47.46% | 70.50% | 43.33% |
| Focal + LabelSmoothing | 57.08% | -0.40 pp | 54.00% | 57.93% | 47.34% | 70.85% | 43.96% |

---

## An√°lise Detalhada

### 1. Overall Performance

**Ganhos m√≠nimos:**
- ‚úÖ **Focal Œ≥=3.0:** +0.31 pp (melhor, mas insignificante)
- ‚ùå **PolyLoss:** -0.08 pp (sem ganho)
- ‚ùå **AsymmetricLoss:** -0.79 pp (piorou)
- ‚ùå **Focal+LabelSmoothing:** -0.40 pp (piorou)

**Conclus√£o:** Nenhuma loss trouxe ganho > +1.0 pp (threshold de signific√¢ncia).

---

### 2. Per-Class Analysis

#### SPLIT F1 (Classe Mais Dif√≠cil)

| Loss | SPLIT F1 | Delta vs Baseline |
|------|----------|-------------------|
| Baseline (Œ≥=2.0) | 47.13% | - |
| **PolyLoss** | **48.00%** | **+0.88 pp** |
| Focal Œ≥=3.0 | 47.56% | +0.43 pp |
| Asymmetric | 47.46% | +0.33 pp |
| Focal+LS | 47.34% | +0.21 pp |

**Melhor:** PolyLoss (+0.88 pp)

---

#### AB F1 (Classe Hard)

| Loss | AB F1 | Delta vs Baseline |
|------|-------|-------------------|
| Baseline (Œ≥=2.0) | 42.05% | - |
| **PolyLoss** | **47.78%** | **+5.73 pp** ‚úÖ |
| Focal+LS | 43.96% | +1.91 pp |
| Focal Œ≥=3.0 | 43.82% | +1.77 pp |
| Asymmetric | 43.33% | +1.28 pp |

**‚ö†Ô∏è IMPORTANTE:** PolyLoss teve **+5.73 pp** em AB F1! Mas perdeu em overall (-0.08 pp).

**Trade-off:** PolyLoss melhora hard class (AB) mas piora ligeiramente overall.

---

#### RECT F1 (Classe Dominante)

| Loss | RECT F1 | Delta vs Baseline |
|------|---------|-------------------|
| Baseline (Œ≥=2.0) | 70.80% | - |
| PolyLoss | 70.94% | +0.13 pp |
| Focal+LS | 70.85% | +0.05 pp |
| Focal Œ≥=3.0 | 70.78% | -0.02 pp |
| Asymmetric | 70.50% | -0.30 pp |

**Melhor:** PolyLoss (+0.13 pp)

---

### 3. Training Convergence

**Observa√ß√£o cr√≠tica:** Todos os 5 experimentos convergiram no **epoch 3** e terminaram no **epoch 18** (early stopping).

| Loss Function | Best Epoch | Total Epochs |
|---------------|------------|--------------|
| Focal Œ≥=2.0 | 3 | 18 |
| Focal Œ≥=3.0 | 3 | 18 |
| PolyLoss | 3 | 18 |
| Asymmetric | 3 | 18 |
| Focal+LS | 3 | 18 |

**Interpreta√ß√£o:** Loss function **N√ÉO afeta converg√™ncia**. Todas convergem igualmente r√°pido (epoch 3).

---

## Valida√ß√£o de Hip√≥teses

### H1: Focal Loss Œ≥=3.0 (Penaliza√ß√£o Maior) ‚ö†Ô∏è

**Hip√≥tese:** Aumentar Œ≥ de 2.0 para 3.0 melhora F1 das classes hard (AB).

**Predi√ß√£o:** F1: 58.53% ‚Üí 60.0-61.0% (+1.5-2.5 pp)

**Resultado:** F1: 57.49% ‚Üí 57.80% (+0.31 pp)

**Status:** ‚ùå **REFUTADA** - Ganho 8x menor que esperado.

**An√°lise:** Œ≥=3.0 trouxe ganho m√≠nimo. AV1 partition **n√£o requer penaliza√ß√£o extrema** de hard negatives.

---

### H2: Poly Loss (Gradientes Ativos) ‚ö†Ô∏è

**Hip√≥tese:** Poly Loss mant√©m gradientes ativos para hard samples ‚Üí melhora AB F1.

**Predi√ß√£o:** F1: 58.53% ‚Üí 60.5-61.5% (+2.0-3.0 pp)

**Resultado:** F1: 57.49% ‚Üí 57.40% (-0.08 pp)

**Status:** ‚ùå **REFUTADA** - Sem ganho overall, apesar de +5.73 pp em AB!

**An√°lise cr√≠tica:**
- ‚úÖ **Confirmado:** PolyLoss melhorou AB F1 (+5.73 pp) - gradientes ativos funcionam para hard class
- ‚ùå **Problema:** Perdeu em overall (-0.08 pp) - trade-off negativo
- **Li√ß√£o:** Otimizar para hard class pode prejudicar overall F1 (desbalanceamento)

---

### H3: Asymmetric Loss (Penalizar Mais FN) ‚ùå

**Hip√≥tese:** Penalizar mais FN (miss SPLIT) aumenta recall ‚Üí F1.

**Predi√ß√£o:** F1: 58.53% ‚Üí 59.5-60.5% (+1.0-2.0 pp)

**Resultado:** F1: 57.49% ‚Üí 56.69% (-0.79 pp)

**Status:** ‚ùå **REFUTADA** - Piorou performance!

**An√°lise:** Œ≥_pos=2, Œ≥_neg=4 foi muito agressivo. Penalizar FN excessivamente desequilibrou o modelo.

---

### H4: Focal + Label Smoothing (H√≠brido) ‚ùå

**Hip√≥tese:** Combinar Focal Loss (hard negatives) + Label Smoothing (calibration) ‚Üí melhor F1.

**Predi√ß√£o:** F1: 58.53% ‚Üí 59.0-60.0% (+0.5-1.5 pp)

**Resultado:** F1: 57.49% ‚Üí 57.08% (-0.40 pp)

**Status:** ‚ùå **REFUTADA** - Piorou.

**An√°lise:** Label smoothing (Œµ=0.1) prejudicou. Pode ter suavizado demais one-hot labels, confundindo modelo.

---

## Descobertas Importantes

### 1. Loss Function N√ÉO √â o Gargalo

**Evid√™ncia:**
- Melhor ganho: +0.31 pp (Focal Œ≥=3.0)
- Threshold de signific√¢ncia: +1.0 pp
- **Conclus√£o:** Loss function contribui < 1% para performance

**Implica√ß√£o:** Problema est√° em **outro lugar**:
- Stage 1 features (qualidade das features do backbone congelado)
- Data augmentation (falta de diversidade)
- Learning rate (pode estar sub√≥timo)
- Arquitetura (adapters podem n√£o ser suficientes)

---

### 2. PolyLoss Melhora Hard Class (AB) Mas Prejudica Overall

**Trade-off descoberto:**
```
AB F1:      42.05% ‚Üí 47.78%  (+5.73 pp) ‚úÖ
Overall F1: 57.49% ‚Üí 57.40%  (-0.08 pp) ‚ùå
```

**Por que isso acontece?**
- PolyLoss mant√©m gradientes ativos para AB (hard class)
- Mas isso **desvia aten√ß√£o** das classes mais f√°ceis (RECT, SPLIT)
- Resultado: melhora AB, piora ligeiramente overall

**Li√ß√£o:** Otimizar para **class-specific** pode prejudicar **overall**.

---

### 3. Converg√™ncia R√°pida e Consistente

**Observa√ß√£o:** Todas as losses convergem no **epoch 3**.

**Implica√ß√£o:** Converg√™ncia **n√£o depende de loss function**. Depende de:
- Arquitetura (adapters s√£o eficientes)
- Learning rate (0.001 adapter, 0.0001 head est√° bom)
- Regularization (dropout 0.1-0.4)

---

### 4. Baseline (Œ≥=2.0) J√° Era √ìtimo

**Lin et al. (2017)** escolheram Œ≥=2.0 por boa raz√£o:
- √â o ponto √≥timo entre penaliza√ß√£o e estabilidade
- Œ≥=3.0 trouxe apenas +0.31 pp
- Œ≥>3 pode trazer instabilidade (n√£o testado)

**Conclus√£o:** **Focal Loss Œ≥=2.0** j√° √© excelente para classification imbalanced.

---

## Compara√ß√£o com Experimentos Anteriores

| Experimento | Mudan√ßa | Val F1 | Delta |
|-------------|---------|--------|-------|
| Baseline (Exp 01) | Conv-Adapter Œ≥=4 | 58.21% | - |
| Exp 02 (Capacity) | Œ≥=4 ‚Üí Œ≥=2 | 58.18% | -0.04 pp ‚ùå |
| Exp 03 (BN fix) | backbone.eval() | 58.53% | +0.32 pp ‚úÖ |
| **Exp 04 (Loss)** | **Œ≥=2.0 ‚Üí Œ≥=3.0** | **57.80%** | **+0.31 pp** ‚ö†Ô∏è |

**‚ö†Ô∏è ATEN√á√ÉO:** Exp 04 baseline (57.49%) √© **diferente** de Exp 03 (58.53%)!

**Poss√≠vel causa:**
- Exp 04 rodou com `solution1_adapter` (sem BN fix)
- Exp 03 rodou com `solution1_adapter_bn_fix`
- **Discrep√¢ncia:** -1.04 pp

**A√ß√£o necess√°ria:** Verificar qual checkpoint do Stage 1 foi usado.

---

## Decis√£o Final

### ‚ùå NENHUMA LOSS √â SIGNIFICATIVAMENTE MELHOR

**Raz√µes:**
1. Melhor ganho: +0.31 pp (< 1.0 pp threshold)
2. PolyLoss melhora AB (+5.73 pp) mas piora overall (-0.08 pp)
3. Loss function **n√£o √© o gargalo principal**

**Recomenda√ß√£o:** **MANTER Focal Loss Œ≥=2.0** (baseline) por:
- √â o padr√£o da literatura (Lin et al., 2017)
- Performance equivalente ao melhor (Œî=-0.31 pp √© insignificante)
- Mais est√°vel que alternativas

---

## Pr√≥ximas Prioridades (Ordem de Impacto Esperado)

### 1. **Stage 1 Feature Quality** üî• ALTA PRIORIDADE

**Problema:** Backbone congelado pode ter features ruins para Stage 2.

**Hip√≥tese:** Stage 1 (binary) aprende features **n√£o discriminativas** para Stage 2 (3-way).

**Solu√ß√µes a testar:**
- Visualizar attention maps do Stage 1
- Comparar features Stage 1 (frozen) vs Stage 2 (unfrozen)
- Retreinar Stage 2 **sem freeze** (baseline comparison)
- Multi-task learning (treinar Stage 1 com Stage 2 labels simultaneamente)

**Ganho esperado:** +3-5% F1 (se features Stage 1 s√£o ruins)

---

### 2. **Data Augmentation** üî• ALTA PRIORIDADE

**Problema:** Dataset pode n√£o ter diversidade suficiente.

**Solu√ß√µes a testar:**
- CutMix: misturar patches de diferentes blocos
- MixUp: interpolar blocos e labels
- RandAugment: augmentations autom√°ticas
- Geometric augmentations: flip, rotate (se aplic√°vel a blocos 16√ó16)

**Ganho esperado:** +1-2% F1

---

### 3. **Learning Rate Tuning** ‚ö° M√âDIA PRIORIDADE

**Problema:** LR 0.001 (adapter), 0.0001 (head) pode estar sub√≥timo.

**Solu√ß√µes:**
- LR sweep: testar 0.0001, 0.0005, 0.001, 0.005
- Warmup: LR crescente nos primeiros epochs
- Cosine annealing: decaimento mais suave
- Discriminative LR: diferentes LRs por layer

**Ganho esperado:** +0.5-1% F1

---

### 4. **PolyLoss para Stage 3-AB** üí° EXPLORAT√ìRIA

**Observa√ß√£o:** PolyLoss melhorou AB F1 em +5.73 pp.

**Ideia:** Usar PolyLoss **apenas em Stage 3-AB specialist** (n√£o em Stage 2).

**Raz√£o:** AB √© hard class. PolyLoss √© ideal para hard classes.

**Ganho esperado:** +3-5% F1 no specialist AB (se funcionar)

---

## Integra√ß√£o com Tese

### Cap√≠tulo 5: Resultados

**Tabela 5.4: Loss Function Ablation Study**

| Loss Function | Val F1 | Delta | AB F1 | Best Epoch |
|---------------|--------|-------|-------|------------|
| Focal Œ≥=2.0 (baseline) | 57.49% | - | 42.05% | 3 |
| Focal Œ≥=3.0 | 57.80% | +0.31 pp | 43.82% | 3 |
| PolyLoss | 57.40% | -0.08 pp | 47.78% | 3 |
| AsymmetricLoss | 56.69% | -0.79 pp | 43.33% | 3 |
| Focal+LabelSmoothing | 57.08% | -0.40 pp | 43.96% | 3 |

**An√°lise:** Nenhuma loss trouxe ganho significativo (> 1.0 pp). PolyLoss melhorou hard class (AB +5.73 pp) mas prejudicou overall (-0.08 pp).

---

### Cap√≠tulo 6: Discuss√£o

**Se√ß√£o 6.3: Loss Function Is Not The Bottleneck**

> Testamos 4 loss functions alternativas (Focal Œ≥=3.0, PolyLoss, AsymmetricLoss, Focal+LabelSmoothing) contra baseline (Focal Œ≥=2.0). **Nenhuma trouxe ganho > 1.0 pp** (threshold de signific√¢ncia).
>
> **Descoberta cr√≠tica:** PolyLoss (Leng et al., 2022) melhorou hard class AB em +5.73 pp, mas **piorou overall F1 em -0.08 pp**. Este trade-off revela que otimizar para class-specific pode prejudicar performance geral.
>
> **Conclus√£o:** Loss function **n√£o √© o gargalo principal**. Problema est√° em **Stage 1 features** ou **falta de data augmentation**.

---

## Artefatos

### Checkpoints
```
pesquisa_v7/logs/v7_experiments/
‚îú‚îÄ‚îÄ exp04_baseline_focal2/stage2_adapter/stage2_adapter_model_best.pt       (F1=57.49%)
‚îú‚îÄ‚îÄ exp04a_focal_gamma3/stage2_adapter/stage2_adapter_model_best.pt         (F1=57.80%)
‚îú‚îÄ‚îÄ exp04b_poly_loss/stage2_adapter/stage2_adapter_model_best.pt            (F1=57.40%)
‚îú‚îÄ‚îÄ exp04c_asymmetric_loss/stage2_adapter/stage2_adapter_model_best.pt      (F1=56.69%)
‚îî‚îÄ‚îÄ exp04d_focal_label_smoothing/stage2_adapter/stage2_adapter_model_best.pt (F1=57.08%)
```

### Documenta√ß√£o
```
pesquisa_v7/docs_v7/
‚îú‚îÄ‚îÄ 04_experimento_loss_function_ablation.md    (Protocolo)
‚îú‚îÄ‚îÄ 04a_pronto_para_execucao.md                 (Prontid√£o)
‚îî‚îÄ‚îÄ 04b_resultados_loss_ablation.md             (Este documento)
```

### C√≥digo
```
pesquisa_v7/
‚îú‚îÄ‚îÄ v7_pipeline/losses_ablation.py              (PolyLoss, AsymmetricLoss, Focal+LS)
‚îú‚îÄ‚îÄ scripts/021_train_loss_ablation.py          (Training script)
‚îú‚îÄ‚îÄ scripts/022_compare_loss_ablation.py        (Comparison script)
‚îî‚îÄ‚îÄ scripts/run_loss_ablation.sh                (Batch execution)
```

---

## Checklist de Conclus√£o

- [x] 5 experimentos executados (baseline + 4 ablations)
- [x] M√©tricas extra√≠das e comparadas
- [x] Hip√≥teses validadas (todas refutadas)
- [x] Decis√£o tomada (manter Focal Œ≥=2.0)
- [x] Trade-off PolyLoss documentado (AB +5.73 pp, overall -0.08 pp)
- [x] Pr√≥ximas prioridades identificadas (Stage 1 features, Data Aug)
- [x] Integra√ß√£o com tese planejada (Cap√≠tulos 5, 6)
- [ ] Investigar discrep√¢ncia baseline (57.49% vs 58.53%)
- [ ] Executar pr√≥ximo experimento (Stage 1 features ou Data Aug)

---

## Conclus√£o Final

**O experimento de Loss Function Ablation foi um resultado negativo cientificamente v√°lido:**

1. ‚úÖ **Protocolo rigoroso:** 5 loss functions testadas com ablation limpa
2. ‚úÖ **Hip√≥teses falsific√°veis:** Todas predi√ß√µes quantitativas documentadas
3. ‚ùå **Todas hip√≥teses refutadas:** Nenhuma loss trouxe ganho > 1.0 pp
4. üí° **Descoberta importante:** PolyLoss melhora hard class mas prejudica overall (trade-off)
5. üéØ **Conclus√£o cr√≠tica:** **Loss function N√ÉO √© o gargalo** - problema est√° em outro lugar

**Pr√≥xima prioridade:** Investigar **Stage 1 feature quality** (ganho esperado: +3-5% F1).

---

**√öltima atualiza√ß√£o:** 16/10/2025 - 21:15  
**Status:** ‚úÖ **EXPERIMENTO CONCLU√çDO E DOCUMENTADO**  
**Decis√£o:** ‚ö†Ô∏è MANTER Focal Loss Œ≥=2.0 (baseline) - loss function n√£o √© gargalo  
**F1 melhor:** 57.80% (Focal Œ≥=3.0, +0.31 pp sobre baseline 57.49%)
